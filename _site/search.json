[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew Swanson, Ph.D.",
    "section": "",
    "text": "Email\n  \n  \n      ORCID\n  \n  \n      LinkedIn\n  \n  \n      Github\n  \n  \n      OSF\n  \n\n  \n  \n\n\nHello and welcome to my portfolio! My name is Matt Swanson and I am an Industrial Organizational Psychologist who has spent the past decade expanding my expertise in the areas of organizational effectiveness, data analysis, training evaluation, DEI, and statistical and research methodology.\nI am excited to present my portfolio showcasing my academic and professional accomplishments. This portfolio reflects my expertise in research design, data analysis, consulting, as well as my commitment to continuous learning and self-development. My goal with this website is to showcase some of my skill set and provide a platform for others interested in IO psychology and data analytics to hopefully learn something about the field. Step into the world of IO psychology with me, where the science of human behavior and performance in the workplace meets the art of creating meaningful and impactful change.\nI spent a lot of time working on my dissertation and I would love to share what I found! For my dissertation, I studied how organizational psychological safety (which I experimentally manipulated) influenced perceptions of authenticity at work as well as feelings of well-being and belongingness for both heterosexual and LGBTQ employees. My dissertation was also mixed-methods, so I got to assess the qualitative nuance behind participant’s perceptions of psychologically safe and unsafe work experiences!\nMy dissertation is hosted on ProQuest.\nFolberg, A. M., Dueland, L. B., Swanson, M., Stepanek, S., Hebl, M., & Ryan, C. S. (2024). Racism underlies seemingly race‐neutral conservative criticisms of DEI statements among Black and White people in the United States. Journal of Occupational and Organizational Psychology, 97(3), 791–816. https://doi.org/10.1111/joop.12491\nSee more here."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Matthew Swanson, Ph.D.",
    "section": "",
    "text": "Welcome!\n\n\n\n\n\nI’m an IO psychologist who has spent the past 7 years gaining knowledge and expertise in the areas of organizational effectiveness, data analysis, DEI, and statistical methodology. My current stream of research centers on organizational conflict management, addressing systems of racial disparity in higher education, and promoting employee authenticity.\n\nMy goal with this website is to showcase my current skill set and provide a platform for others interested in IO psychology and data analytics to hopefully learn something about the field.\n\n\nMy Career Thus Far\n\n\n\n\n\nI have worked in both industry and academia and have learned a lot over these years! I’ve had the pleasure of teaching and supporting graduate level analysis of variance (ANOVA) at the University of Nebraska Omaha using both SAS and R for the past three years. While COVID certainly added barriers to the teaching process, it gave me a great opportunity to learn and teach data analysis in R at the graduate level (hence the Zoom screenshot).\nConcurrently, I’ve helped organizations recognize, analyze, and address common issues such as turnover, training effectiveness, and job performance. I have also helped organizations survey and assess the DEI climate of their workforce, worked with selection procedures to determine adverse impact, built statistical models to predict turnover likelihood, worked on developing psychometrically sound assessments, and served as lead on qualitative and quantitative data collection efforts. Additionally, I have served as a guest editor for the Journal of Social Issues and as editor for research conferences and academic awards.\nIn my spare time I love jamming out with my cello!\n\n\nDissertation\n\nI spent a lot of time working on my dissertation and I would love to share what I found with anyone interested! For my dissertation, I studied how organizational psychological safety (which I experimentally manipulated) influenced perceptions of authenticity at work as well as feelings of well-being and belongingness for both heterosexual and LGBTQ employees. My dissertation was also mixed-methods, so I got to assess the qualitative nuance behind participant’s perceptions of psychologically safe and unsafe work experiences!\nMy dissertation is hosted on ProQuest. I am currently reworking it for publication so be on the lookout for that article!"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume / CV",
    "section": "",
    "text": "Education\n\n\n\n\n\n\n\n\n\n2019 - 2022\nUNIVERSITY OF NEBRASKA OMAHA\nPh.D. in Industrial Organizational Psychology\nGPA = 4.00\n2016 - 2018\nUNIVERSITY OF Akron\nM.A. in Industrial Organizational Psychology\nGPA = 3.78\n2013 - 2016\nKENT STATE UNIVERSITY\nB.S. in Psychology\nGPA = 3.75\n\n\n\n\n\n\n\n\n\nExperience\n\n\n\n\n\n\n\n\nWORK EXPERIENCE:\n2022 - Present\nWriter - IOATWORK\n\nTranslated academic research to an applied audience via monthly research article reviews\nAssessed and reported research designs and advanced statistics to a practitioner audience\n\n\n\n\n2019 - 2021\nExternal Consultant - The Center for Applied Psychological Services\n\nActed as project lead and managed data collection procedures (both for large- and small-scale data sets)\nAnalyzed big data using Excel, SPSS, and R to create and present technical reports\nUtilized dashboard/visualization toolsets (Tableau) for data presentation and analysis\nServed as the expert for people analytics and data reporting\nDeveloped Knowledge, Skills, & Ability-linked items for evidence-based employee selection\nConducted a variety of data analyses (Regression, Validity, Frequencies, Data Visualization)\nWorked with law enforcement to develop officer promotional exams\n\n\n\n\n2017 - 2019\nIO Analyst, Client Solutions & Program Management - Corporate College\n\nConducted job analyses and redesigned hiring procedures and evaluative criteria\nDeveloped over 70 evidence-based training courses targeting workplace competencies such as Diversity, Equity, and Inclusion, change management, and accountability\nEnsured data quality and proper collection standards Converted data into business insights through predictive modeling\nCreated predictive models to assess satisfaction, turnover, and organizational climate & culture\nImproved the frontline and leadership talent of dozens of clients via organizational training\nWorked with companies like American Greetings, Cleveland Cavalier and Oatey to address absenteeism and training issues, resulting in healthier work environments and improved retention\nTrained a small team on how to evaluate and score evaluations\n\n\n\n\n2017 - 2017\nConsultant - Center for Organizational Research\n\nConducted data analyses and compiled results into professional reports\nCustomized tests, measures, survey, and selection systems\nCreated and validated surveys to assess perceptions of success for client’s internal projects\n\n\n\n\n\n\n\n\n\n\n\nTEACHING AND PROFESSIONAL ACTIVITIES:\n2019 - 2022\nTeaching Assistant - University of Nebraska Omaha\n\nDeveloped course content, taught graduate-level R coding, and graded coursework\nLectured weekly for graduate-level analysis of variance (ANOVA)\n\n\n\n\n2020 - 2023\nConference Reviewer\n\nReviewed submissions, provided feedback, and determined application status for research submitted to the following conferences:\n\nMidwest Academy of Management\nSociety for the Psychological Study of Social Issues\nReviewer for APS Student Research & Grant Award\n\n\n\n\n\n2023 - 2023\nJournal Editor\n\nServed as Guest Editor for a special edition of the Journal of Social Issues\n\n\n\n\n\n\n\n\n\n\n\nRESEARCH EXPERIENCE:\n2019 - Present\nLab Researcher - Diversity and Inclusion Research Lab\n\nProvided input to research methodology and presentation efforts\nDeveloped empirical work to test questions regarding diversity and inclusion at work\nAnalyzed data sets using R, SPSS, SAS, and Python, and wrote empirical findings for both academic and practitioner audiences\n\n\n\n\n2019 - 2021\nExternal Research Consultant - Tri-Faith\n\nServed as a research consultant to Tri-Faith’s research initiatives\nHelped align project ideas with research questions\n\n\n\n\n2015 - 2016\nResearch Assistant - Emotion, Stress, and Relationships Lab\n\nOversaw clinical trials targeted at developing a language-based psychotherapy\n\n\n\n\n\nConference Presentations & Posters\n\nFolberg, A., Votruba, A., Marshburn, C., Swanson, M., Crawford, D., Kaiser, C. (2023). Reimagining Resolution: Addressing Racism in Academic Institutions with Conflict Resolution. Interactive discussion to be presented at the annual conference for the Society for the Psychological Study of Social Issues, Denver, CO.\n\nXimenes, M., Folberg, A., Swanson, M. A., Dueland, L., Stepanek, S., Ryan C. (2023). The Role of Conservatism in Evaluations of Diversity Statements. Poster to be presented at the Midwestern Psychological Association, Chicago, IL.\n\nSwanson, M. A.(2022). Can I Remain True to Myself at Work? An Experimental Study of Psychologically Safe versus Unsafe Workplaces on LGBTQ+ and Heterosexual Perceptions of Authenticity, Belongingness, Vigilance, and Resiliency. Poster presented at the Student Research and Creative Activity Fair, Omaha, NE. \n\nStepanek, S., Dueland, L., Folberg, A. M., Swanson, M. A., & Ryan, C. S. (2021).Whites (vs. Blacks) and conservatives exhibit less interest in applying for jobs that request diversity statements in application materials. Poster presented at the annual conference of the Association for Psychological Science, Virtual. \n\n‍Swanson, M. A.(2021). Essential Meaningful Work: A Multistage Proposal for the Development and Validation of the Essential Meaningful Work Inventory (EMWI). Poster presented at the annual conference of the Association for Psychological Science, Virtual. \n\nCrawford, D., Swanson, M. A., Dueland, L. B., Stepanek, S., & Ryan, C. (2021). Leveraging DEI Policies and Training to Navigate Conflict in Organizations. Poster presented at the annual conference for the Society for the Psychological Study of Social Issues, Virtual.\n\n‍Dodds, B. L., Ryan, C. S., Swanson, M. A. (2021). The Relationships of Perceived Parental Social Support to Vigilance and Resilience among LGBTQ and Straight Cisgender Adults. Poster presented at the University of Nebraska Omaha’s University Honors Program, Omaha, NE. \n\nDueland, L. B., Folberg, A. M., Swanson, M. A., & Ryan, C. S. (2020). Perceptions of Requests for Diversity Statements in Job Advertisements. Poster presented at the annual meeting of the Society for the Psychological Study of Social Issues, Denver, CO. (Conference cancelled) \n\nCrawford, D., Swanson, M. A., Dueland, L. B., Stepanek, S., & Ryan, C. (2020).Conflict management techniques within diversity initiatives: A critical missing link. Interactive discussion facilitated at the Society for the Psychological Study of Social Issues Conference, Denver, CO. (Conference cancelled) \nDueland, L. B., Folberg, A., Swanson, M. A., & Ryan, C. (2020). Reactions to selection processes involving diversity statements. Poster presented at the Society for Industrial Organizational Psychology Conference, Austin, TX.(Conference cancelled) \n\n‍Swanson,M. A. (2015) The Effects of Positive Teacher-Child Relationships on School Achievement and Motivation in Adolescence. Poster presented at the Undergraduate Research Conference, Kent, OH.\n\nSkills & Coursework\n\n\n\n\n\n\n\n\nSkills\nCoursework\n\n\nStatistical Analysis Software: R, SPSS,PROCESS, SAS, Excel, Mplus, LIWC, SQL\n\nExperience using SQL, R, Python, SAS, Mplus, and SPSS for predictive data modeling and analyses\nExpertise in qualitative data collection and analysis procedures using natural language processing and textual analysis with programs like R and LIWC\nWed design using HMTL, YAML and CSS\n\nData Collection Software: Qualtrics, Survey Monkey, Cloud Research\n\nData sets ranged from a few dozen to over 50,000 sampled individuals\n\n\nAfrican American Psychology\nDiversity in Organizations\nGroups and Teams\nIndustrial Motivation and Morale\nLeadership\nLearning\nMultilevel Modeling in R\nResearch Methods\nSocial Psychology\nStructural Equation Modeling\nAdvanced Psychological Tests and Measurements\nIndustrial Organizational Psychology\nMultivariate and Computational Methods in Psychology\nOrganizational Psychology\nPerformance Feedback and Evaluations\nPersonnel Selection and Advanced App Test ISS\nPsychological Research: Quantitative and Computational Methods 1\nPsychological Research: Quantitative and Computational Methods 2\nRole of Attitudes and Values in Industrial Organizational Psychology\nTraining"
  },
  {
    "objectID": "Resume.html#conference-presentations-posters",
    "href": "Resume.html#conference-presentations-posters",
    "title": "Resume / CV",
    "section": "Conference Presentations & Posters",
    "text": "Conference Presentations & Posters"
  },
  {
    "objectID": "Resume.html#skills-coursework",
    "href": "Resume.html#skills-coursework",
    "title": "Resume / CV",
    "section": "Skills & Coursework",
    "text": "Skills & Coursework"
  },
  {
    "objectID": "Mainblog.html",
    "href": "Mainblog.html",
    "title": "Matthew Swanson’s Blog",
    "section": "",
    "text": "Welcome To My Blog!\n\n\n1 min\n\n\n\nIntro\n\n\n1st Post\n\n\n\n\n\n\n\nMatthew Swanson\n\n\nApr 25, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog pages/Mainblog.html",
    "href": "blog pages/Mainblog.html",
    "title": "Matthew Swanson’s Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "blog pages/Welcome Blog.html",
    "href": "blog pages/Welcome Blog.html",
    "title": "Welcome To My Blog!",
    "section": "",
    "text": "Hello and welcome to my first post! As a passionate enthusiast of of Industrial Organizational Psychology, data analysis, and coding, I am thrilled to share my insights, experiences, and knowledge through this platform. Whether you’re a fellow professional in the field or simply curious about these topics, this blog aims to provide engaging and informative content that will ignite your curiosity and spark insightful discussions. So, grab your favorite beverage, settle in, and let’s embark on this exciting journey together!\n\nI believe my next blog post will discuss the importance of contrast coding and its application to people analytics. Stay tuned for that!"
  },
  {
    "objectID": "Portfolio pages/Repeated Measures.html",
    "href": "Portfolio pages/Repeated Measures.html",
    "title": "Does Birth Order Impact SAT Scores?",
    "section": "",
    "text": "The overarching goal of this project is to teach readers how to run a repeated measures ANOVA in R. I will walk through the code below but readers should be familiar with R code prior to working through this code.\nThis data contains SAT scores for six families, with each family consisting of an older, middle, and younger child. Higher values indicate better SAT scores. I am interested in assessing if birth order in a family with three children effects the child’s SAT score.\nFirst I will need to read in the data and a few packages. I only do a bit of cleaning at this stage (i.e., reformatting the dataframe) so prior data cleaning should have already been completed.\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ez)\nlibrary(tidyr)\nlibrary(tidyverse)\n\nhead(Data)\n\n# A tibble: 6 × 4\n  family young middle oldest\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1      1    54     55     65\n2      2    48     63     59\n3      3    57     54     67\n4      4    61     61     62\n5      5    63     60     61\n6      6    52     54     55\nIf you look at the data file, you will notice that participants are listed column wise and not row wise since the same family has multiple evaluation points (i.e., three children in a family). I need to flip this so that the data is in long form.\n#Need to convert data set to long format - it is currently in wide format\n\nDataLong &lt;- Data %&gt;%\ngather(key = \"order\", value = \"score\", young, middle, oldest) \n#I called these new variables order and score (you can name them what you like so long as it makes sense). \n\n#Here you want to gather by the grouping factor and make sure the values by group are contained in a new variable, row-wise\n\nDataLong$family &lt;- as.factor(DataLong$family)\nDataLong$order &lt;- as.factor(DataLong$order)\nDataLong\n\n# A tibble: 30 × 3\n   family order score\n   &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt;\n 1 1      young    54\n 2 2      young    48\n 3 3      young    57\n 4 4      young    61\n 5 5      young    63\n 6 6      young    52\n 7 7      young    43\n 8 8      young    69\n 9 9      young    50\n10 10     young    58\n# ℹ 20 more rows\nCheck how the long version of this data set and the original data set compare. This should help signal how I converted the data to long form and why I had to do so. Next, I want to check that the design is balanced. Unbalanced designs typically violate important assumptions of equal variance (among other issues) so great caution should be taken when conducting unbalanced repeated measures ANOVA.\nDataLong %&gt;%\ngroup_by(order) %&gt;%\nsummarise(n = n(), sd = sd(score), var = var(score))\n\n# A tibble: 3 × 4\n  order      n    sd   var\n  &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 middle    10  6.09  37.1\n2 oldest    10  8.63  74.4\n3 young     10  7.71  59.4\nEach cell has the same n size so this design is balanced. Now I am going to run this repeated measures ANOVA three different ways to get different information and so that subsequent functions have the correct object to run. Let’s assess if birth order in a family with three children effects the child’s SAT score.\nRepeated &lt;- ezANOVA(DataLong, dv = score, wid = family, within = order, detailed = TRUE, return_aov = TRUE)\n\nRepeated\n\n$ANOVA\n       Effect DFn DFd         SSn       SSd          F            p p&lt;.05\n1 (Intercept)   1   9 98957.63333 1267.3667 702.731675 7.476640e-10     *\n2       order   2  18    95.26667  270.7333   3.166954 6.630493e-02      \n         ges\n1 0.98469487\n2 0.05832534\n\n$`Mauchly's Test for Sphericity`\n  Effect       W         p p&lt;.05\n2  order 0.96018 0.8499836      \n\n$`Sphericity Corrections`\n  Effect       GGe      p[GG] p[GG]&lt;.05      HFe      p[HF] p[HF]&lt;.05\n2  order 0.9617049 0.06888936           1.217684 0.06630493          \n\n$aov\n\nCall:\naov(formula = formula(aov_formula), data = data)\n\nGrand Mean: 57.43333\n\nStratum 1: family\n\nTerms:\n                Residuals\nSum of Squares   1267.367\nDeg. of Freedom         9\n\nResidual standard error: 11.8667\n\nStratum 2: family:order\n\nTerms:\n                    order Residuals\nSum of Squares   95.26667 270.73333\nDeg. of Freedom         2        18\n\nResidual standard error: 3.878239\nEstimated effects may be unbalanced\n\nModel &lt;- aov(score ~ order + Error(family/order), DataLong) \n#Because order is crossed with the random factor family (i.e., each order exists in a particular family), you must specify the error term for order, which in this case is family by order. Do this by adding the term Error(family/order) to the factor order, as shown above.\n\nsummary(Model)\n\n\nError: family\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals  9   1267   140.8               \n\nError: family:order\n          Df Sum Sq Mean Sq F value Pr(&gt;F)  \norder      2  95.27   47.63   3.167 0.0663 .\nResiduals 18 270.73   15.04                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlibrary(rstatix)\nSphereCorr &lt;- anova_test(DataLong, dv = score, wid = family, within = order)\nSphereCorr\n\nANOVA Table (type III tests)\n\n$ANOVA\n  Effect DFn DFd     F     p p&lt;.05   ges\n1  order   2  18 3.167 0.066       0.058\n\n$`Mauchly's Test for Sphericity`\n  Effect    W    p p&lt;.05\n1  order 0.96 0.85      \n\n$`Sphericity Corrections`\n  Effect   GGe      DF[GG] p[GG] p[GG]&lt;.05   HFe      DF[HF] p[HF] p[HF]&lt;.05\n1  order 0.962 1.92, 17.31 0.069           1.218 2.44, 21.92 0.066\nResults indicate that there is no order effect here on SAT scores, F(2, 18) = 3.17, p = .066 (effect is marginal so we will interpret it for the sake of this example).\nSome notes on the output: GES refers to the generalized eta square value. W refers to the Machly’s W statistic, a non-significant p value indicates that sphericity assumption is met. GGe is the greenhouse-geisser epsilon value and HFe is the Huynh-Feldt epsilon value (use these tests if the spherictiy assumption isn’t met). If you want the correct df for either epsilon value, look at the output from the “anova_test” function.\nNote that sphericity output is only provided when you have at least 3 factors for an IV (the results are the same for 2 levels of a factor so no output is provided).\n# Post Hoc Testing\nlibrary(emmeans)\n\nCell_Means &lt;- emmeans(Model, ~ order)\nCell_Means\n\n order  emmean   SE df lower.CL upper.CL\n middle   57.0 2.39 13     51.8     62.2\n oldest   59.8 2.39 13     54.6     65.0\n young    55.5 2.39 13     50.3     60.7\n\nWarning: EMMs are biased unless design is perfectly balanced \nConfidence level used: 0.95 \n\npwc &lt;- DataLong %&gt;%\npairwise_t_test(score ~ order, paired = TRUE,p.adjust.method = \"bonferroni\")\n\npwc\n\n# A tibble: 3 × 10\n  .y.   group1 group2    n1    n2 statistic    df     p p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 score middle oldest    10    10    -1.51      9 0.164 0.492 ns          \n2 score middle young     10    10     0.841     9 0.422 1     ns          \n3 score oldest young     10    10     2.76      9 0.022 0.066 ns\nUsing the pairwise t test code above, I can compare levels of the factor to each other just like in previous weeks. For example, t-test results indicate that the oldest siblings had marginally larger SAT scores (M = 59.8) than the youngest siblings (M = 55.5) within each family t(9) = 2.76, p = .07. Again, none of these results meet traditional p-value cutoff scores, so we would conclude that SAT scores did not depend on birth order.\nIn this data, children were ordered by birth and grouped within family and SAT values were compared between children. However, all data were collected at one time point. How might this data be analyzed if each child had multiple SAT scores? I cover how to conduct a between and within repeated measures ANOVA in another project already posted in my portfolio tab so feel free to check it out if you are curious!"
  },
  {
    "objectID": "Portfolio pages/NLPSentiment.html",
    "href": "Portfolio pages/NLPSentiment.html",
    "title": "NLP Sentiment Analysis",
    "section": "",
    "text": "For this project, I am working with a large scale qualitative data set of responses to a research question by adult employees in the United States. There are over 50,000 words housed in several hundred rows of participant responses and it is of interest to the researcher (me) to understand if there are unique words used in one particular condition. It was also of interest to determine if the characteristics of participant’s shaped what words they utilized to respond to the experimental prompt.\nI decided to use Natural Language Processing, in particular sentiment analysis, to analyze the words present in the data set. I also decided to set my tokenization parameter at the word level.\nFirst, I need to load in the data and set the tokenization of the text corpus.\nlibrary(tidytext)\nlibrary(readxl)\nlibrary(tidyverse)\nglimpse(HealthResp)\n\nRows: 595\nColumns: 2\n$ ProlificID             &lt;chr&gt; \"5e8ea20ad1aa0d164f407023\", \"6029785ce4c6ff34c7…\n$ Healthy_Work_Condition &lt;chr&gt; NA, NA, NA, \"Last week one of the employees hus…\n\nHealthResp_tidy &lt;- HealthResp %&gt;%\nunnest_tokens(output = word, input = Healthy_Work_Condition, token = \"words\")\nhead(HealthResp_tidy)\n\n# A tibble: 6 × 2\n  ProlificID               word \n  &lt;chr&gt;                    &lt;chr&gt;\n1 5e8ea20ad1aa0d164f407023 &lt;NA&gt; \n2 6029785ce4c6ff34c77a50ea &lt;NA&gt; \n3 60380d3c5275b918ad26e8b5 &lt;NA&gt; \n4 5d9e9204340c7700150ab74c last \n5 5d9e9204340c7700150ab74c week \n6 5d9e9204340c7700150ab74c one\nI can see that the tokenization was successful since each word has been separated as its own row, grouped at the participant level. Now lets look at some basic things such as word frequencies and counts.\nhead(HealthResp_tidy %&gt;%\ncount(word, sort= TRUE), n = 10)\n\n# A tibble: 10 × 2\n   word      n\n   &lt;chr&gt; &lt;int&gt;\n 1 to     1008\n 2 and     988\n 3 i       952\n 4 the     746\n 5 a       732\n 6 my      562\n 7 was     489\n 8 of      410\n 9 me      407\n10 that    376\nAs revealed in the tibble above, the 10 most used words are articles and connectives, and likely will not be of use to my analysis. I want to remove these words but also take a conservative approach so that I don’t remove too many words that might be of interest. The tidytext package has a data set called “stop_words” containing a lexicon of words comprised of article words such as “to”, “and”, “the” which I would like to remove.\ndata(\"stop_words\")\n\nHealthResp_tidy &lt;- anti_join(HealthResp_tidy, stop_words)\n\nHealthResp_Freq &lt;- HealthResp_tidy %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  filter(word != \"NA\")\n\nhead(HealthResp_Freq, n = 10)\n\n# A tibble: 10 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 time          110\n 2 job            95\n 3 environment    87\n 4 feel           83\n 5 team           77\n 6 healthy        74\n 7 stands         72\n 8 boss           67\n 9 day            65\n10 moment         56\nNow the most frequent words better reflect words of interest to my sentiment analysis. Before moving forward, I visualized the frequency of words that occurred over 50 times in participant’s responses.\nlibrary(ggthemes)\nHealthResp_Freq %&gt;%\n  filter(n &gt; 50) %&gt;%\n  filter(word != \"NA\") %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n    ggplot(aes(word, n)) +\n    geom_bar(stat = \"identity\") +\n    xlab(NULL) +\n    coord_flip() +\n    ggtitle(\"Most Commonly Used Words\\n in Healthy Condition\") +\n    theme_clean()\nI also created a word cloud visualization of these words, setting the parameters to a max of 50 words used in the visualization.\nlibrary(wordcloud)\nlibrary(RColorBrewer)\n\nHealthResp_Freq %&gt;%\nwith(wordcloud(word, n, max.words = 100, colors = brewer.pal(12, \"Paired\"), scale=c(3.5,0.25)))\nYou may have noticed that I have only been analyzing words from one condition, called healthy. It is now time to repeat the above analysis for the second condition, called unhealthy.\nglimpse(UnhealthResp)\n\nRows: 595\nColumns: 2\n$ ProlificID               &lt;chr&gt; \"5e8ea20ad1aa0d164f407023\", \"6029785ce4c6ff34…\n$ Unhealthy_Work_Condition &lt;chr&gt; \"My job involves processing payroll, however,…\n\nUnhealthResp_tidy &lt;- UnhealthResp %&gt;%\n  unnest_tokens(output = word, input = Unhealthy_Work_Condition, token = \"words\") %&gt;%\n    anti_join(stop_words, by = \"word\")\n\nUnhealthResp_Freq &lt;- UnhealthResp_tidy %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  filter(word != \"NA\")\nhead(UnhealthResp_Freq, 10)\n\n# A tibble: 10 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 job           149\n 2 time          106\n 3 feel          102\n 4 environment    79\n 5 unhealthy      76\n 6 people         74\n 7 stands         73\n 8 boss           72\n 9 supervisor     65\n10 manager        61\nI will now generate some visualizations of this condition before comparing the two conditions.\nUnhealthResp_Freq %&gt;%\n  filter(n &gt; 50) %&gt;%\n  filter(word != \"NA\") %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n    ggplot(aes(word, n)) +\n    geom_bar(stat = \"identity\") +\n    xlab(NULL) +\n    coord_flip() +\n    ggtitle(\"Most Commonly Used Words\\n in Unhealthy Condition\") +\n    theme_clean()\n\n\n\n\n\n\n\nUnhealthResp_Freq %&gt;%\nwith(wordcloud(word, n, max.words = 100, colors = brewer.pal(9, \"Set1\"), scale=c(3.5,0.25)))\nI’ve looked at each condition separately, now I will merge the two cleaned data sets back together and run analyses on this new data set.\nnames(HealthResp_tidy)\n\n[1] \"ProlificID\" \"word\"      \n\nnames(UnhealthResp_tidy) #Both data sets now have the same names for all variables in the same order\n\n[1] \"ProlificID\" \"word\"      \n\nwordfreq &lt;- bind_rows(mutate(HealthResp_tidy, condition = \"Healthy\"),mutate(UnhealthResp_tidy, condition = \"Unhealthy\"))\n\nwordfreqtotal &lt;- wordfreq %&gt;%\n  count(condition, word) %&gt;%\n  group_by(condition) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  select(condition, word, proportion) %&gt;%\n  spread(condition, proportion) %&gt;%\n    filter(Healthy &gt; .002 | Unhealthy &gt; .002)\n\nhead(wordfreqtotal, 10)\n\n# A tibble: 10 × 3\n   word         Healthy Unhealthy\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 ago         0.00333   0.00325 \n 2 angry       0.000513  0.00302 \n 3 bad         0.00154   0.00290 \n 4 boss        0.00859   0.00835 \n 5 call        0.00128   0.00209 \n 6 called      0.00179   0.00255 \n 7 care        0.00359   0.00104 \n 8 cared       0.00218   0.000696\n 9 client      0.000897  0.00290 \n10 comfortable 0.00205   0.00139\nThis new data frame contains three columns: the first column contains the utilized words, the second column is the proportion that a particular word appears across all words used in the healthy condition, the third column is the same as the second column except the proportion is compared to words used in the unhealthy condition. Like above, it is important to visualize these words in conjunction to each other.\nggplot(data = wordfreqtotal, mapping = aes(x = Healthy, y = Unhealthy, label = word)) +\n  scale_x_log10() + scale_y_log10() +\n  geom_text(alpha = .7, size = 3) +\n  geom_abline(lty = 2) +\n  theme_few()\nWords closer to the diagonal line appear at similar rates in both conditions while words farther out to the top-left or bottom-right occur more in the unhealthy or healthy condition, respectively. For example, words like coworker, manager, job, and COVID are right on the diagonal line, indicating that they occurred for similar proportions in both conditions. This makes sense as the original prompt presented to participants asked about their job experiences in 2020-21. However, words like care, supportive, and understanding occurred more frequently in the healthy condition while words like angry, uncomfortable, and bad occurred more often in the unhealthy condition.\nNow that I have a good feel about the sorts of words that participants used to describe their work experiences across and within conditions, I want to analyze the word tokens for emotional valance (i.e., the general perception of positive or negative contained within the words used in each condition).\nlibrary(textdata)\n\nnrc &lt;- get_sentiments(\"nrc\")\nAFINN &lt;- get_sentiments(\"afinn\")\nBING &lt;- get_sentiments(\"bing\")\n\nsort(unique(nrc$sentiment)) #Use this to see the unique codes that are present in the nrc sentiment column\n\n [1] \"anger\"        \"anticipation\" \"disgust\"      \"fear\"         \"joy\"         \n [6] \"negative\"     \"positive\"     \"sadness\"      \"surprise\"     \"trust\"       \n\nwordfreq &lt;- bind_rows(mutate(HealthResp_tidy, condition = \"Healthy\"),\n  mutate(UnhealthResp_tidy, condition = \"Unhealthy\"))%&gt;%\n    count(condition, word) %&gt;%\n    group_by(condition) %&gt;%\n  mutate(proportion = n / sum(n))\nThe nrc, afinn, and bing sentiments compare a corpus of words to their predefined emotions or word valance lexicons. For example, evaluating corpus sentiments using the nrc column evaluates the degree of emotions or feelings like anger, anticipation, and disgust present in the analyzed corpus. You can find more detailed information about the nrc sentiment data frame here.\nBecause I want to provide evidence that the experimental conditions produced differently valanced words in either condition, determining how positive or negative the overall words used in either condition is important. I will first analyze by a particular emotion and then will use all three popular sentiment lexicons to get a more complete understanding of the sentiments that participants had in both conditions.\n#Look at the proportion of fear based words per condition\n\ncondition_fear &lt;- nrc %&gt;%\n  filter(sentiment == \"fear\") %&gt;%\n    inner_join(wordfreq, by = \"word\") %&gt;%\n    arrange(desc(proportion))\nhead(condition_fear, 20)\n\n# A tibble: 20 × 5\n   word      sentiment condition     n proportion\n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;\n 1 unhealthy fear      Unhealthy    76   0.00882 \n 2 feeling   fear      Healthy      27   0.00346 \n 3 bad       fear      Unhealthy    25   0.00290 \n 4 pandemic  fear      Healthy      14   0.00179 \n 5 feeling   fear      Unhealthy    15   0.00174 \n 6 pandemic  fear      Unhealthy    15   0.00174 \n 7 bad       fear      Healthy      12   0.00154 \n 8 difficult fear      Healthy      11   0.00141 \n 9 worry     fear      Healthy      10   0.00128 \n10 insecure  fear      Unhealthy    10   0.00116 \n11 change    fear      Healthy       9   0.00115 \n12 difficult fear      Unhealthy     9   0.00104 \n13 fire      fear      Unhealthy     9   0.00104 \n14 hospital  fear      Healthy       8   0.00103 \n15 hostile   fear      Unhealthy     8   0.000928\n16 nervous   fear      Healthy       7   0.000897\n17 punished  fear      Unhealthy     6   0.000696\n18 afraid    fear      Healthy       5   0.000641\n19 honest    fear      Healthy       5   0.000641\n20 medical   fear      Healthy       5   0.000641\nJust by looking at the fear emotion alone, I have determined that both conditions use fear words pretty regularly. This likely captures some of the COVID anxiety that participants described in their responses across both conditions. However, looking at the proportion of these words, the most used fear word, “unhealthy”, occurs three or more times as much as any other word and occurred at this rate in the unhealthy condition. While expected, this does provide evidence that participants are at least paying attention to the instruction prompts in their respective conditions.\nNow I will expand this analysis to all sentiment categories contained in the nrc lexicon and provide some visualizations of this data.\n#Distribution of sentiments across both conditions using nrc (this uses positive/negative/ and 8 emotion words)\n\ndist_sentiments &lt;- nrc %&gt;%\n  inner_join(wordfreq, by = \"word\") %&gt;%\n  group_by(condition, sentiment) %&gt;%\n  summarize(n = sum(n)) %&gt;%\n  mutate(prop = n/sum(n)) %&gt;%\n  arrange(desc(prop))\ndist_sentiments\n\n# A tibble: 20 × 4\n# Groups:   condition [2]\n   condition sentiment        n   prop\n   &lt;chr&gt;     &lt;chr&gt;        &lt;int&gt;  &lt;dbl&gt;\n 1 Healthy   positive      1232 0.283 \n 2 Unhealthy positive      1002 0.197 \n 3 Unhealthy negative       941 0.185 \n 4 Healthy   trust          714 0.164 \n 5 Healthy   anticipation   517 0.119 \n 6 Healthy   negative       469 0.108 \n 7 Unhealthy trust          528 0.104 \n 8 Unhealthy sadness        501 0.0986\n 9 Unhealthy fear           457 0.0900\n10 Unhealthy anticipation   448 0.0882\n11 Healthy   joy            377 0.0865\n12 Unhealthy anger          438 0.0862\n13 Unhealthy disgust        350 0.0689\n14 Healthy   sadness        299 0.0686\n15 Healthy   fear           270 0.0619\n16 Unhealthy joy            254 0.05  \n17 Healthy   anger          189 0.0434\n18 Healthy   surprise       169 0.0388\n19 Unhealthy surprise       161 0.0317\n20 Healthy   disgust        123 0.0282\n\nggplot(data = dist_sentiments, mapping = aes(x = condition, y = prop, fill = sentiment)) +\ngeom_bar(stat = \"identity\")\nResults of this sentiment analysis support the effectiveness of the experiment: that is, participants in the healthy condition provided a greater proportion of positively-valanced words than participants in the unhealthy condition (28.3% vs. 19.7% of words), and participants provided a greater proportion of negatively-valanced words in the unhealthy condition than those in the healthy condition (18.7% vs. 10.8%). Glancing at the ggplot also highlights a greater proportion of anger, disgust, fear and sadness words in the unhealthy condition and a greater proportion of anticipation, joy, and trust words in the healthy condition.\nFor the sake of robustness, I will also analyse the corpus of words using the BING and AFINN lexicons. The results should converge on the same point but may be slightly different as different words are used to create each specific lexicon.\n#Look at different sentiment lexicons - BING (just positive/negative)\n\ndist_sentimentsbing &lt;- BING %&gt;%\n  inner_join(wordfreq, by = \"word\") %&gt;%\n  group_by(condition, sentiment) %&gt;%\n  summarize(n = sum(n)) %&gt;%\n  mutate(prop = n/sum(n))\ndist_sentimentsbing\n\n# A tibble: 4 × 4\n# Groups:   condition [2]\n  condition sentiment     n  prop\n  &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Healthy   negative    525 0.414\n2 Healthy   positive    744 0.586\n3 Unhealthy negative   1097 0.758\n4 Unhealthy positive    351 0.242\n\nggplot(data = dist_sentimentsbing, mapping = aes(x = condition, y = prop, fill = sentiment)) +\ngeom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n#AFINN lexicon - This uses a scale ranging from 5- -5 with higher values indicating more positively associated words; 0 is neutral\n\ndist_sentimentsAFINN &lt;- AFINN %&gt;%\n  inner_join(wordfreq, by = \"word\") %&gt;%\n  group_by(condition, value) %&gt;%\n  summarize(n = sum(n)) %&gt;%\n  mutate(prop = n/sum(n))\n  dist_sentimentsAFINN\n\n# A tibble: 17 × 4\n# Groups:   condition [2]\n   condition value     n     prop\n   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n 1 Healthy      -4     2 0.00177 \n 2 Healthy      -3    74 0.0655  \n 3 Healthy      -2   212 0.188   \n 4 Healthy      -1    92 0.0814  \n 5 Healthy       1   195 0.173   \n 6 Healthy       2   443 0.392   \n 7 Healthy       3    95 0.0841  \n 8 Healthy       4    17 0.0150  \n 9 Unhealthy    -4    11 0.00828 \n10 Unhealthy    -3   155 0.117   \n11 Unhealthy    -2   606 0.456   \n12 Unhealthy    -1   174 0.131   \n13 Unhealthy     1   137 0.103   \n14 Unhealthy     2   202 0.152   \n15 Unhealthy     3    35 0.0263  \n16 Unhealthy     4     8 0.00602 \n17 Unhealthy     5     1 0.000752\n\nggplot(data = dist_sentimentsAFINN, mapping = aes(x = condition, y = prop, fill = value)) +\ngeom_bar(stat = \"identity\")\nJust as I predicted, the results converge on the same point! An additional piece of information gleamed from the AFINN sentiment analysis is that no participant in either condition utilized verbiage that could be classified as extremely positive (a score of 5 in AFINN) or extremely negative (a score of -5 in AFINN).\nggplot(dist_sentimentsAFINN, aes(x = value, fill = condition)) + \n  geom_density(alpha = 0.5) + \n  theme_gdocs() +\n  ggtitle(\"AFINN Score Densities\")\n\n\n\n\n\n\n\nlibrary(reshape2)\n\nwordfreq %&gt;%\n  inner_join(get_sentiments(\"bing\")) %&gt;%\n  count(word, sentiment, sort = TRUE) %&gt;%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %&gt;%\n  comparison.cloud(colors = c(\"red\", \"blue\"),\n                   max.words = 200, scale=c(0.50,0.50), title.size = 1.5)"
  },
  {
    "objectID": "Portfolio pages/NLPSentiment.html#visualization",
    "href": "Portfolio pages/NLPSentiment.html#visualization",
    "title": "NLP Sentiment Analysis",
    "section": "Visualization",
    "text": "Visualization"
  },
  {
    "objectID": "Portfolio pages/BWRepeart.html",
    "href": "Portfolio pages/BWRepeart.html",
    "title": "Between and Within Repeated Measures ANOVA in R",
    "section": "",
    "text": "A researcher is interested in how memory for a list of words can be influenced by instructions on how to process the words. She assigns participants to one of three instruction conditions (COND): no instructions (coded as 1), rote memorization (told to just rehearse each word, coded as 2), and image (told to form an image for each word, coded as 3). Participants are presented with a list of 30 words and the researcher records the number out of 30 words that each participant recalls correctly. Because she is interested in practice effects, she presents each subject with two lists, one after another, and records performance on each list (LIST1 and LIST2).\nFirst, I will read in the data and check the data type (i.e., is the data wide or long?).\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ez)\nlibrary(tidyr)\nlibrary(tidyverse)\n\nhead(Data)\n\n# A tibble: 6 × 4\n  subjid  cond list1 list2\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1     1    21    25\n2      2     1    11    12\n3      3     1    15    20\n4      4     1    11    12\n5      5     1    18    15\n6      6     1    12    12\n\n#Need to convert data set to long format - it is currently in wide format\n\n#Changing data to long format - we want to gather by list as this is the repeated variable\nDataLong &lt;- Data %&gt;%\ngather(key = \"list\", value = \"score\", list1, list2)\nDataLong$list &lt;- as.factor(DataLong$list)\nDataLong$cond &lt;- as.factor(DataLong$cond)\nDataLong$subjid &lt;- as.factor(DataLong$subjid)\nhead(DataLong)\n\n# A tibble: 6 × 4\n  subjid cond  list  score\n  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;\n1 1      1     list1    21\n2 2      1     list1    11\n3 3      1     list1    15\n4 4      1     list1    11\n5 5      1     list1    18\n6 6      1     list1    12\nNow, let’s double check that the design is balanced.\nDataLong %&gt;%\ngroup_by(list, cond) %&gt;%\nsummarise(n = n(), sd = sd(score), var = var(score))\n\n# A tibble: 6 × 5\n# Groups:   list [2]\n  list  cond      n    sd   var\n  &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 list1 1        10  3.33 11.1 \n2 list1 2        10  2.59  6.71\n3 list1 3        10  4.06 16.5 \n4 list2 1        10  4.37 19.1 \n5 list2 2        10  2.42  5.88\n6 list2 3        10  3.73 13.9\nWe have a balanced design. Now let’s analyze the data. We are interested in how memory for a list of words can be influenced by instructions on how to process the words.\nlibrary(rstatix)\nRepeated &lt;- ezANOVA(DataLong, dv = score, wid = subjid, within = list, between = cond, detailed = TRUE, return_aov = TRUE, type = 3)\nRepeated\n\n$ANOVA\n       Effect DFn DFd         SSn    SSd          F            p p&lt;.05\n1 (Intercept)   1  27 13053.75000 584.35 603.150937 5.272153e-20     *\n2        cond   2  27   206.40000 584.35   4.768375 1.684833e-02     *\n3        list   1  27   198.01667  73.95  72.298174 4.077683e-09     *\n4   cond:list   2  27    32.53333  73.95   5.939148 7.283889e-03     *\n         ges\n1 0.95199113\n2 0.23869550\n3 0.23124234\n4 0.04709288\n\n$aov\n\nCall:\naov(formula = formula(aov_formula), data = data)\n\nGrand Mean: 14.75\n\nStratum 1: subjid\n\nTerms:\n                  cond Residuals\nSum of Squares  206.40    584.35\nDeg. of Freedom      2        27\n\nResidual standard error: 4.65216\n2 out of 4 effects not estimable\nEstimated effects may be unbalanced\n\nStratum 2: subjid:list\n\nTerms:\n                     list cond:list Residuals\nSum of Squares  198.01667  32.53333  73.95000\nDeg. of Freedom         1         2        27\n\nResidual standard error: 1.654959\nEstimated effects may be unbalanced\n\n#Additional ways to estimate this model\nModel &lt;- aov(score ~ list*cond + Error(subjid/list), DataLong)\n#summary(Model)\n\n#SphereCorr &lt;- anova_test(DataLong, dv = score, wid = subjid, within = list, between = cond)\n#SphereCorr\n\nlibrary(ggpubr)\n\nbxp &lt;- ggboxplot(DataLong, x = \"list\", y = \"score\",color = \"cond\", palette = \"jco\")\nbxp\nSince the interaction term is significant, F(2,27) = 5.94, p = .047, that means that values in one factor depend, in part, on the values in the other factor so we must look at pairwise comparisons (simple effects and simple effects comparison). As denoted in the figure and output, there are some significant mean memory score differences between list 1 and list 2 of words for each condition.\nLet’s directly calculate these means and the marginal means to aid in our interpretation of this data.\nlibrary(emmeans)\n\n#the pairs function comes from the emmeans package - verbiage should feel similar to emmeans commands in SPSS\nSimpleCond &lt;- emmeans(Model, pairwise ~ cond, adjust = \"bonferroni\")\nSimpleCond\n\n$emmeans\n cond emmean   SE df lower.CL upper.CL\n 1      15.8 1.04 27     13.6     17.9\n 2      12.2 1.04 27     10.0     14.3\n 3      16.4 1.04 27     14.2     18.5\n\nResults are averaged over the levels of: list \nWarning: EMMs are biased unless design is perfectly balanced \nConfidence level used: 0.95 \n\n$contrasts\n contrast      estimate   SE df t.ratio p.value\n cond1 - cond2      3.6 1.47 27   2.447  0.0636\n cond1 - cond3     -0.6 1.47 27  -0.408  1.0000\n cond2 - cond3     -4.2 1.47 27  -2.855  0.0245\n\nResults are averaged over the levels of: list \nP value adjustment: bonferroni method for 3 tests \n\nSimpleList &lt;- emmeans(Model, pairwise ~ list, adjust = \"bonferroni\")\nSimpleList\n\n$emmeans\n list  emmean    SE   df lower.CL upper.CL\n list1   12.9 0.637 33.7     11.6     14.2\n list2   16.6 0.637 33.7     15.3     17.9\n\nResults are averaged over the levels of: cond \nWarning: EMMs are biased unless design is perfectly balanced \nConfidence level used: 0.95 \n\n$contrasts\n contrast      estimate    SE df t.ratio p.value\n list1 - list2    -3.63 0.427 27  -8.503  &lt;.0001\n\nResults are averaged over the levels of: cond\nLet’s interpret a few of these results. Across lists, participants told to form an image of the words in the mind recalled significantly greater words (M = 16.4) than those who practice rote memorization of the words (M = 12.2), t(27) = -2.86, p = 03. Additionally, the practice effect has strongest for participants in the rote memory condition as more words were recalled in the second list (M = 14.9) as compared to the first list (M = 9.4), t(27) = -7.43, p &lt;.001. In other words, participants recalled more words overall when asked to assign an image to each word, however, they learned the best (i.e., the practice effect was the strongest) when they were asked to recall words via rote memory."
  },
  {
    "objectID": "Portfolio pages/FactorialANOVA.html",
    "href": "Portfolio pages/FactorialANOVA.html",
    "title": "Religion and Politics Factorial ANOVA in R",
    "section": "",
    "text": "A client requested to better understand the religious attendance of their constituents and how both their constituent’s race and political identity shape religious attendance (with higher scores indicating greater attendance to religious gatherings/events). Additionally, the client specifically requested to compare White, Black, and Hispanic constituents. Thus, the goal of this project was to clean the data and understand how identity shapes religious attendance.\nFirst, the data needs to be cleaned:\nlibrary(readxl)\nlibrary(dplyr)\n\nData &lt;- subset(Data, PARTYID!= 1 & PARTYID!= 2 & PARTYID!= 3 & PARTYID!= 4 & PARTYID!= 5 & PARTYID!= 9)\nData &lt;- subset(Data, ATTEND!= 9)\nData %&gt;%\nmutate(RACECEN1 = case_when(RACECEN1 == 1 ~ 1,\n                            RACECEN1 ==2 ~ 2,\n                            RACECEN1 ==16 ~ 3,\n                            RACECEN1 ==3 ~ 4,\n                            RACECEN1 ==4 ~ 4,\n                            RACECEN1 == 5 ~ 4,\n                            RACECEN1 ==6 ~ 4,\n                            RACECEN1 ==7 ~ 4,\n                            RACECEN1 ==8 ~ 4,\n                            RACECEN1 ==9 ~ 4,\n                            RACECEN1 == 10 ~ 4,\n                            RACECEN1 ==11 ~ 4,\n                            RACECEN1 ==12 ~ 4,\n                            RACECEN1 ==13 ~ 4,\n                            RACECEN1 ==14 ~ 4,\n                            RACECEN1 ==15 ~ 4,\n                            TRUE ~ 99)) -&gt; Data\n\nData &lt;- subset(Data, RACECEN1!= 99)\nData$PARTYID &lt;- as.factor(Data$PARTYID)\nData$RACECEN1 &lt;- as.factor(Data$RACECEN1)\nData$RACECEN1 &lt;- recode_factor(Data$RACECEN1, '1' = \"White\", '2' = \"Black\", '3' = \"Hispanic\", '4' = \"Other\")\nData$PARTYID &lt;- recode_factor(Data$PARTYID, '0' = \"Democratic\", '6' = \"Republican\", '7' = \"Other\")\nstr(Data)\n\ntibble [797 × 3] (S3: tbl_df/tbl/data.frame)\n $ ATTEND  : num [1:797] 6 0 5 3 6 8 5 2 8 0 ...\n $ PARTYID : Factor w/ 3 levels \"Democratic\",\"Republican\",..: 1 2 1 2 1 1 1 1 1 1 ...\n $ RACECEN1: Factor w/ 4 levels \"White\",\"Black\",..: 2 1 2 1 2 3 2 1 1 1 ...\n\nhead(Data)\n\n# A tibble: 6 × 3\n  ATTEND PARTYID    RACECEN1\n   &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;   \n1      6 Democratic Black   \n2      0 Republican White   \n3      5 Democratic Black   \n4      3 Republican White   \n5      6 Democratic Black   \n6      8 Democratic Hispanic\nThe original “Religion Data 2” has several values in each factor that we do not want to look at (either coded as missing or an ambiguous code like “independent but more democratic”). I was also asked to look at Black, White, Hispanic individuals as compared to the rest of the participants so I needed to write code that recoded the race variable into something more meaningful (there is originally 16 levels in this variable - some with just single digit cell sizes). I also wanted to assign labels to each level of the factor to aid in clarity.\nNow, let’s build a two-way ANOVA model and look at the output.\nAov &lt;- aov(ATTEND ~ RACECEN1*PARTYID, Data)\nsummary(Aov)\n\n                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nRACECEN1           3    228   76.12  10.384 1.05e-06 ***\nPARTYID            2    392  195.88  26.721 5.93e-12 ***\nRACECEN1:PARTYID   6     91   15.14   2.066    0.055 .  \nResiduals        785   5754    7.33                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResults of the two-way ANOVA indicate both a significant main effect for race, F(3,785) = 10.38, p &lt;.001, and for political party, F(2,785) = 26.721, p&lt;.001, and also a marginally significant interaction, F(6,785) = 2.07, p = .06. For the sake of this client’s project, I am going to determine that the marginal p value is close enough to our arbitrary alpha level so I will go ahead and look at the simple effects.\nFirst, I want to get the condition means and the cell means.\nlibrary(emmeans)\n\nCondition_Means &lt;- emmeans(Aov, ~ RACECEN1*PARTYID)\nCondition_Means\n\n RACECEN1 PARTYID    emmean    SE  df lower.CL upper.CL\n White    Democratic   2.75 0.172 785    2.417     3.09\n Black    Democratic   4.88 0.212 785    4.461     5.29\n Hispanic Democratic   4.59 0.577 785    3.458     5.72\n Other    Democratic   3.44 0.541 785    2.377     4.50\n White    Republican   4.44 0.173 785    4.104     4.78\n Black    Republican   4.83 0.782 785    3.299     6.37\n Hispanic Republican   5.00 1.910 785    1.242     8.76\n Other    Republican   4.44 0.902 785    2.673     6.22\n White    Other        2.60 0.365 785    1.883     3.32\n Black    Other        2.50 0.957 785    0.621     4.38\n Hispanic Other        1.33 1.560 785   -1.735     4.40\n Other    Other        4.50 1.350 785    1.843     7.16\n\nConfidence level used: 0.95 \n\nMarginal_MeansRace &lt;- emmeans(Aov, ~ RACECEN1)\nMarginal_MeansRace\n\n RACECEN1 emmean    SE  df lower.CL upper.CL\n White      3.27 0.146 785     2.98     3.55\n Black      4.07 0.418 785     3.25     4.89\n Hispanic   3.64 0.846 785     1.98     5.30\n Other      4.13 0.572 785     3.01     5.25\n\nResults are averaged over the levels of: PARTYID \nConfidence level used: 0.95 \n\nMarginal_MeansParty &lt;- emmeans(Aov, ~ PARTYID)\nMarginal_MeansParty\n\n PARTYID    emmean    SE  df lower.CL upper.CL\n Democratic   3.92 0.209 785     3.50     4.33\n Republican   4.68 0.566 785     3.57     5.79\n Other        2.73 0.577 785     1.60     3.87\n\nResults are averaged over the levels of: RACECEN1 \nConfidence level used: 0.95\nYou can also get these means using the dplyr. However, you have to be cautious on the marginal mean calculation which is why I used the emmeans function.\nNow that I found a significant interaction, I want to look at the simple effects. In other words, I want to look at the relationship between one predictor on the DV, across all levels of the other predictor. The testinteractions function in the phia package allows me to do this. Note that I have to convert the aov model to a lm model to fit the first argument of this function.\nlibrary(phia)\n\nLM &lt;- lm(ATTEND ~ RACECEN1*PARTYID, Data)\nTest1 &lt;- testInteractions(LM, fixed = \"RACECEN1\", across = \"PARTYID\", adjustment = \"bonferroni\")\nTest1\n\nF Test: \nP-value adjustment method: bonferroni\n          PARTYID1 PARTYID2    SE1    SE2 Df Sum of Sq       F    Pr(&gt;F)    \n   White    0.1540   1.8431   0.40    0.4  2    401.34 27.3745 1.287e-11 ***\n   Black    2.3773   2.3333   0.98    1.2  2     43.15  2.9430    0.2132    \nHispanic    3.2576   3.6667   1.67    2.5  2     29.20  1.9917    0.5486    \n   Other   -1.0600  -0.0556   1.46    1.6  2      8.93  0.6093    1.0000    \nResiduals                   785.00 5754.5                                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTest2 &lt;- testInteractions(LM, fixed = \"PARTYID\", across = \"RACECEN1\", adjustment = \"bonferroni\")\nTest2\n\nF Test: \nP-value adjustment method: bonferroni\n           RACECEN11 RACECEN12 RACECEN13    SE1    SE2     SE3 Df Sum of Sq\nDemocratic  -0.68597   1.43730    1.1509   0.57    0.6 0.79147  3    465.09\nRepublican  -0.00136   0.38889    0.5556   0.92    1.2 2.11655  3      2.32\n     Other  -1.90000  -2.00000   -3.1667   1.40    1.7 2.06789  3     19.20\nResiduals                                785.00 5754.5                     \n                 F    Pr(&gt;F)    \nDemocratic 21.1483 1.052e-12 ***\nRepublican  0.1056         1    \n     Other  0.8733         1    \nResiduals                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResults indicate that the effect of political party within White individuals is significant, F(2,785) = 27.38, p &lt;.001. Results also indicate that the effect of race within Democratic individuals is significant, F(3,785) = 21.15, p &lt;.001. Since I found several significant simple effects, I can now follow it up with a simple effects comparison. This is a pairwise comparison of cell means, similar to post-hocs from a one-way ANOVA. I need to break down the multiple degree of freedom effects into single degree of freedom tests so that I know exactly where the difference lies for each effect.\nSimple &lt;- testInteractions(LM, pairwise = \"RACECEN1\", fixed = \"PARTYID\", adjustment = \"bonferroni\")\nSimple\n\nF Test: \nP-value adjustment method: bonferroni\n                              Value     SE     Df Sum of Sq       F    Pr(&gt;F)\n   White-Black : Democratic -2.1233   0.27    1.0    443.41 60.4882 4.189e-13\nWhite-Hispanic : Democratic -1.8369   0.60    1.0     68.18  9.3011   0.04261\n   White-Other : Democratic -0.6860   0.57    1.0     10.69  1.4578   1.00000\nBlack-Hispanic : Democratic  0.2864   0.61    1.0      1.59  0.2169   1.00000\n   Black-Other : Democratic  1.4373   0.58    1.0     44.78  6.1084   0.24597\nHispanic-Other : Democratic  1.1509   0.79    1.0     15.50  2.1145   1.00000\n   White-Black : Republican -0.3902   0.80    1.0      1.74  0.2377   1.00000\nWhite-Hispanic : Republican -0.5569   1.92    1.0      0.62  0.0839   1.00000\n   White-Other : Republican -0.0014   0.92    1.0      0.00  0.0000   1.00000\nBlack-Hispanic : Republican -0.1667   2.07    1.0      0.05  0.0065   1.00000\n   Black-Other : Republican  0.3889   1.19    1.0      0.78  0.1061   1.00000\nHispanic-Other : Republican  0.5556   2.12    1.0      0.51  0.0689   1.00000\n   White-Black :      Other  0.1000   1.02    1.0      0.07  0.0095   1.00000\nWhite-Hispanic :      Other  1.2667   1.61    1.0      4.56  0.6227   1.00000\n   White-Other :      Other -1.9000   1.40    1.0     13.46  1.8363   1.00000\nBlack-Hispanic :      Other  1.1667   1.83    1.0      2.97  0.4051   1.00000\n   Black-Other :      Other -2.0000   1.66    1.0     10.67  1.4551   1.00000\nHispanic-Other :      Other -3.1667   2.07    1.0     17.19  2.3450   1.00000\nResiduals                           785.00 5754.5                            \n                               \n   White-Black : Democratic ***\nWhite-Hispanic : Democratic *  \n   White-Other : Democratic    \nBlack-Hispanic : Democratic    \n   Black-Other : Democratic    \nHispanic-Other : Democratic    \n   White-Black : Republican    \nWhite-Hispanic : Republican    \n   White-Other : Republican    \nBlack-Hispanic : Republican    \n   Black-Other : Republican    \nHispanic-Other : Republican    \n   White-Black :      Other    \nWhite-Hispanic :      Other    \n   White-Other :      Other    \nBlack-Hispanic :      Other    \n   Black-Other :      Other    \nHispanic-Other :      Other    \nResiduals                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nLooking at the simple effects comparison test output, several results are significant. Specifically, the White-Black difference in religious attendance scores for democrats is significant, F(1, 785) = 60.49, p &lt;.001. Since the value is negative, I know that the mean of Black individuals is larger than the mean of White individuals. Sure enough, the mean religious attendance for White democrats (M = 2.75) is significantly smaller than the mean religious attendance for Black democrats (M = 4.88). Additionally, the White-Hispanic difference for religious attendance for democrats is significant, F(1,785) = 9.30, p = .04. The effect is also negative, so I know the mean for Hispanic democrats is larger. Indeed, the mean religious attendance scores for White democrats (M = 2.75) is smaller than the mean religious attendance scores for Hispanic democrats (M = 4.59). I can also look at more pairwise comparisons (the mean of republican Whites vs. Other Hispanics) - but this requires additional codes and considerations.\nI can also look at interactions by contrast coding for each effect in both main effects and their interactions. This is especially applicable since both main effects and the interaction are significant (while marginal). Since there is a total of 12 groups (3X4 design) I need 11 contrast coded predictors. Race has 4 levels so it needs 3 contrast coded predictors and political party has 3 levels so it needs 2 contrast coded predictors. Then each effect will be crossed, resulting in 5 interaction contrast coded predictors (3 + 2 + 5 = 11). Contrast codes that are assigned to each predictor have to satisfy two conditions to be orthogonal: sum across each group equal zero and the sum of the product of all pairs equal zero (Data Analysis: A Model Comparison Approach to Regression, ANOVA, and Beyond by Judd, McClelland, & Ryan, 2017 provides excellent background on the necessity and development of contrast coding for those curious).\nData%&gt;%\nmutate(x1 = case_when(RACECEN1 ==\"White\" ~ -3,\n                      RACECEN1 ==\"Black\" ~ 1,\n                      RACECEN1 ==\"Hispanic\" ~ 1,\n                      RACECEN1 == \"Other\" ~ 1,\n                      TRUE ~ 99),\nx2 = case_when(RACECEN1 ==\"White\" ~ 0,\n                      RACECEN1 ==\"Black\" ~ -2,\n                      RACECEN1 ==\"Hispanic\" ~ 1,\n                      RACECEN1 == \"Other\" ~ 1,\n                      TRUE ~ 99),\nx3 = case_when(RACECEN1 == \"White\" ~ 0,\n                      RACECEN1 == \"Black\" ~ 0,\n                      RACECEN1 == \"Hispanic\" ~ -1,\n                      RACECEN1 == \"Other\" ~ 1,\n                      TRUE ~ 99),\nx4 = case_when(PARTYID == \"Democratic\" ~ -2,\n                      PARTYID == \"Republican\" ~ 1,\n                      PARTYID == \"Other\" ~ 1,\n                      TRUE ~ 99),\nx5 = case_when(PARTYID == \"Democratic\" ~ 0,\n                      PARTYID == \"Republican\" ~ -1,\n                      PARTYID == \"Other\" ~ 1,\n                      TRUE ~ 99),\nx6 = x1 * x4,\nx7 = x1 * x5,\nx8 = x2 * x4,\nx9 = x2 * x5,\nx10 = x3 * x4,\nx11 = x3 * x5) -&gt; Data\n\nAov2 &lt;- aov(ATTEND ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11, Data)\nsummary(Aov2)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nx1            1    195  195.24  26.634 3.12e-07 ***\nx2            1     30   29.62   4.041  0.04475 *  \nx3            1      3    3.48   0.475  0.49080    \nx4            1    169  169.40  23.109 1.83e-06 ***\nx5            1    222  222.36  30.333 4.93e-08 ***\nx6            1     53   52.62   7.179  0.00753 ** \nx7            1      0    0.11   0.015  0.90155    \nx8            1      9    8.87   1.209  0.27180    \nx9            1      1    1.38   0.188  0.66474    \nx10           1     16   16.28   2.220  0.13660    \nx11           1     12   11.60   1.582  0.20880    \nResiduals   785   5754    7.33                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nNeed to be clear on what each contrast coded predictor codes for:\nx1: difference between White and the average of Black, Hispanic, and other races, across political party\nx2: difference between Black and the average of Hispanic and other races, across political party\nx3: difference between Hispanic and other races, across political party\nx4: difference between democratic and the average of republican and other, across race\nx5: difference between republican and other, across race\nx6-x11: the interaction of the above contrasts\nI won’t interpret all effects that are significant, but let’s look at a few. x1 codes the difference between White individuals and the average of Black, Hispanic, and other races. This effect is significant, F(1,785) = 26.63, p &lt;.001. Since I am comparing White individuals to the average of all other groups across political party, I need to average together some means. The marginal mean for White individuals is 3.27, now we need to get the means for the other racial groups. The average of the other 9 conditions (Black/Hispanic/Other for all 3 political parties) is 3.95 (this is also the mean of the three marginal means of race not including White). So then, x1 is coding the difference between 3.27 and 3.95, which is significant, providing evidence that White individuals attended religious institutions significantly less than the three other racial groups.\nx2 codes the difference between democrats and the average of both republicans and others, across race. Looking at marginal means, this effect is coding the difference between a mean of 3.92 for democrats, and a mean of 3.71 for both republicans and others. This effect is significant, suggesting that democrats attended religious institutions significantly more than both republicans and others, F(1,785) = 23.11, p &lt;.001.\nx6 codes if the difference between White and the average of Black, Hispanic, and other races and for the difference between democrats and the average of republicans and others. The difference for democrats can be found by getting the difference between mean of White dems and the average of the three other racial groups = this difference is -1.55. This value is being compared to a difference between mean of White republican and other and the average of Black, Hispanic, and other individuals in both republic and other groups - this value is -.25. In other words, Whites were less likely to go to religious attendance than the three other racial groups, and this is more true for White democrats, as compared to White republicans and others."
  },
  {
    "objectID": "Portfolio pages/FactorialANOVA.html#solution-1",
    "href": "Portfolio pages/FactorialANOVA.html#solution-1",
    "title": "Religion and Politics Factorial ANOVA in R",
    "section": "Solution 1",
    "text": "Solution 1"
  },
  {
    "objectID": "Portfolio pages/FactorialANOVA.html#solution-2",
    "href": "Portfolio pages/FactorialANOVA.html#solution-2",
    "title": "Religion and Politics Factorial ANOVA in R",
    "section": "Solution 2",
    "text": "Solution 2"
  },
  {
    "objectID": "Portfolio pages/FactorPower.html",
    "href": "Portfolio pages/FactorPower.html",
    "title": "Factorial Anova and Power Analysis in R",
    "section": "",
    "text": "It is of interest to a client to analyze data from a clinical trial on tooth growth. It is of particular interest to ascertain how different dosage amounts and types of supplements influence tooth growth. First, I will need to load the data set and calculate data descriptives and run some ANOVAs.\nLoading the data:\ndata(\"ToothGrowth\")\nhead(ToothGrowth)\n\n   len supp dose\n1  4.2   VC  0.5\n2 11.5   VC  0.5\n3  7.3   VC  0.5\n4  5.8   VC  0.5\n5  6.4   VC  0.5\n6 10.0   VC  0.5\nNeed cell means and variances next.\nlibrary(dplyr)\nToothGrowth %&gt;%\ngroup_by(supp, dose)%&gt;%\nsummarize(group_mean = mean(len), n = n(), var = var(len)) \n\n# A tibble: 6 × 5\n# Groups:   supp [2]\n  supp   dose group_mean     n   var\n  &lt;fct&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 OJ      0.5      13.2     10 19.9 \n2 OJ      1        22.7     10 15.3 \n3 OJ      2        26.1     10  7.05\n4 VC      0.5       7.98    10  7.54\n5 VC      1        16.8     10  6.33\n6 VC      2        26.1     10 23.0\nGlancing at these cell means indicates a potential pattern. Namely, that across supplement, as dosage increases, tooth length increases. It also appears that across dosage, the OJ supplement produces longer teeth growth than the VC supplement. However, to determine if these mean differences are significant, I need to conduct a factorial ANOVA.\n#Need to ensure that R reads factor variables as factors first\n\nToothGrowth$supp &lt;- as.factor(ToothGrowth$supp)\nToothGrowth$dose &lt;- as.factor(ToothGrowth$dose)\nFactorANOVA &lt;- aov(len ~ dose*supp, ToothGrowth) #you can also just add in dose*supp and R understands to include the interaction and both main effects in the model - nice way to shorten code\n\nsummary(FactorANOVA)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndose         2 2426.4  1213.2  92.000  &lt; 2e-16 ***\nsupp         1  205.3   205.3  15.572 0.000231 ***\ndose:supp    2  108.3    54.2   4.107 0.021860 *  \nResiduals   54  712.1    13.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Calculating Marginal Means\n\nlibrary(emmeans)\n\nCondition_Means &lt;- emmeans(FactorANOVA, ~ supp*dose)\nCondition_Means\n\n supp dose emmean   SE df lower.CL upper.CL\n OJ   0.5   13.23 1.15 54    10.93     15.5\n VC   0.5    7.98 1.15 54     5.68     10.3\n OJ   1     22.70 1.15 54    20.40     25.0\n VC   1     16.77 1.15 54    14.47     19.1\n OJ   2     26.06 1.15 54    23.76     28.4\n VC   2     26.14 1.15 54    23.84     28.4\n\nConfidence level used: 0.95 \n\nMarginal_MeansSupp &lt;- emmeans(FactorANOVA, ~ supp)\nMarginal_MeansSupp\n\n supp emmean    SE df lower.CL upper.CL\n OJ     20.7 0.663 54     19.3     22.0\n VC     17.0 0.663 54     15.6     18.3\n\nResults are averaged over the levels of: dose \nConfidence level used: 0.95 \n\nMarginal_MeansDose &lt;- emmeans(FactorANOVA, ~ dose)\nMarginal_MeansDose\n\n dose emmean    SE df lower.CL upper.CL\n 0.5    10.6 0.812 54     8.98     12.2\n 1      19.7 0.812 54    18.11     21.4\n 2      26.1 0.812 54    24.47     27.7\n\nResults are averaged over the levels of: supp \nConfidence level used: 0.95\nA quick glance at the output highlights that both the main effects of dose and supplement are significant as well as their interaction. Across supplement type, the means of tooth length were significantly different among three dosage conditions: 0.5 dosage (M = 10.61), 1.0 dosage (M = 19.74), and 2.0 dosage (M = 26.10), F(2,54) = 92.00, p &lt;.001. There was also a statistically significant difference in the average tooth length between supplement type, across dosage: OJ (M = 20.66) and VC condition (M = 16.96), F(1,54) = 15.57, p &lt;.001. The interaction between dosage amount and supplement type was significant, F(2,54) = 4.11, p =.02.\nGiven the significant interaction, I know that the effects of one variable depends on the levels of the other variable. One way to understand this effect is to look at the simple effects.\nlibrary(phia)\n\nlm1 &lt;- lm(len ~ dose*supp, ToothGrowth)\n\nTest1 &lt;- testInteractions(lm1, fixed = \"dose\", across = \"supp\")\nTest1\n\nF Test: \nP-value adjustment method: holm\n          Value     SE     Df Sum of Sq       F   Pr(&gt;F)   \n0.5        5.25  1.624   1.00   137.813 10.4505 0.004185 **\n  1        5.93  1.624   1.00   175.824 13.3330 0.001769 **\n  2       -0.08  1.624   1.00     0.032  0.0024 0.960893   \nResiduals       54.000 712.11                              \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTest2 &lt;- testInteractions(lm1, fixed = \"supp\", across = \"dose\")\nTest2\n\nF Test: \nP-value adjustment method: holm\n           dose1 dose2    SE1    SE2 Df Sum of Sq      F    Pr(&gt;F)    \nOJ        -12.83 -3.36  1.624   1.62  2    885.26 33.565 3.363e-10 ***\nVC        -18.16 -9.37  1.624   1.62  2   1649.49 62.541 1.751e-14 ***\nResiduals              54.000 712.11                                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncontrast(Condition_Means, \"revpairwise\", by= \"dose\", adjust = \"none\")\n\ndose = 0.5:\n contrast estimate   SE df t.ratio p.value\n VC - OJ     -5.25 1.62 54  -3.233  0.0021\n\ndose = 1:\n contrast estimate   SE df t.ratio p.value\n VC - OJ     -5.93 1.62 54  -3.651  0.0006\n\ndose = 2:\n contrast estimate   SE df t.ratio p.value\n VC - OJ      0.08 1.62 54   0.049  0.9609\nNote here the significance levels: for example, the effects of supplement within 0.5 & 1.0 dosage levels are significant. Additionally, the effects of dosage within OJ and VC supplement are also significant. Looking at the contrast estimates, it becomes clear that For both dosage levels of 0.5 and 1.0, teeth grow significantly less when the patient is also provided a vitamin C supplement, compared to the teeth of patients provided an orange juice supplement, t(54) = -3.23, p =.002 and t(54) = -3.65, p &lt;.001 respectively.\nNow let’s collect both biased and unbiased estimates of effect size for our three effects in this model. This will help clarify the magnitude of effect of both dosage and supplement type.\nlibrary(lsr)\nlibrary(effectsize)\n\nEta &lt;- etaSquared(FactorANOVA, type = 2, anova = TRUE)\nEta\n\n              eta.sq eta.sq.part       SS df         MS         F            p\ndose      0.70286419   0.7731092 2426.434  2 1213.21717 91.999965 0.0000000000\nsupp      0.05948365   0.2238254  205.350  1  205.35000 15.571979 0.0002311828\ndose:supp 0.03137672   0.1320279  108.319  2   54.15950  4.106991 0.0218602690\nResiduals 0.20627544          NA  712.106 54   13.18715        NA           NA\n\nomega_squared(FactorANOVA, partial = FALSE, ci = 0.95)\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Omega2 |       95% CI\n---------------------------------\ndose      |   0.69 | [0.57, 1.00]\nsupp      |   0.06 | [0.00, 1.00]\ndose:supp |   0.02 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\nomega_squared(FactorANOVA, partial = TRUE, ci = 0.95)\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Omega2 (partial) |       95% CI\n-------------------------------------------\ndose      |             0.75 | [0.65, 1.00]\nsupp      |             0.20 | [0.06, 1.00]\ndose:supp |             0.09 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\nThe eta squared value associated with the interaction between dosage and supplement is .03. In other words, the interaction term explains approximately 3% of the variance in tooth length. Its’ respective partial eta squared value is .13, indicating that the interaction term explains about 13% of the unique variance in tooth length, accounting for any shared variance with dosage and supplement. The interaction term’s omega squared and partial omega squared values are slightly smaller as these effect sizes have been corrected for bias but the interpretation stays the same.\nNow, I will calculate power for this analysis.\nlibrary(pwr)\n\nf &lt;- sqrt(0.031376 / (1-0.031376))\npwr.anova.test(k = 6, n = 10, f = f, sig.level = .95)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 6\n              n = 10\n              f = 0.1799787\n      sig.level = 0.95\n          power = 0.9778168\n\nNOTE: n is number in each group\nI have about a 98% power level to detect the interaction effect in our data, should the effect truly exist. What about the main effect of supplement?\nf &lt;- sqrt(0.05948365 / (1-0.05948365))\npwr.anova.test(k = 2, n = 30, f = f, sig.level = .95)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 2\n              n = 30\n              f = 0.2514871\n      sig.level = 0.95\n          power = 0.9924828\n\nNOTE: n is number in each group\nI have a 99% power level to detect the effect of supplement, should the effect exist.\nThe client also requested some visualizations, so I will create a bar and line graph of the above group means.\nlibrary(ggplot2)\n\nggplot(ToothGrowth, aes(x =as.factor(dose), y = len, fill = as.factor(supp))) + \n  stat_summary(fun='mean',geom='bar', position = \"dodge\") +\n  labs(x = \"Dosage\", y = \"Tooth Length\", fill = \"Supplement Type\", title = \"Figure 1\", subtitle = \"Mean Tooth Length Scores by Dosage and Supplement\")+\n  theme(plot.title = element_text(face = \"bold\"),\n        plot.subtitle = element_text(face = \"italic\")) +\n  scale_fill_manual(values = c(\"#D71920\", \"#002266\"))\n\n\n\n\n\n\n\ninteraction.plot(x.factor = ToothGrowth$dose, #x-axis variable\n                 trace.factor = ToothGrowth$supp, #variable for lines\n                 response = ToothGrowth$len, #y-axis variable\n                 fun = mean, #metric to plot\n                 ylab = \"Tooth Length\",\n                 xlab = \"Dosage\",\n                 col = c(\"pink\", \"blue\"),\n                 lty = 1, #line type\n                 lwd = 2, #line width\n                 trace.label = \"Supplement Type\")"
  },
  {
    "objectID": "PortMain.html",
    "href": "PortMain.html",
    "title": "Portfolio Projects",
    "section": "",
    "text": "Check out my portfolio and research projects!\n\n\n\n\n\n\n\n\n\n\n\n\n\nBetween and Within Repeated Measures ANOVA in R\n\n\n3 min\n\n\n\nRepeated Measures ANOVA\n\n\nR\n\n\nEmmeans\n\n\n\n\n\n\n\nMatthew Swanson\n\n\n\n\n\n\n\n\n\n\n\n\nDoes Birth Order Impact SAT Scores?\n\n\n4 min\n\n\n\nRepeated Measures ANOVA\n\n\nR\n\n\nemmeans\n\n\nData Conversion\n\n\n\n\n\n\n\nMatthew Swanson\n\n\n\n\n\n\n\n\n\n\n\n\nEmployee Turnover: A Survival Analysis\n\n\n7 min\n\n\n\nSurvival Analysis\n\n\nTurnover\n\n\nR\n\n\nVisualizations\n\n\nsurvminer\n\n\n\n\n\n\n\nMatthew Swanson\n\n\n\n\n\n\n\n\n\n\n\n\nEngagement and Diversity\n\n\n12 min\n\n\n\nEngagement\n\n\nDiversity\n\n\nR\n\n\nVisualizations\n\n\nConfirmatory Factor Analysis\n\n\n\n\n\n\n\nMatthew Swanson\n\n\n\n\n\n\n\n\n\n\n\n\nFactorial Anova and Power Analysis in R\n\n\n5 min\n\n\n\nFactorial ANOVA\n\n\nR\n\n\nPower Analysis\n\n\nEffect Size\n\n\nClinical Trial Data\n\n\n\n\n\n\n\nMatthew Swanson\n\n\n\n\n\n\n\n\n\n\n\n\nNLP Sentiment Analysis\n\n\n9 min\n\n\n\nNatural Language Processing\n\n\nSentiment Analysis\n\n\nR\n\n\nVisualizations\n\n\nTokenization\n\n\n\n\n\n\n\nMatthew Swanson\n\n\n\n\n\n\n\n\n\n\n\n\nPerforming Moderation Analysis Within Multilevel Meta-Analysis\n\n\n4 min\n\n\n\nMultilevel Meta-Analysis\n\n\nR\n\n\nOrganizational Training\n\n\nModeration\n\n\nEngagement\n\n\nData Science\n\n\n\n\n\n\n\nMatthew Swanson\n\n\n\n\n\n\n\n\n\n\n\n\nPsychometric Properties of Self Esteem via SEM and IRT\n\n\n7 min\n\n\n\nStructural Equation Modeling\n\n\nItem Response Theory\n\n\nR\n\n\nlavaan\n\n\n\n\n\n\n\nMatthew Swanson\n\n\n\n\n\n\n\n\n\n\n\n\nReligion and Politics Factorial ANOVA in R\n\n\n12 min\n\n\n\nFactorial ANOVA\n\n\nR\n\n\nReligion\n\n\nConstrast Coding\n\n\nphia\n\n\n\n\n\n\n\nMatthew Swanson\n\n\n\n\n\n\n\n\n\n\n\n\nTesting for Quadratic Effects in Multilevel Meta-Analysis\n\n\n5 min\n\n\n\nMultilevel Meta-Analysis\n\n\nR\n\n\nQuadratic Effects\n\n\nOrganizational Training\n\n\nEngagement\n\n\nData Science\n\n\n\n\n\n\n\nMatthew Swanson\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Multilevel Meta-Analysis to Better Understand Complex Business Research\n\n\n19 min\n\n\n\nMultilevel Meta-Analysis\n\n\nR\n\n\nOrganizational Training\n\n\nEngagement\n\n\nData Science\n\n\n\n\n\n\n\nMatthew Swanson\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Multilevel Modeling To Understand Engagement, Autonomy, and Team Cohesion\n\n\n6 min\n\n\n\nMultilevel Modeling\n\n\nR\n\n\nlavaan\n\n\nICC1 & 2\n\n\nEngagement\n\n\nTeam Cohesion\n\n\nInter-rater Reliability\n\n\nSimple Slopes\n\n\n\n\n\n\n\nMatthew Swanson\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "DataViz.html",
    "href": "DataViz.html",
    "title": "Data Viz Portfolio",
    "section": "",
    "text": "Here is a collection of data visualizations that I have conducted. Many of these visualizations I created in one of my portfolio projects so feel free to check them out to see how I coded these graphs and my interpretations!\n\n\n\n\n\n \n\n                  \n\n\n\n\n\n\n\n\n\n\nLinear and Quadratic Trend Lines for Time (in Months) Since Post-Test Training in Multi-Level Meta-Analysis | © Matthew Swanson 2025\n\n\n\n\n\n\n\nFunnel Plot of Multi-Level Meta-Analysis Effects | © Matthew Swanson 2025\n\n\n\n\n\n\n\nForest Plot for Multi-Level Meta-Analysis | © Matthew Swanson 2025\n\n\n\n\n\n\n\n\n\nWord Tokens by Condition | © Matthew Swanson 2023\n\n\n\n\n\n\n\nMultilevel Interaction | © Matthew Swanson 2023\n\n\n\n\n\n\n\nStructural Data Modeling | © Matthew Swanson 2023\n\n\n\n\n\n\n\n\n\nSurvival Analysis Grouped by Coaching and Commuting Preference | © Matthew Swanson 2023\n\n\n\n\n\n\n\nSentiment Analysis Density Plot | © Matthew Swanson 2023\n\n\n\n\n\n\n\nANOVA Box Plot | © Matthew Swanson 2023\n\n\n\n\n\n\n\n\n\nFactorial ANOVA Bar Plot | © Matthew Swanson 2023\n\n\n\n\n\n\n\nSurvival Analysis | © Matthew Swanson 2023\n\n\n\n\n\n\n\nWord Cloud Grouped by Valance | © Matthew Swanson 2023\n\n\n\n\n\n\n\n\n\nNLP Sentiment Analysis Frequency by Condition | © Matthew Swanson 2023"
  },
  {
    "objectID": "Portfolio pages/Turnover.html",
    "href": "Portfolio pages/Turnover.html",
    "title": "Employee Turnover: A Survival Analysis",
    "section": "",
    "text": "Employees choose to leave organizations for many reasons. Thus, it is sometimes important for Human Resources and People Analytic departments to identify variables that may influence the likelihood that employees will turnover.\nUsing this real employee dataset shared by Edward Babushkin and housed on Kaggle, I will analyze several factors that influence the likelihood that an employee will leave their company. Also, a big thank you to Keith McNulty, Alex LoPilato, and Liz Romero for their people analytics course on github.\nData columns:\nFirst, I will load in the data and make any necessary corrections.\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(psych)\n\n\n#Need to change string data to factors\nTurnData$event &lt;- factor(TurnData$event, labels = c(\"no\", \"yes\"))\nnames &lt;- c('gender', 'industry', 'profession', 'traffic', 'coach', 'head_gender', 'greywage', 'way')\nTurnData[,names] &lt;- lapply(TurnData[,names], factor)\nTurnData &lt;- TurnData  %&gt;%\n    dplyr::mutate(\n    Coach = dplyr::case_when(\n      coach == \"no\" ~ \"no\",\n      coach == \"yes\" ~ \"yes\",\n      coach == \"my head\" ~ \"no\"))\nTurnData$Coach &lt;- as.factor(TurnData$Coach)\nstr(TurnData)\n\n'data.frame':   1129 obs. of  17 variables:\n $ stag        : num  7.03 22.97 15.93 15.93 8.41 ...\n $ event       : Factor w/ 2 levels \"no\",\"yes\": 2 2 2 2 2 2 2 2 2 2 ...\n $ gender      : Factor w/ 2 levels \"f\",\"m\": 2 2 1 1 2 1 1 1 1 1 ...\n $ age         : num  35 33 35 35 32 42 42 28 29 30 ...\n $ industry    : Factor w/ 16 levels \" HoReCa\",\"Agriculture\",..: 3 3 11 11 13 8 8 13 3 5 ...\n $ profession  : Factor w/ 15 levels \"Finan\\xf1e\",\"Accounting\",..: 8 8 8 8 4 8 8 8 8 12 ...\n $ traffic     : Factor w/ 8 levels \"advert\",\"empjs\",..: 5 2 5 5 8 2 2 7 2 8 ...\n $ coach       : Factor w/ 3 levels \"my head\",\"no\",..: 2 2 2 2 3 3 3 2 2 3 ...\n $ head_gender : Factor w/ 2 levels \"f\",\"m\": 1 2 2 2 1 2 2 2 1 2 ...\n $ greywage    : Factor w/ 2 levels \"grey\",\"white\": 2 2 2 2 2 2 2 2 2 2 ...\n $ way         : Factor w/ 3 levels \"bus\",\"car\",\"foot\": 1 1 1 1 1 1 1 1 1 1 ...\n $ extraversion: num  6.2 6.2 6.2 5.4 3 6.2 6.2 3.8 8.6 5.4 ...\n $ independ    : num  4.1 4.1 6.2 7.6 4.1 6.2 6.2 5.5 6.9 5.5 ...\n $ selfcontrol : num  5.7 5.7 2.6 4.9 8 4.1 4.1 8 2.6 3.3 ...\n $ anxiety     : num  7.1 7.1 4.8 2.5 7.1 5.6 5.6 4 4 7.9 ...\n $ novator     : num  8.3 8.3 8.3 6.7 3.7 6.7 6.7 4.4 7.5 8.3 ...\n $ Coach       : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 2 2 2 1 1 2 ...\nFactors have been made - now I will look at the descriptives of the data.\nlibrary(ggplot2)\nDescriptives &lt;- describe(TurnData)\nDescriptives\n\n             vars    n  mean    sd median trimmed   mad   min    max  range\nstag            1 1129 36.63 34.10  24.34   31.05 24.50  0.39 179.45 179.06\nevent*          2 1129  1.51  0.50   2.00    1.51  0.00  1.00   2.00   1.00\ngender*         3 1129  1.24  0.43   1.00    1.18  0.00  1.00   2.00   1.00\nage             4 1129 31.07  7.00  30.00   30.54  7.41 18.00  58.00  40.00\nindustry*       5 1129  9.00  4.07   8.00    9.04  5.93  1.00  16.00  15.00\nprofession*     6 1129  8.21  2.39   8.00    8.13  0.00  1.00  15.00  14.00\ntraffic*        7 1129  5.00  2.42   5.00    5.04  4.45  1.00   8.00   7.00\ncoach*          8 1129  1.84  0.61   2.00    1.80  0.00  1.00   3.00   2.00\nhead_gender*    9 1129  1.52  0.50   2.00    1.52  0.00  1.00   2.00   1.00\ngreywage*      10 1129  1.89  0.32   2.00    1.98  0.00  1.00   2.00   1.00\nway*           11 1129  1.50  0.68   1.00    1.38  0.00  1.00   3.00   2.00\nextraversion   12 1129  5.59  1.85   5.40    5.58  2.37  1.00  10.00   9.00\nindepend       13 1129  5.48  1.70   5.50    5.48  2.08  1.00  10.00   9.00\nselfcontrol    14 1129  5.60  1.98   5.70    5.59  2.22  1.00  10.00   9.00\nanxiety        15 1129  5.67  1.71   5.60    5.63  2.22  1.70  10.00   8.30\nnovator        16 1129  5.88  1.90   6.00    5.94  2.22  1.00  10.00   9.00\nCoach*         17 1129  1.12  0.32   1.00    1.02  0.00  1.00   2.00   1.00\n              skew kurtosis   se\nstag          1.49     2.02 1.01\nevent*       -0.02    -2.00 0.01\ngender*       1.19    -0.59 0.01\nage           0.66     0.06 0.21\nindustry*    -0.03    -1.28 0.12\nprofession*   0.26     2.52 0.07\ntraffic*     -0.03    -1.50 0.07\ncoach*        0.10    -0.43 0.02\nhead_gender* -0.07    -2.00 0.01\ngreywage*    -2.45     4.00 0.01\nway*          1.00    -0.23 0.02\nextraversion  0.01    -0.40 0.06\nindepend     -0.01    -0.34 0.05\nselfcontrol   0.03    -0.63 0.06\nanxiety       0.15    -0.48 0.05\nnovator      -0.25    -0.48 0.06\nCoach*        2.38     3.67 0.01\n\nsum(is.na(TurnData))\n\n[1] 0\nSince each data point represents an employee who has either left the organization or is currently employed, and there are no missing data points, I will move on to model building and running some analyses on the data.\nFirst, I conducted a survival analysis and related the survival object to several of the variables that were measured during the employee on-boarding period.\nlibrary(survival)\nlibrary(survminer)\nlibrary(lubridate)\nlibrary(ggfortify)\n# Create a censor indicator variable that is 1 if the employee has left and 0 otherwise\nTurnData &lt;-\n  TurnData %&gt;%\n    dplyr::mutate(\n    CENSOR = dplyr::case_when(\n      event == \"no\" ~ 0,\n      TRUE ~ 1))\n\n# Create a survival object\nsurv_object &lt;- survival::Surv(\n  event = TurnData$CENSOR,\n  time = TurnData$stag\n)\n\n# Estimate survival probabilities using Kaplan Meier estimator\nSurvProb &lt;- survival::survfit(\n  surv_object ~ 1,\n  data = TurnData)\nhead(fortify(SurvProb),10)\n\n        time n.risk n.event n.censor      surv      std.err     upper     lower\n1  0.3942505   1129       1        0 0.9991143 0.0008861321 1.0000000 0.9973805\n2  0.4271047   1128       1        0 0.9982285 0.0012537359 1.0000000 0.9957786\n3  0.4928131   1127       0        3 0.9982285 0.0012537359 1.0000000 0.9957786\n4  0.5256674   1124       0        1 0.9982285 0.0012537359 1.0000000 0.9957786\n5  0.6899384   1123       0        1 0.9982285 0.0012537359 1.0000000 0.9957786\n6  0.7556468   1122       1        0 0.9973388 0.0015384787 1.0000000 0.9943360\n7  0.7885010   1121       0        1 0.9973388 0.0015384787 1.0000000 0.9943360\n8  0.8870637   1120       0        1 0.9973388 0.0015384787 1.0000000 0.9943360\n9  0.9199179   1119       1        0 0.9964476 0.0017793961 0.9999288 0.9929784\n10 1.1170431   1118       2        0 0.9946650 0.0021838533 0.9989316 0.9904167\n\nSurvProbcalc &lt;- \n  tibble::tibble(\n    time = SurvProb$time,\n    n.risk = SurvProb$n.risk,\n    n.censor = SurvProb$n.censor,\n    n.event = SurvProb$n.event,\n    survival = SurvProb$surv\n  )\n\nsum(SurvProbcalc$n.event)\n\n[1] 571\n\nsum(SurvProbcalc$n.censor)\n\n[1] 558\n\nmin(which(SurvProbcalc$n.censor != 0))\n\n[1] 3\nFYI: To reduce the length of the printed data, I only asked for the first 10 rows but a row is created until the “surv” column reaches zero so this output can become quite long.\nThe first item I notice from this output is that turnover occurs at a relatively rapid rate at this company (this interpretation may slightly change if the organization is a massive company - total N size is not accessable). Since the time metric is in months, by the first year, approximately 297 employees (1129 - 832) are predicted to turnover, for a survival percentage of about 85%. Additionally, I would expect only half of the employees to remain at the company about 4 years into the job (approximately 50 months).\nI also determined that 571 employees turned over at this company and there are 558 censored observations. These censored observations are participants who have not yet experienced the event of interest in this dataset: turnover. Also, the first censored participant showed up at time point three (about 2 weeks).\nNow I will visualize the Kaplan Meier estimator output.\nsurvminer::ggsurvplot(\n  SurvProb,\n  pval = TRUE,\n  conf.int = TRUE,\n  risk.table = TRUE,\n  xlab = \"Months since Hire\",\n  ylab = \"Probability of Staying at Company\"\n)\nConsidering no other explanatory variables, results of my survival analysis predicts that most employees will turnover about 175 months (14.5 years) into their tenure at the company. However, perhaps additional variables, such a the presence of a coach and commuting style better explain the tenure trajectory of employees at this company.\nSurvProb_Coach &lt;- survival::survfit(surv_object ~ Coach, data = TurnData)\nhead(fortify(SurvProb_Coach), 10)\n\n        time n.risk n.event n.censor      surv     std.err     upper     lower\n1  0.3942505    997       1        0 0.9989970 0.001003512 1.0000000 0.9970340\n2  0.4928131    996       0        3 0.9989970 0.001003512 1.0000000 0.9970340\n3  0.5256674    993       0        1 0.9989970 0.001003512 1.0000000 0.9970340\n4  0.6899384    992       0        1 0.9989970 0.001003512 1.0000000 0.9970340\n5  0.7556468    991       1        0 0.9979889 0.001423486 1.0000000 0.9952084\n6  0.7885010    990       0        1 0.9979889 0.001423486 1.0000000 0.9952084\n7  0.8870637    989       0        1 0.9979889 0.001423486 1.0000000 0.9952084\n8  1.1170431    988       2        0 0.9959687 0.002019739 0.9999192 0.9920338\n9  1.1498973    986       0        1 0.9959687 0.002019739 0.9999192 0.9920338\n10 1.1827515    985       0        2 0.9959687 0.002019739 0.9999192 0.9920338\n   strata\n1      no\n2      no\n3      no\n4      no\n5      no\n6      no\n7      no\n8      no\n9      no\n10     no\nAt a quick glance, it appears that the presence of a coach speeds up the attrition rate at this company! Perhaps the company should investigate the coaches they have employed to work with their staff as employees have a lower survival rate when paired with a coach than those who do not have a coach. However, I want a statistical test to tell the difference between these two survival curves. I will now visualize these survival rates and ask for a test of the difference in survival curves.\nsurvminer::ggsurvplot(\n  SurvProb_Coach,\n  pval = TRUE,\n  conf.int = TRUE,\n  xlab = \"Months since Hire\",\n  ylab = \"Probability of Staying at Company\"\n)\nAs is made more clear by this graph, the survival curves for those who had and did not have a coach did not significantly differ from each other, log-likelihood p = .27. I am also curious how commuting style (if the employee commuted by way of a bus, car, or by foot) impacted attrition and if commuting style interacted with coaching. First, I will change my model to include commuting style and will plot the model.\nSurvProb_Way &lt;- survival::survfit(surv_object ~ way, data = TurnData)\n#summary(SurvProb_Sup)\nsurvminer::ggsurvplot(\n  SurvProb_Way,\n  pval = TRUE,\n  conf.int = TRUE,\n  xlab = \"Months since Hire\",\n  ylab = \"Probability of Staying at Company\"\n)\nResults indicate that commuting style did significantly change the attrition estimate for employees. Since there are more than two curves, this only tells me that at least one of the curves differs from the others, but not which curves those are. I will come back to this issue in a moment.\nNow I want to see how the occurrence of coaching potentially interacts with commuting style in predicting attrition rates.\nIntModel &lt;- survival::survfit(surv_object ~ Coach + way, data = TurnData)\nsurvminer::ggsurvplot(IntModel, \n  pval = TRUE,\n  xlab = \"Months since Hire\",\n  ylab = \"Probability of Staying at Company\")\nNote here the significant log-likelihood p value. I also removed the shading for confidence intervals to aid in readability. To really understand the group differences in survivability, the Cox-Proportional Hazards Model, a semi-parametric regression model, will be utilized. I also decided to add in participant’s conscientiousness scores to the model.\nCoxMod &lt;- survival::coxph(\n  surv_object ~  Coach + way + selfcontrol + Coach*way,\n  data = TurnData)\nsummary(CoxMod)\n\nCall:\nsurvival::coxph(formula = surv_object ~ Coach + way + selfcontrol + \n    Coach * way, data = TurnData)\n\n  n= 1129, number of events= 571 \n\n                     coef exp(coef) se(coef)      z Pr(&gt;|z|)   \nCoachyes          0.20012   1.22155  0.15303  1.308  0.19097   \nwaycar           -0.22041   0.80219  0.09940 -2.217  0.02659 * \nwayfoot          -0.42755   0.65210  0.17874 -2.392  0.01675 * \nselfcontrol      -0.05915   0.94257  0.02118 -2.793  0.00522 **\nCoachyes:waycar  -0.19537   0.82253  0.29951 -0.652  0.51421   \nCoachyes:wayfoot -0.24507   0.78265  0.42224 -0.580  0.56165   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                 exp(coef) exp(-coef) lower .95 upper .95\nCoachyes            1.2215     0.8186    0.9050    1.6488\nwaycar              0.8022     1.2466    0.6602    0.9747\nwayfoot             0.6521     1.5335    0.4594    0.9257\nselfcontrol         0.9426     1.0609    0.9042    0.9825\nCoachyes:waycar     0.8225     1.2158    0.4573    1.4794\nCoachyes:wayfoot    0.7827     1.2777    0.3421    1.7905\n\nConcordance= 0.552  (se = 0.013 )\nLikelihood ratio test= 21.52  on 6 df,   p=0.001\nWald test            = 21.13  on 6 df,   p=0.002\nScore (logrank) test = 21.29  on 6 df,   p=0.002\nResults indicate that coaching style did not significantly interact with commuting type, all ps&gt;.05, nor was there any group difference for participants who had a coach vs. those who did not, b = 0.20, p = .19. However, the difference between riding the bus to work and commuting by either car, b = -0.22, p = .03, or by foot, b = -0.43, p = .02, were significant and negative. This suggests that when compared to riding the bus, driving to work reduces the hazard (of attrition) by a factor of .80 and walking to work reduces the hazard by .65. Self-control was also a significant predictor, such that every unit increase in conscientiousness reduces the hazard by a factor of .94.\nIt is my recommendation that the company evaluates the utility of their coaching program, as employees are no less likely to leave the company when they have a coach, compared to those who do not have coaches. Conscientious and commuting style stand out as significant predictors of the survival curves. Thus, it may be advantageous to the company to consider adding conscientiousness personality tests to their selection battery (when it makes sense with established job analyses), as more conscientious employees left the company at slower rates. Also, the company may want to encourage carpooling and reduce barriers to walking to work (e.g., keeping sidewalks clean; providing flexible start times so that employees have ample time to walk to work where applicable; etc.)."
  },
  {
    "objectID": "Portfolio pages/Turnover.html#about-this-data",
    "href": "Portfolio pages/Turnover.html#about-this-data",
    "title": "Employee Turnover: A Survival Analysis",
    "section": "About This Data",
    "text": "About This Data"
  },
  {
    "objectID": "Portfolio pages/Engagement and Diversity.html",
    "href": "Portfolio pages/Engagement and Diversity.html",
    "title": "Engagement and Diversity",
    "section": "",
    "text": "The overarching goal of this project is to better understand the demographic makeup of this company using their HR data and to assess how engaged employees are.\nThis HR data set was provided on Kaggle here. Data is housed in four text files:\nFirst, I want to clean and match up the data and provide some general oversights on the demographic makeup of the employees at this company.\nFirst, the data needs to be read in and then matched up using indicator keys in the data sets.\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\nMerge1 &lt;- merge(CompanyData, DiversityData, by.x = \"EmployeeID\")\nMasterData &lt;- merge(Merge1, EngagementData, by.x = \"EmployeeID\")\nnames &lt;- c('level', 'Department', 'Gender', 'Gender.Identity', 'Race.Ethnicity', 'Veteran', 'Disability', 'Education', 'Sexual.Orientation')\nMasterData[,names] &lt;- lapply(MasterData[,names], factor)\nNow to calculate some general demographic statistics.\nmean(MasterData$Age)\n\n[1] 44.18465\n\nsd(MasterData$Age)\n\n[1] 12.76855\n\nMasterData %&gt;%\n  group_by(Veteran) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(freq = n / sum(n))\n\n# A tibble: 2 × 3\n  Veteran     n   freq\n  &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;\n1 0        2688 0.951 \n2 1         139 0.0492\n\nMasterData %&gt;%\n  group_by(Disability) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(freq = n / sum(n))\n\n# A tibble: 2 × 3\n  Disability     n   freq\n  &lt;fct&gt;      &lt;int&gt;  &lt;dbl&gt;\n1 0           2696 0.954 \n2 1            131 0.0463\n\nMasterData %&gt;%\n  group_by(Race.Ethnicity) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(freq = n / sum(n))\n\n# A tibble: 10 × 3\n   Race.Ethnicity                                  n     freq\n   &lt;fct&gt;                                       &lt;int&gt;    &lt;dbl&gt;\n 1 \"\"                                            321 0.114   \n 2 \"American Indian or Alaska Native\"              1 0.000354\n 3 \"Asian\"                                       721 0.255   \n 4 \"Black or African American\"                    89 0.0315  \n 5 \"Hispanic or Latino\"                          117 0.0414  \n 6 \"Native American or Alaska Native\"              3 0.00106 \n 7 \"Native Hawaiian or Other Pacific Islander\"     2 0.000707\n 8 \"Native Hawaiian or Pacific Islander\"           7 0.00248 \n 9 \"Two or More Races\"                            31 0.0110  \n10 \"White\"                                      1535 0.543   \n\nMasterData %&gt;%\n  group_by(Gender.Identity) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(freq = n / sum(n))\n\n# A tibble: 5 × 3\n  Gender.Identity             n   freq\n  &lt;fct&gt;                   &lt;int&gt;  &lt;dbl&gt;\n1 female                   1232 0.436 \n2 male                     1298 0.459 \n3 Non-binary/third gender    75 0.0265\n4 Prefer not to say         166 0.0587\n5 Prefer to self-describe    56 0.0198\n\nMasterData %&gt;%\n  group_by(Sexual.Orientation) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(freq = n / sum(n))\n\n# A tibble: 6 × 3\n  Sexual.Orientation     n   freq\n  &lt;fct&gt;              &lt;int&gt;  &lt;dbl&gt;\n1 Bisexual              55 0.0195\n2 Gay                   60 0.0212\n3 Heterosexual        1817 0.643 \n4 Lesbian               43 0.0152\n5 Missing              749 0.265 \n6 Other LGBTQ+         103 0.0364\n\nMasterData %&gt;%\n  group_by(Education) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(freq = n / sum(n))\n\n# A tibble: 5 × 3\n  Education         n   freq\n  &lt;fct&gt;         &lt;int&gt;  &lt;dbl&gt;\n1 Graduate        197 0.0697\n2 High School     481 0.170 \n3 PhD              68 0.0241\n4 Some College    177 0.0626\n5 Undergraduate  1904 0.674\nFrom this output, I can tell that the average employee is 44 years of age, typically not a veteran (approx. %5 of employees are a veteran), and did not report a disability (approx. 5% of employees reported a disability status). Further, the organization is 55% White with Asian employees making up the second most frequent demographic at 26%. Employees are about evenly split between male and female (around 44% each), most identify as heterosexual (64%), and about 67% have at least an undergraduate degree.\nThe client is particularly interested in the gender and racial makeup of each level of their organization (i.e., individual contributor, manager, director, etc.) and asked to have some visualizations made up.\nLevelG &lt;- MasterData %&gt;%\n  group_by(level, Gender.Identity) %&gt;%\n  filter(Gender.Identity != \"Prefer not to say\")%&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(freq = n / sum(n))\nLevelR &lt;- MasterData %&gt;%\n  group_by(level, Race.Ethnicity) %&gt;%\n  filter(Race.Ethnicity != \"\")%&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(freq = n / sum(n))\n\nggdotchart(LevelG, x = \"level\", y = \"freq\",\n           color = \"Gender.Identity\",                                \n           sorting = \"asc\", sort.by.groups = TRUE,                      \n           add = \"segments\",                            \n           add.params = list(color = \"lightgray\", size = 2), \n           group = \"Gender.Identity\",                                \n           dot.size = 4,                                 \n           ggtheme = theme_pubclean()\n           )+\n  font(\"x.text\", size = 8, vjust = 0.5)\n\n\n\n\n\n\n\nggplot(LevelR, aes(fill=Race.Ethnicity, y=freq, x=level)) + \n    geom_bar(position=\"stack\", stat=\"identity\") +\n  geom_text(aes(label = scales::percent(freq, accuracy = 1)),\n            position = position_stack(vjust = .5), size = 1.5) +\n  scale_x_discrete(guide=guide_axis(n.dodge=4))\nOf particular note is that only White and Asian employees currently hold executive and C-suite positions (CSuite, SVP, & VP positions).\nNow that I have a sense of the demographic breakdown of the employees at this company, my next goal is to better understand how engaged the employees feel at work and if these engagement perceptions significantly differ depending on employee’s identities. First, I was asked to create a composite engagement variable and assess the reliability and validity of the scale. However, looking at the items, I doubt that the items will play together since the “engagement” items cover topics such as perspectives on DEI, market survivability of their company, compensation, and engagement.\nNote: none of the engagement items were reverse coded so there is no need to reverse score any items.\nlibrary(psych)\n#Create composite engagement score\nMasterData %&gt;%\n  rowwise()%&gt;%\n  mutate(Engagement = mean(c(E1, E2, E3, E4, E5, E6, E7, E8, E9, E10, E11, E12, E13, E14, E15, E16, E17, E18, E19)))-&gt; MasterData\n#Create engagement scale Reliability Analysis\nEngagementScale &lt;- select(MasterData, E1, E2, E3, E4, E5, E6, E7, E8, E9, E10, E11, E12, E13, E14, E15, E16, E17, E18, E19)\npsych::alpha(EngagementScale)\n\nSome items ( E2 E4 E7 E9 E11 E13 E14 E15 E16 ) were negatively correlated with the first principal component and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\n\n\nReliability analysis   \nCall: psych::alpha(x = EngagementScale)\n\n  raw_alpha std.alpha G6(smc) average_r    S/N   ase mean   sd median_r\n    0.0073    0.0074   0.013   0.00039 0.0074 0.027  2.9 0.23  -0.0012\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt    -0.05  0.01  0.06\nDuhachek -0.05  0.01  0.06\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r     S/N alpha se   var.r    med.r\nE1     0.0022    0.0020  0.0075   1.1e-04  0.0020    0.027 0.00033 -0.00104\nE2     0.0119    0.0119  0.0164   6.7e-04  0.0120    0.027 0.00030 -0.00083\nE3     0.0079    0.0078  0.0130   4.4e-04  0.0079    0.027 0.00033 -0.00125\nE4     0.0093    0.0093  0.0142   5.2e-04  0.0094    0.027 0.00032 -0.00116\nE5    -0.0084   -0.0085 -0.0024  -4.7e-04 -0.0085    0.028 0.00033 -0.00302\nE6     0.0033    0.0035  0.0090   1.9e-04  0.0035    0.027 0.00033 -0.00125\nE7     0.0148    0.0149  0.0196   8.4e-04  0.0151    0.027 0.00033 -0.00083\nE8     0.0190    0.0191  0.0231   1.1e-03  0.0195    0.027 0.00030 -0.00116\nE9     0.0053    0.0053  0.0107   3.0e-04  0.0053    0.027 0.00033 -0.00083\nE10   -0.0040   -0.0042  0.0015  -2.3e-04 -0.0042    0.027 0.00032 -0.00189\nE11    0.0084    0.0085  0.0137   4.8e-04  0.0086    0.027 0.00033 -0.00125\nE12    0.0156    0.0154  0.0200   8.7e-04  0.0157    0.027 0.00032 -0.00083\nE13    0.0038    0.0041  0.0095   2.3e-04  0.0041    0.027 0.00033 -0.00125\nE14    0.0121    0.0124  0.0175   7.0e-04  0.0126    0.027 0.00034 -0.00034\nE15    0.0144    0.0147  0.0194   8.3e-04  0.0149    0.027 0.00033 -0.00011\nE16    0.0018    0.0019  0.0073   1.0e-04  0.0019    0.027 0.00032 -0.00189\nE17   -0.0044   -0.0040  0.0019  -2.2e-04 -0.0040    0.027 0.00033 -0.00189\nE18    0.0190    0.0189  0.0232   1.1e-03  0.0192    0.027 0.00032 -0.00104\nE19   -0.0012   -0.0010  0.0045  -5.7e-05 -0.0010    0.027 0.00032 -0.00189\n\n Item statistics \n       n raw.r std.r   r.cor   r.drop mean   sd\nE1  2827  0.24  0.24  0.1106  0.01138  2.9 0.99\nE2  2827  0.22  0.22 -0.0537 -0.00826  2.9 1.03\nE3  2827  0.23  0.23  0.0095 -0.00028  2.9 1.00\nE4  2827  0.23  0.23 -0.0127 -0.00308  2.9 1.02\nE5  2827  0.26  0.26  0.2903  0.03292  2.9 1.00\nE6  2827  0.24  0.24  0.0843  0.00890  2.9 1.01\nE7  2827  0.21  0.21 -0.1128 -0.01433  2.9 1.01\nE8  2827  0.20  0.21 -0.1791 -0.02317  2.9 1.00\nE9  2827  0.23  0.23  0.0530  0.00504  2.9 1.00\nE10 2827  0.25  0.25  0.2207  0.02397  2.9 1.00\nE11 2827  0.23  0.23 -0.0031 -0.00136  2.9 1.02\nE12 2827  0.21  0.21 -0.1204 -0.01591  2.9 1.01\nE13 2827  0.24  0.24  0.0755  0.00801  2.9 1.02\nE14 2827  0.22  0.22 -0.0726 -0.00897  2.9 1.00\nE15 2827  0.21  0.21 -0.1093 -0.01360  2.9 1.00\nE16 2827  0.24  0.24  0.1158  0.01200  2.9 1.02\nE17 2827  0.26  0.25  0.2135  0.02398  2.9 1.03\nE18 2827  0.21  0.21 -0.1803 -0.02255  2.9 1.02\nE19 2827  0.25  0.25  0.1667  0.01776  2.9 1.02\n\nNon missing response frequency for each item\n       1    2    3    4    5 miss\nE1  0.09 0.25 0.31 0.34 0.01    0\nE2  0.10 0.24 0.28 0.36 0.01    0\nE3  0.09 0.25 0.30 0.35 0.01    0\nE4  0.10 0.25 0.28 0.35 0.01    0\nE5  0.09 0.27 0.30 0.32 0.01    0\nE6  0.11 0.24 0.31 0.33 0.01    0\nE7  0.09 0.25 0.29 0.35 0.01    0\nE8  0.09 0.26 0.29 0.35 0.01    0\nE9  0.09 0.25 0.29 0.36 0.01    0\nE10 0.09 0.25 0.31 0.35 0.01    0\nE11 0.10 0.24 0.29 0.35 0.01    0\nE12 0.09 0.25 0.29 0.36 0.01    0\nE13 0.10 0.24 0.30 0.35 0.01    0\nE14 0.10 0.24 0.31 0.35 0.01    0\nE15 0.10 0.25 0.30 0.34 0.01    0\nE16 0.10 0.24 0.29 0.35 0.01    0\nE17 0.11 0.25 0.30 0.33 0.01    0\nE18 0.10 0.25 0.27 0.37 0.01    0\nE19 0.11 0.24 0.30 0.35 0.01    0\nIndeed, analyses support my contention that combining all “engagement” items into one scale does not make sense from a measurement standpoint. Already there is strong evidence that there is a lack of reliability here from a composite scale perspective. Cronbach’s alpha for this scale is unacceptable, sitting at α= .01 (I would accept values above .70). I also ran a confirmatory factor analysis and none of the items loaded together onto a global latent factor of engagement; all factor loadings are less than .255. I would like to see loadings of at least .40 and realistic CFI and TLI values (within 0-1 with values close to 1 without exceeding 1).\nlibrary(lavaan)\nCFA1  &lt;- 'Engage =~ E1 + E2 + E3 + E4 + E5 + E6 + E7 + E8 + E9 + E10 + E11 + E12 + E13 + E14 + E15 + E16 + E17 + E18 + E19'\nCFAOut &lt;- cfa(CFA1, data=MasterData) \nsummary(CFAOut, fit.measures=TRUE, standardized=TRUE)\n\nlavaan 0.6-18 ended normally after 19 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        38\n\n  Number of observations                          2827\n\nModel Test User Model:\n                                                      \n  Test statistic                               157.795\n  Degrees of freedom                               152\n  P-value (Chi-square)                           0.357\n\nModel Test Baseline Model:\n\n  Test statistic                               157.795\n  Degrees of freedom                               171\n  P-value                                        0.757\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.000\n  Tucker-Lewis Index (TLI)                       1.494\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -76775.576\n  Loglikelihood unrestricted model (H1)     -76696.678\n                                                      \n  Akaike (AIC)                              153627.152\n  Bayesian (BIC)                            153853.136\n  Sample-size adjusted Bayesian (SABIC)     153732.397\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.004\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.010\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.017\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Engage =~                                                             \n    E1                1.000                               0.003    0.003\n    E2               -0.058 1529.201   -0.000    1.000   -0.000   -0.000\n    E3                0.117 1727.173    0.000    1.000    0.000    0.000\n    E4                0.175 2079.327    0.000    1.000    0.001    0.001\n    E5                0.031 1429.437    0.000    1.000    0.000    0.000\n    E6                0.021 1439.041    0.000    1.000    0.000    0.000\n    E7                0.111 1707.275    0.000    1.000    0.000    0.000\n    E8                0.213 2328.665    0.000    1.000    0.001    0.001\n    E9               -0.005 1413.694   -0.000    1.000   -0.000   -0.000\n    E10               0.048 1464.347    0.000    1.000    0.000    0.000\n    E11               0.095 1646.012    0.000    1.000    0.000    0.000\n    E12              -0.062 1521.521   -0.000    1.000   -0.000   -0.000\n    E13              -0.254 2653.472   -0.000    1.000   -0.001   -0.001\n    E14              -0.142 1862.255   -0.000    1.000   -0.000   -0.000\n    E15              -0.013 1417.918   -0.000    1.000   -0.000   -0.000\n    E16               0.006 1440.679    0.000    1.000    0.000    0.000\n    E17              -0.245 2587.500   -0.000    1.000   -0.001   -0.001\n    E18               0.019 1451.525    0.000    1.000    0.000    0.000\n    E19               0.192 2198.624    0.000    1.000    0.001    0.001\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .E1                0.980    0.122    8.025    0.000    0.980    1.000\n   .E2                1.052    0.028   37.502    0.000    1.052    1.000\n   .E3                1.003    0.027   37.182    0.000    1.003    1.000\n   .E4                1.033    0.028   36.621    0.000    1.033    1.000\n   .E5                0.991    0.026   37.568    0.000    0.991    1.000\n   .E6                1.024    0.027   37.584    0.000    1.024    1.000\n   .E7                1.019    0.027   37.232    0.000    1.019    1.000\n   .E8                1.004    0.028   36.006    0.000    1.004    1.000\n   .E9                1.003    0.027   37.596    0.000    1.003    1.000\n   .E10               0.995    0.027   37.530    0.000    0.995    1.000\n   .E11               1.036    0.028   37.338    0.000    1.036    1.000\n   .E12               1.025    0.027   37.487    0.000    1.025    1.000\n   .E13               1.041    0.030   35.268    0.000    1.041    1.000\n   .E14               0.996    0.027   36.954    0.000    0.996    1.000\n   .E15               1.004    0.027   37.592    0.000    1.004    1.000\n   .E16               1.042    0.028   37.596    0.000    1.042    1.000\n   .E17               1.063    0.030   35.525    0.000    1.063    1.000\n   .E18               1.045    0.028   37.586    0.000    1.045    1.000\n   .E19               1.049    0.029   36.414    0.000    1.049    1.000\n    Engage            0.000    0.119    0.000    1.000    1.000    1.000\nThus, I will focus on three items that the client was interested in understanding their employee’s perspectives on.\nThe three items of interest are:\nFirst I calculated means and standard deviations for these three items and then broke the means up by gender, race, and sexual orientation.\nNote: greater scores on these items indicate greater agreement with the item.\nShortData &lt;- subset(MasterData,Sexual.Orientation != \"Missing\" & Race.Ethnicity != \"\", select=c('Race.Ethnicity','Gender.Identity','Sexual.Orientation', 'E2', 'E5', 'E11'))\n\ndescribe(ShortData)\n\n                    vars    n mean   sd median trimmed  mad min max range  skew\nRace.Ethnicity*        1 1844 7.49 3.24     10    7.74 0.00   2  10     8 -0.56\nGender.Identity*       2 1844 1.79 0.92      2    1.61 1.48   1   5     4  1.54\nSexual.Orientation*    3 1844 3.09 0.78      3    3.00 0.00   1   6     5  2.04\nE2                     4 1844 2.92 1.03      3    3.01 1.48   1   5     4 -0.37\nE5                     5 1844 2.88 1.00      3    2.96 1.48   1   5     4 -0.28\nE11                    6 1844 2.92 1.02      3    3.02 1.48   1   5     4 -0.36\n                    kurtosis   se\nRace.Ethnicity*        -1.64 0.08\nGender.Identity*        2.41 0.02\nSexual.Orientation*     8.35 0.02\nE2                     -0.95 0.02\nE5                     -0.98 0.02\nE11                    -0.97 0.02\n\nShortData %&gt;%\n  group_by(Race.Ethnicity) %&gt;%\n  summarise_at(vars(c(\"E2\", \"E5\", \"E11\")), list(mean = mean))\n\n# A tibble: 9 × 4\n  Race.Ethnicity                            E2_mean E5_mean E11_mean\n  &lt;fct&gt;                                       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 American Indian or Alaska Native             4       3        4   \n2 Asian                                        2.93    2.89     2.89\n3 Black or African American                    2.89    2.77     3.01\n4 Hispanic or Latino                           3.01    3.09     3.01\n5 Native American or Alaska Native             3       3        1.5 \n6 Native Hawaiian or Other Pacific Islander    3.5     3.5      4   \n7 Native Hawaiian or Pacific Islander          3.29    2.71     2.57\n8 Two or More Races                            3.19    3.05     2.48\n9 White                                        2.90    2.87     2.94\n\nShortData %&gt;%\n  group_by(Gender.Identity) %&gt;%\n  summarise_at(vars(c(\"E2\", \"E5\", \"E11\")), list(mean = mean))\n\n# A tibble: 5 × 4\n  Gender.Identity         E2_mean E5_mean E11_mean\n  &lt;fct&gt;                     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 female                     2.93    2.86     2.93\n2 male                       2.92    2.91     2.94\n3 Non-binary/third gender    2.78    2.80     2.81\n4 Prefer not to say          2.97    2.88     2.77\n5 Prefer to self-describe    2.87    2.82     2.89\n\nShortData %&gt;%\n  group_by(Sexual.Orientation) %&gt;%\n  summarise_at(vars(c(\"E2\", \"E5\", \"E11\")), list(mean = mean))\n\n# A tibble: 5 × 4\n  Sexual.Orientation E2_mean E5_mean E11_mean\n  &lt;fct&gt;                &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 Bisexual              2.76    2.71     2.69\n2 Gay                   2.89    2.88     2.68\n3 Heterosexual          2.93    2.88     2.93\n4 Lesbian               2.77    3.08     3.03\n5 Other LGBTQ+          2.99    2.89     2.97\nThere are a lot of calculated means so I will first visualize them and then conduct a few ANOVAs to test for significant mean differences.\nStarting with question E2, which is “I feel engaged in my work”:\nggplot(ShortData, aes(x=Race.Ethnicity, y=E2, fill=Race.Ethnicity)) + \n    geom_bar(position=position_dodge(), stat=\"identity\") +\n  scale_x_discrete(guide=guide_axis(n.dodge=6)) +\n  theme(axis.text.x = element_text(size = 5)) +\n  labs(y = \"I feel engaged in my work\")\n\n\n\n\n\n\n\nggplot(ShortData, aes(x=Gender.Identity, y=E2, fill=Gender.Identity)) + \n    geom_bar(position=position_dodge(), stat=\"identity\") +\n  scale_x_discrete(guide=guide_axis(n.dodge=2)) +\n  theme(axis.text.x = element_text(size = 10)) +\n  labs(y = \"I feel engaged in my work\")\n\n\n\n\n\n\n\nggplot(ShortData, aes(x=Sexual.Orientation, y=E2, fill=Sexual.Orientation)) + \n    geom_bar(position=position_dodge(), stat=\"identity\") +\n    theme(axis.text.x = element_text(size = 10)) +\n  labs(y = \"I feel engaged in my work\")\nGlancing at the graphs, there does not seem to be a lot of variability for this question. Most participants across race, gender, and sexual orientation felt positively about how engaged they feel at work. The exception here may be that Native Hawaiian or Other Pacific Islanders may feel less engaged than other groups of people. I will not know if these differences are significant until I run an ANOVA.\nNext, question E5, “The company cares about Diversity, Equity, and Inclusion”:\nggplot(ShortData, aes(x=Race.Ethnicity, y=E5, fill=Race.Ethnicity)) + \n    geom_bar(position=position_dodge(), stat=\"identity\") +\n  scale_x_discrete(guide=guide_axis(n.dodge=6)) +\n  theme(axis.text.x = element_text(size = 5)) +\n  theme(axis.title.y = element_text(size = 8)) +\n  labs(y = \"The company cares about Diversity, Equity, and Inclusion\")\n\n\n\n\n\n\n\nggplot(ShortData, aes(x=Gender.Identity, y=E5, fill=Gender.Identity)) + \n    geom_bar(position=position_dodge(), stat=\"identity\") +\n  scale_x_discrete(guide=guide_axis(n.dodge=2)) +\n  theme(axis.text.x = element_text(size = 10)) +\n  theme(axis.title.y = element_text(size = 8)) +\n  labs(y = \"The company cares about Diversity, Equity, and Inclusion\")\n\n\n\n\n\n\n\nggplot(ShortData, aes(x=Sexual.Orientation, y=E5, fill=Sexual.Orientation)) + \n    geom_bar(position=position_dodge(), stat=\"identity\") +\n    theme(axis.text.x = element_text(size = 10)) +\n  theme(axis.title.y = element_text(size = 8)) +\n  labs(y = \"The company cares about Diversity, Equity, and Inclusion\")\nSimilar to question E2, most participants felt that their company does care about diversity, equity, and inclusion, with the exceptions potentially being American Indians or Alaska Natives and Native Hawaiians or Other Pacific Islanders.\nLastly, question E11, “I believe there are good career opportunities for me at the company”:\nggplot(ShortData, aes(x=Race.Ethnicity, y=E11, fill=Race.Ethnicity)) + \n    geom_bar(position=position_dodge(), stat=\"identity\") +\n  scale_x_discrete(guide=guide_axis(n.dodge=6)) +\n  theme(axis.text.x = element_text(size = 5)) +\n  theme(axis.title.y = element_text(size = 8)) +\n  labs(y = \"I believe there are good career opportunities for me at the company\")\n\n\n\n\n\n\n\nggplot(ShortData, aes(x=Gender.Identity, y=E11, fill=Gender.Identity)) + \n    geom_bar(position=position_dodge(), stat=\"identity\") +\n  scale_x_discrete(guide=guide_axis(n.dodge=2)) +\n  theme(axis.text.x = element_text(size = 10)) +\n  theme(axis.title.y = element_text(size = 8)) +\n  labs(y = \"I believe there are good career opportunities for me at the company\")\n\n\n\n\n\n\n\nggplot(ShortData, aes(x=Sexual.Orientation, y=E11, fill=Sexual.Orientation)) + \n    geom_bar(position=position_dodge(), stat=\"identity\") +\n    theme(axis.text.x = element_text(size = 10)) +\n  theme(axis.title.y = element_text(size = 8)) +\n  labs(y = \"I believe there are good career opportunities for me at the company\")\nPretty similar across the board with the exception of Native Hawaiian or Other Pacific Islanders who do not agree that there are good opportunities for them at their company.\nNow I will run three ANOVAs with each engagement item as the dependent variable and the three identity variables, race, gender, and sexual orientation, as predictors in each model.\nE2Model &lt;- aov(E2 ~ Race.Ethnicity + Gender.Identity + Sexual.Orientation, ShortData)\nsummary(E2Model)\n\n                     Df Sum Sq Mean Sq F value Pr(&gt;F)\nRace.Ethnicity        8    5.5  0.6902   0.650  0.736\nGender.Identity       4    1.4  0.3430   0.323  0.863\nSexual.Orientation    4    2.7  0.6729   0.634  0.639\nResiduals          1827 1940.3  1.0620               \n\nE5Model &lt;- aov(E5 ~ Race.Ethnicity + Gender.Identity + Sexual.Orientation, ShortData)\nsummary(E5Model)\n\n                     Df Sum Sq Mean Sq F value Pr(&gt;F)\nRace.Ethnicity        8    6.1  0.7574   0.757  0.641\nGender.Identity       4    1.6  0.3972   0.397  0.811\nSexual.Orientation    4    3.4  0.8585   0.858  0.488\nResiduals          1827 1828.1  1.0006               \n\nE11Model &lt;- aov(E11 ~ Race.Ethnicity + Gender.Identity + Sexual.Orientation, ShortData)\nsummary(E11Model)\n\n                     Df Sum Sq Mean Sq F value Pr(&gt;F)  \nRace.Ethnicity        8   14.7  1.8353   1.764 0.0798 .\nGender.Identity       4    3.3  0.8231   0.791 0.5309  \nSexual.Orientation    4    7.5  1.8780   1.805 0.1253  \nResiduals          1827 1901.3  1.0407                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Does identity interact in predicting perceptions of the DEI culture at their company?\nE5INTModel &lt;- aov(E5 ~ Race.Ethnicity*Gender.Identity*Sexual.Orientation, ShortData)\nsummary(E5INTModel)\n\n                                                    Df Sum Sq Mean Sq F value\nRace.Ethnicity                                       8    6.1  0.7574   0.759\nGender.Identity                                      4    1.6  0.3972   0.398\nSexual.Orientation                                   4    3.4  0.8585   0.860\nRace.Ethnicity:Gender.Identity                      15   18.9  1.2583   1.261\nRace.Ethnicity:Sexual.Orientation                   14    8.7  0.6244   0.625\nGender.Identity:Sexual.Orientation                  13   21.6  1.6619   1.665\nRace.Ethnicity:Gender.Identity:Sexual.Orientation    8    5.0  0.6239   0.625\nResiduals                                         1777 1773.9  0.9982        \n                                                  Pr(&gt;F)  \nRace.Ethnicity                                    0.6394  \nGender.Identity                                   0.8103  \nSexual.Orientation                                0.4872  \nRace.Ethnicity:Gender.Identity                    0.2194  \nRace.Ethnicity:Sexual.Orientation                 0.8458  \nGender.Identity:Sexual.Orientation                0.0623 .\nRace.Ethnicity:Gender.Identity:Sexual.Orientation 0.7574  \nResiduals                                                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nNote that I have not contrast coded these variables which would aid greatly in interpretation of significant effects. However, since none of the main effects are significant (likely due to small cell sizes in some categories), I will not go back and contrast code these main effects. However, I do show how to contrast code and the interpretation of those effects in a different project already uploaded to my portfolio if you are interested.\nResults across the board indicate that race, gender, and sexual orientation were not significant predictors of perceptions of engagement, DEI culture, and career opportunities, all ps &gt; .05. Also, identity does not interact in predicting perceptions of the DEI culture, all ps &gt; .05.\nmean(ShortData$E2)\n\n[1] 2.922451\n\nmean(ShortData$E5)\n\n[1] 2.883948\n\nmean(ShortData$E11)\n\n[1] 2.921909\nLooking at the overall means of those three items and considering that each item was rated on a scale of 1-5 with 5 representing a “strongly agree” anchor, it seems that across the board, employees feel relatively neutral when asked about their engagement levels, the DEI culture of the company, and the career opportunities at their company. These perceptions did not significantly differ across racial, gender, and sexual orientation groups. However, glancing back at the bar charts that displayed the spread of employees across industry level, it is clear that only White and Asian employees work in the executive and C-Suite roles at this company, which certainly limits the voices of other groups of people in major decision making roles.\nThus, my recommendation is to collect more effective data. For example, psychometric engagement scales should be utilized to obtain reliable and valid engagement scores in this company; the current data collection process contains many items that have nothing to do with engagement nor do they load onto the same latent factor of engagement, even though they are classified as engagement items in this company’s data. Next, qualitative data should be collected, allowing for employees to voice their concerns and provide nuance to these items. Last, some work should be done to look at the current selection procedure for management and executive positions at this company. There could be a case for adverse impact if the company is not careful with their hiring strategy."
  },
  {
    "objectID": "Portfolio pages/Engagement and Diversity.html#about-this-data",
    "href": "Portfolio pages/Engagement and Diversity.html#about-this-data",
    "title": "Engagement and Diversity",
    "section": "About This Data",
    "text": "About This Data"
  },
  {
    "objectID": "Portfolio pages/MLMEngageTeam.html",
    "href": "Portfolio pages/MLMEngageTeam.html",
    "title": "Using Multilevel Modeling To Understand Engagement, Autonomy, and Team Cohesion",
    "section": "",
    "text": "A company has collected employee data and wants to understand the factors that shape the engagement of their workers. They collected data on the pay structure, autonomy level, and team cohesion each employee has with the overall goal of relating these variables to individual level engagement. Unique to this data is the level 1 / level 2 structure. For example, pay, engagement, and job autonomy (all level 1 variables) are assessed at the individual level, while team cohesion (a level 2 variable) is assessed at the team level. Thus, individuals are nested in teams. My goal is to use these variables to predict engagement.\nVariables need to be centered and scaled prior to entering them into a multilevel model.\nlibrary(multilevel)\nlibrary(readxl)\nlibrary(robumeta)\n\n\nhead(data)\n\n  level1_id grpid      engage      cohes      jobsat        pay     jobauto\n1         1     1  0.76164141 -0.5461742  0.73576787 -0.2145807 -0.10330841\n2         2     1 -0.05155921 -0.5461742  0.09589171 -1.1725662  0.01494606\n3         3     1  1.60837420 -0.5461742  0.47694372 -0.9820727 -0.04202197\n4         4     1  1.23486381 -0.5461742  0.58489184 -0.4141129 -0.42891177\n5         5     1  6.73776145 -0.5461742 -0.83047804  1.9041059  0.68604898\n6         6     1  6.54714955 -0.5461742  1.58131185  0.2765618  1.19430414\n  commitment1 commitment2 commitment3\n1           4           5           4\n2           5           5           5\n3           1           2           2\n4           3           4           3\n5           5           5           5\n6           3           4           4\n\noptions(scipen=999)\n\ndata$cohesion.grand.c &lt;- scale(data$cohes, scale = FALSE)\ndata$autonomy.grp.c &lt;- group.center(data$jobauto, data$grpid)\ndata$jobsat.grp.c &lt;- group.center(data$jobsat, data$grpid)\ndata$pay.grp.c &lt;- group.center(data$pay, data$grpid)\nNow that I have group and grand mean centered the data, I will begin building some models to predict engagement. First I will add the predictors into a model with engagement as the dependent variable. I also added in interactions between autonomy and pay and cohesion and pay to the model.\nModel1 &lt;- lme(engage~ 1 + pay.grp.c + autonomy.grp.c + pay.grp.c:autonomy.grp.c + cohesion.grand.c + pay.grp.c:cohesion.grand.c, random = ~1 + pay.grp.c + autonomy.grp.c + pay.grp.c:autonomy.grp.c|grpid,\ndata = data, control = lmeControl(opt = \"optim\"))\n\nsummary(Model1)\n\nLinear mixed-effects model fit by REML\n  Data: data \n       AIC      BIC    logLik\n  3599.774 3678.179 -1782.887\n\nRandom effects:\n Formula: ~1 + pay.grp.c + autonomy.grp.c + pay.grp.c:autonomy.grp.c | grpid\n Structure: General positive-definite, Log-Cholesky parametrization\n                         StdDev    Corr                \n(Intercept)              0.4298969 (Intr) py.gr. atnm..\npay.grp.c                0.7669658 -0.169              \nautonomy.grp.c           0.6277085 -0.215  0.131       \npay.grp.c:autonomy.grp.c 1.2852694  0.317  0.288  0.117\nResidual                 2.3189244                     \n\nFixed effects:  engage ~ 1 + pay.grp.c + autonomy.grp.c + pay.grp.c:autonomy.grp.c +      cohesion.grand.c + pay.grp.c:cohesion.grand.c \n                               Value Std.Error  DF   t-value p-value\n(Intercept)                2.7311089 0.1061344 696 25.732554  0.0000\npay.grp.c                  0.3836225 0.1410103 696  2.720528  0.0067\nautonomy.grp.c             0.6181183 0.1337924 696  4.619980  0.0000\ncohesion.grand.c           0.1544943 0.1241618  48  1.244298  0.2194\npay.grp.c:autonomy.grp.c   0.3487103 0.2123779 696  1.641933  0.1011\npay.grp.c:cohesion.grand.c 0.5352599 0.1663569 696  3.217540  0.0014\n Correlation: \n                           (Intr) py.gr. atnm.. chsn.. py.grp.c:t..\npay.grp.c                  -0.074                                  \nautonomy.grp.c             -0.083  0.070                           \ncohesion.grand.c            0.007 -0.003 -0.001                    \npay.grp.c:autonomy.grp.c    0.166  0.182  0.012  0.014             \npay.grp.c:cohesion.grand.c -0.001  0.001  0.008 -0.112  0.032      \n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-2.4868889 -0.6959695 -0.1058310  0.5644816  3.9079997 \n\nNumber of Observations: 750\nNumber of Groups: 50 \n\nVarCorr(Model1)\n\ngrpid = pdLogChol(1 + pay.grp.c + autonomy.grp.c + pay.grp.c:autonomy.grp.c) \n                         Variance  StdDev    Corr                \n(Intercept)              0.1848113 0.4298969 (Intr) py.gr. atnm..\npay.grp.c                0.5882365 0.7669658 -0.169              \nautonomy.grp.c           0.3940179 0.6277085 -0.215  0.131       \npay.grp.c:autonomy.grp.c 1.6519175 1.2852694  0.317  0.288  0.117\nResidual                 5.3774103 2.3189244                     \n\nlibrary(lme4)\nlibrary(interactions)\nlibrary(jtools)\nlibrary(lmerTest)\n\nMod1 &lt;- lmer(engage ~ 1 + pay.grp.c + autonomy.grp.c + pay.grp.c:autonomy.grp.c + cohesion.grand.c + pay.grp.c:cohesion.grand.c + (1 + pay.grp.c + autonomy.grp.c + pay.grp.c:autonomy.grp.c|grpid), data = data, control = lmerControl(calc.derivs = FALSE))\nSeveral significant results emerged from these models. Specifically, employee pay was significantly related to individual levels of work engagement b=0.384, t(696)=2.721, p=.007. Additionally, results suggest that for every unit increase in pay, work engagement will increase by a value of 0.384, when autonomy and team cohesion are zero. However, job autonomy was did not moderate the relationship between pay and work engagement, b=0.349, t(696)=1.642, p=.101.\nRegarding the level 2 predictor, team cohesion, results indicate that team cohesion was found to moderate the relationship between pay and work engagement, b=0.535, t(696)=3.218, p=.001.\nGiven the significant interaction, I will now plot the interaction and calculate the simple slopes of this interaction to aid in interpretation.\ninteract_plot(model = Mod1, pred = pay.grp.c, modx = cohesion.grand.c,\nx.label = \"Pay\",\nmain.title = \"Interaction Between Team Cohesion and Pay Predicting Engagement\",\ny.label = \"Engagement\", legend.main = \"Team Cohesion\",\ncolors = c(\"Green\",\"Blue\")) + theme_apa(legend.use.title = T)\n\n\n\n\n\n\n\nsim_slopes(model = Mod1, pred = pay.grp.c, modx = cohesion.grand.c)\n\nJOHNSON-NEYMAN INTERVAL\n\nWhen cohesion.grand.c is OUTSIDE the interval [-2.10, -0.19], the slope of\npay.grp.c is p &lt; .05.\n\nNote: The range of observed values of cohesion.grand.c is [-2.23, 1.69]\n\nSIMPLE SLOPES ANALYSIS\n\nSlope of pay.grp.c when cohesion.grand.c = -0.83100000291645215177994 (- 1 SD): \n\n   Est.   S.E.   t val.      p\n------- ------ -------- ------\n  -0.06   0.20    -0.31   0.76\n\nSlope of pay.grp.c when cohesion.grand.c =  0.00000000000000001276756 (Mean): \n\n  Est.   S.E.   t val.      p\n------ ------ -------- ------\n  0.38   0.14     2.72   0.01\n\nSlope of pay.grp.c when cohesion.grand.c =  0.83100000291645215177994 (+ 1 SD): \n\n  Est.   S.E.   t val.      p\n------ ------ -------- ------\n  0.83   0.20     4.19   0.00\nThe relationship between pay and work engagement is stronger for individuals with higher levels of team cohesion. The slope for pay will increase by 0.535 with every unit change of team cohesion. In other words, the simple effect of engagement on pay gets strengthened, or more positive, p=.001.\nSimple slopes analysis reveals a significant moderated relationship between pay and engagement when autonomy is at its mean (b=0.38, t=2.72, p=.01) and when autonomy is one standard deviation above its mean (b=0.83, t=4.19, p&lt;.01), but not for those who indicated having levels of autonomy at work that was at least one standard deviation below the mean (b-0.06, t=-0.31, p=.76).\nThe client also collected data on job satisfaction and is interested in knowing if job satisfaction, at the individual level, also interacts with team-level cohesion in predicting work engagement. I also swapped dependent variables to test if pay was a significant predictor of job satisfaction.\nmodel2.1 &lt;- lme(engage ~pay.grp.c, random = ~1 + pay.grp.c|grpid, data = data, control = lmeControl(opt = \"optim\"))\nsummary(model2.1)\n\nLinear mixed-effects model fit by REML\n  Data: data \n       AIC      BIC    logLik\n  3768.164 3795.868 -1878.082\n\nRandom effects:\n Formula: ~1 + pay.grp.c | grpid\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev    Corr  \n(Intercept) 0.2572344 (Intr)\npay.grp.c   0.8923049 0.552 \nResidual    2.8579445       \n\nFixed effects:  engage ~ pay.grp.c \n                Value Std.Error  DF   t-value p-value\n(Intercept) 2.9035266 0.1105163 699 26.272382  0.0000\npay.grp.c   0.4368898 0.1639982 699  2.663991  0.0079\n Correlation: \n          (Intr)\npay.grp.c 0.14  \n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-2.1116006 -0.6941114 -0.1975845  0.4728133  6.5971460 \n\nNumber of Observations: 750\nNumber of Groups: 50 \n\nmodel2.2 &lt;- lme(jobsat ~ pay.grp.c, random = ~1 + pay.grp.c|grpid, data = data, control = lmeControl(opt = \"optim\"))\nsummary(model2.2)\n\nLinear mixed-effects model fit by REML\n  Data: data \n       AIC      BIC    logLik\n  2168.444 2196.148 -1078.222\n\nRandom effects:\n Formula: ~1 + pay.grp.c | grpid\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 0.13429577 (Intr)\npay.grp.c   0.03052776 0.003 \nResidual    1.00544251       \n\nFixed effects:  jobsat ~ pay.grp.c \n                  Value  Std.Error  DF    t-value p-value\n(Intercept) -0.06326777 0.04133513 699 -1.5306053  0.1263\npay.grp.c    0.03569705 0.03633069 699  0.9825591  0.3262\n Correlation: \n          (Intr)\npay.grp.c 0     \n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.80428971 -0.67306262  0.02052851  0.70766759  2.69852694 \n\nNumber of Observations: 750\nNumber of Groups: 50 \n\nmodel2.3 &lt;- lme(engage ~ pay.grp.c + jobsat.grp.c, random = ~1 + pay.grp.c + jobsat.grp.c|grpid, data = data,control = lmeControl(opt = \"optim\"))\nsummary(model2.3)\n\nLinear mixed-effects model fit by REML\n  Data: data \n      AIC      BIC    logLik\n  3758.69 3804.851 -1869.345\n\nRandom effects:\n Formula: ~1 + pay.grp.c + jobsat.grp.c | grpid\n Structure: General positive-definite, Log-Cholesky parametrization\n             StdDev    Corr         \n(Intercept)  0.3136581 (Intr) py.gr.\npay.grp.c    0.9135909  0.425       \njobsat.grp.c 0.5947158 -0.321  0.388\nResidual     2.7700579              \n\nFixed effects:  engage ~ pay.grp.c + jobsat.grp.c \n                 Value Std.Error  DF   t-value p-value\n(Intercept)  2.9035266 0.1104472 698 26.288815  0.0000\npay.grp.c    0.4211243 0.1653490 698  2.546882  0.0111\njobsat.grp.c 0.3767563 0.1374256 698  2.741530  0.0063\n Correlation: \n             (Intr) py.gr.\npay.grp.c     0.133       \njobsat.grp.c -0.079  0.175\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-2.1932653 -0.6760609 -0.1646372  0.4790537  6.6411560 \n\nNumber of Observations: 750\nNumber of Groups: 50\nWhile both pay (b=0.421, t(698)=2.547, p=.011) and job satisfaction (b=0.377, t(698)=2.742, p=.006) significantly predicted work engagement, pay was not a significant predictor of job satisfaction (b=0.036, t(699)=.983, p=.326).\nNow that I have determined which variables predict engagement, the question still remains of how strong these effects are. Given the multilevel nature of this data, I will analyze the ICC1 values which tells me the proportion of individual ratings that are due to group membership. I will also look at ICC2 values to establish the stability of mean ratings in discriminating between groups (participants are grouped by their work teams).\nmult.icc(data[, c(\"jobsat\", \"jobauto\")], data$grpid)\n\n  Variable       ICC1      ICC2\n1   jobsat 0.01743368 0.2102011\n2  jobauto 0.01826727 0.2182050\nICC1 for both job satisfaction (ICC1=.017) and autonomy (ICC1=.018), both of which would be considered “small” effects. The proportion of variance in ratings that is due to between-target differences is small, suggesting that individuals rating are only slightly attributable to group membership.\nThe ICC2 for job satisfaction (ICC2=.210) is larger than the ICC2 for autonomy (ICC2=.218), suggesting that the groups’ mean ratings were more stable and reliable for ratings of autonomy than for ratings of job satisfaction. In other words, the mean ratings of autonomy were better able to distinguish between groups than the mean ratings of job satisfaction.\nWhen looking at the data, I noticed that the commitment scale was created by aggregating three commitment items into an overall scale. To double-check that employees tended to rate similarly across the three items (i.e., an individual rating a score of 4 on the first commitment item would likely rate similarly for commitment items 2 & 3) and that it is appropriate to use an aggregate commitment scale, I will assess inter-rater agreement. Note: rwg is the symbol used to denote this form of inter-rater agreement and will be utilized in the code and interpretation below.\nrwg.commit1.un = rwg(data$commitment1, data$grpid, ranvar = 2.00)\nsummary(rwg.commit1.un)\n\n    grpid                rwg             gsize   \n Length:50          Min.   :0.0000   Min.   :15  \n Class :character   1st Qu.:0.2345   1st Qu.:15  \n Mode  :character   Median :0.4333   Median :15  \n                    Mean   :0.3962   Mean   :15  \n                    3rd Qu.:0.5571   3rd Qu.:15  \n                    Max.   :0.7952   Max.   :15  \n\nhist(rwg.commit1.un$rwg, xlab = \"Estimate of rwg\", ylab = \"Frequency\", main = \"Histogram of 1st Commitment Item's rwg Values\")\n\n\n\n\n\n\n\nrwgj.commit.un = rwg.j(data[, c(8:10)], data$grpid, ranvar = 2.00)\nsummary(rwgj.commit.un)\n\n    grpid               rwg.j            gsize   \n Length:50          Min.   :0.0000   Min.   :15  \n Class :character   1st Qu.:0.5202   1st Qu.:15  \n Mode  :character   Median :0.6825   Median :15  \n                    Mean   :0.6040   Mean   :15  \n                    3rd Qu.:0.7884   3rd Qu.:15  \n                    Max.   :0.8996   Max.   :15  \n\nhist(rwgj.commit.un$rwg, xlab = \"Estimate of rwg\", ylab = \"Frequency\", main = \"Histogram of Commitment Scale's rwg Values\")\nThe mean rwg value for the first commitment item is 0.396 and the median is .433. Since this value is below the standard cutoff of .70, it is apparent that raters had low levels of agreement for the first commitment item. The mean rwg.j value for the commitment scale is .604 and the median is .683. While still below the cutoff score of .70 for high inter-rater agreement, the agreement level did increase when all three commitment items were considered, as compared to a single commitment item, helping support the usability of the composite score over a particular commitment item."
  },
  {
    "objectID": "Portfolio pages/MLMEngageTeam.html#preparing-data-for-model-building",
    "href": "Portfolio pages/MLMEngageTeam.html#preparing-data-for-model-building",
    "title": "Using Multilevel Modeling To Understand Engagement, Autonomy, and Team Cohesion",
    "section": "Preparing Data for Model Building",
    "text": "Preparing Data for Model Building"
  },
  {
    "objectID": "Portfolio pages/Repeated Measures.html#post-hoc-tests",
    "href": "Portfolio pages/Repeated Measures.html#post-hoc-tests",
    "title": "Does Birth Order Impact SAT Scores?",
    "section": "Post Hoc Tests",
    "text": "Post Hoc Tests"
  },
  {
    "objectID": "Portfolio pages/BWRepeart.html#marginal-means",
    "href": "Portfolio pages/BWRepeart.html#marginal-means",
    "title": "Between and Within Repeated Measures ANOVA in R",
    "section": "Marginal Means",
    "text": "Marginal Means"
  },
  {
    "objectID": "Portfolio pages/Engagement and Diversity.html#factorial-anova",
    "href": "Portfolio pages/Engagement and Diversity.html#factorial-anova",
    "title": "Engagement and Diversity",
    "section": "Factorial ANOVA",
    "text": "Factorial ANOVA"
  },
  {
    "objectID": "Portfolio pages/FactorPower.html#simple-effects",
    "href": "Portfolio pages/FactorPower.html#simple-effects",
    "title": "Factorial Anova and Power Analysis in R",
    "section": "Simple Effects",
    "text": "Simple Effects"
  },
  {
    "objectID": "Portfolio pages/FactorPower.html#effect-sizes",
    "href": "Portfolio pages/FactorPower.html#effect-sizes",
    "title": "Factorial Anova and Power Analysis in R",
    "section": "Effect Sizes",
    "text": "Effect Sizes"
  },
  {
    "objectID": "index.html#featured-projects",
    "href": "index.html#featured-projects",
    "title": "Matthew Swanson, Ph.D.",
    "section": "Featured Projects",
    "text": "Featured Projects"
  },
  {
    "objectID": "Portfolio pages/SelfEsteem SEM & IRT.html",
    "href": "Portfolio pages/SelfEsteem SEM & IRT.html",
    "title": "Psychometric Properties of Self Esteem via SEM and IRT",
    "section": "",
    "text": "The data for this project was downloaded from Kaggle here and originally housed on OpenPsychometrics.org. The data set contains approximately 48,000 participants who took the Rosenberg Self-Esteem Scale. Participant’s gender, age, and country were also collected. This scale contains 10 items, of which five items are positively worded (Q1, Q2, Q4, Q6, & Q7) and five items are negatively worded (Q3, Q5, & Q8-10) on a 1-4 scale (anchors ranged from strongly disagree (1) to strongly agree (4)).\nFor this data, I will first compute descriptive values for the data set and then I will look at the psychometric properties of this data using both structural equation modeling (SEM) and Item Response Theory (IRT) principals.\nFirst, I will load in the data and check for missing values. (Data file location is hidden from code chunk)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nData &lt;- read.csv(\"C:/Users/matts/OneDrive/Desktop/002 Matt Desktop/WebsiteData/SelfEsteem Data/data.csv\", na.strings = \"\")\nsum(is.na(Data))\n\n[1] 0\nNo cells have been flagged as missing. However, the data originally coded all missing values as “0” so I will need to filter these values out to get complete responses before moving on to calculating data descriptives.\nNote: Original results suggest that there are four missing values in the data, however, looking at the rows that contain these four values, it becomes clear that r detected the country code of NA (Nambia) and incorrectly read those values as missing. I specified that the data be read in with the argument na.strings = “” to solve this problem and reloaded the data in.\nlibrary(psych)\n\nWarning: package 'psych' was built under R version 4.4.3\n\nstr(Data) #checking for numeric status\n\n'data.frame':   47974 obs. of  14 variables:\n $ Q1     : int  3 4 2 4 4 4 4 1 4 3 ...\n $ Q2     : int  3 4 3 3 4 4 4 1 4 4 ...\n $ Q3     : int  1 1 2 2 1 1 2 4 2 1 ...\n $ Q4     : int  4 3 3 3 4 3 4 1 4 3 ...\n $ Q5     : int  3 1 3 2 1 1 2 4 1 2 ...\n $ Q6     : int  4 3 3 3 4 3 2 1 4 3 ...\n $ Q7     : int  3 3 2 2 4 4 3 1 3 3 ...\n $ Q8     : int  2 2 3 3 1 2 4 1 1 2 ...\n $ Q9     : int  3 3 3 3 1 2 3 4 1 1 ...\n $ Q10    : int  3 2 3 3 1 1 3 4 1 1 ...\n $ gender : int  1 1 2 1 1 2 1 1 1 2 ...\n $ age    : int  40 36 22 31 30 25 32 51 37 41 ...\n $ source : int  1 1 1 1 1 1 1 3 1 1 ...\n $ country: chr  \"US\" \"US\" \"US\" \"US\" ...\n\nData_new &lt;- Data[apply(Data!=0, 1, all),]\n#Get means, sd, and range\ndescribe(Data_new)\n\n         vars     n   mean     sd median trimmed   mad min    max range   skew\nQ1          1 45844   3.01   0.86      3    3.09  1.48   1      4     3  -0.60\nQ2          2 45844   3.11   0.79      3    3.19  1.48   1      4     3  -0.73\nQ3          3 45844   2.31   0.95      2    2.26  1.48   1      4     3   0.23\nQ4          4 45844   2.92   0.80      3    2.97  1.48   1      4     3  -0.44\nQ5          5 45844   2.39   0.98      2    2.36  1.48   1      4     3   0.08\nQ6          6 45844   2.56   0.92      3    2.58  1.48   1      4     3  -0.08\nQ7          7 45844   2.45   0.93      2    2.43  1.48   1      4     3   0.02\nQ8          8 45844   2.70   0.96      3    2.74  1.48   1      4     3  -0.31\nQ9          9 45844   2.79   0.99      3    2.86  1.48   1      4     3  -0.43\nQ10        10 45844   2.58   1.07      3    2.60  1.48   1      4     3  -0.17\ngender     11 45844   1.64   0.50      2    1.66  0.00   1      3     2  -0.31\nage        12 45844  32.07 637.24     22   24.67  7.41   1 100000 99999 135.77\nsource     13 45844   1.54   0.85      1    1.43  0.00   1      3     2   1.03\ncountry*   14 45844 114.72  57.22    147  120.09 28.17   1    176   175  -0.47\n         kurtosis   se\nQ1          -0.28 0.00\nQ2           0.28 0.00\nQ3          -0.88 0.00\nQ4          -0.22 0.00\nQ5          -1.01 0.00\nQ6          -0.84 0.00\nQ7          -0.88 0.00\nQ8          -0.82 0.00\nQ9          -0.83 0.00\nQ10         -1.22 0.01\ngender      -1.29 0.00\nage      19255.40 2.98\nsource      -0.83 0.00\ncountry*    -1.47 0.27\nGlancing at the age row suggests that age ranged from 1 - 100,000 years of age, which is, of course, impossible. Thus, I will filter for a reasonable adult age range (18 - 100). This step is important as participants who reported being a single year old or 100,000 years old likely did not pay attention to other items in this scale and so I want to remove their responses.\nData_new &lt;- Data_new %&gt;%\n  filter(between(age, 18, 100))\ndescribe(Data_new)\n\n         vars     n   mean    sd median trimmed   mad min max range  skew\nQ1          1 34943   3.09  0.83      3    3.17  1.48   1   4     3 -0.67\nQ2          2 34943   3.21  0.73      3    3.29  0.00   1   4     3 -0.76\nQ3          3 34943   2.25  0.94      2    2.18  1.48   1   4     3  0.29\nQ4          4 34943   2.99  0.77      3    3.04  0.00   1   4     3 -0.47\nQ5          5 34943   2.31  0.96      2    2.26  1.48   1   4     3  0.16\nQ6          6 34943   2.62  0.90      3    2.65  1.48   1   4     3 -0.13\nQ7          7 34943   2.49  0.91      3    2.49  1.48   1   4     3 -0.02\nQ8          8 34943   2.66  0.96      3    2.71  1.48   1   4     3 -0.28\nQ9          9 34943   2.71  0.98      3    2.76  1.48   1   4     3 -0.35\nQ10        10 34943   2.49  1.06      3    2.48  1.48   1   4     3 -0.06\ngender     11 34943   1.61  0.50      2    1.63  0.00   1   3     2 -0.25\nage        12 34943  30.07 12.25     25   28.22  8.90  18 100    82  1.21\nsource     13 34943   1.57  0.87      1    1.46  0.00   1   3     2  0.95\ncountry*   14 34943 110.04 55.56    132  114.78 44.48   1 171   170 -0.39\n         kurtosis   se\nQ1          -0.11 0.00\nQ2           0.56 0.00\nQ3          -0.82 0.01\nQ4          -0.09 0.00\nQ5          -0.97 0.01\nQ6          -0.77 0.00\nQ7          -0.82 0.00\nQ8          -0.86 0.01\nQ9          -0.87 0.01\nQ10         -1.22 0.01\ngender      -1.46 0.00\nage          0.84 0.07\nsource      -0.99 0.00\ncountry*    -1.52 0.30\n\nData_new %&gt;%\n  count(Data_new$gender)\n\n  Data_new$gender     n\n1               1 13938\n2               2 20712\n3               3   293\nLooking at the means, most items hover around the mid point of the scale (between scores of 2-3). The average age of participants is 30 with a standard deviation of about 12 years and 59% of participants identify as female, 40% identify as male, and about 1% identify as non-binary.\nNow to calculate correlations between scale items and visualize them.\nlibrary(Hmisc)\n\nWarning: package 'Hmisc' was built under R version 4.4.3\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following object is masked from 'package:psych':\n\n    describe\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.4.3\n\n\ncorrplot 0.95 loaded\n\nCorrelation &lt;- rcorr(as.matrix(Data_new[, 1:10]))\nCorrelation\n\n       Q1    Q2    Q3    Q4    Q5    Q6    Q7    Q8    Q9   Q10\nQ1   1.00  0.69 -0.52  0.54 -0.49  0.59  0.55 -0.38 -0.44 -0.50\nQ2   0.69  1.00 -0.48  0.54 -0.48  0.55  0.51 -0.32 -0.40 -0.46\nQ3  -0.52 -0.48  1.00 -0.44  0.63 -0.59 -0.59  0.45  0.58  0.62\nQ4   0.54  0.54 -0.44  1.00 -0.40  0.48  0.46 -0.30 -0.39 -0.41\nQ5  -0.49 -0.48  0.63 -0.40  1.00 -0.53 -0.55  0.41  0.53  0.56\nQ6   0.59  0.55 -0.59  0.48 -0.53  1.00  0.73 -0.48 -0.52 -0.57\nQ7   0.55  0.51 -0.59  0.46 -0.55  0.73  1.00 -0.46 -0.51 -0.55\nQ8  -0.38 -0.32  0.45 -0.30  0.41 -0.48 -0.46  1.00  0.49  0.51\nQ9  -0.44 -0.40  0.58 -0.39  0.53 -0.52 -0.51  0.49  1.00  0.73\nQ10 -0.50 -0.46  0.62 -0.41  0.56 -0.57 -0.55  0.51  0.73  1.00\n\nn= 34943 \n\n\nP\n    Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10\nQ1      0  0  0  0  0  0  0  0  0 \nQ2   0     0  0  0  0  0  0  0  0 \nQ3   0  0     0  0  0  0  0  0  0 \nQ4   0  0  0     0  0  0  0  0  0 \nQ5   0  0  0  0     0  0  0  0  0 \nQ6   0  0  0  0  0     0  0  0  0 \nQ7   0  0  0  0  0  0     0  0  0 \nQ8   0  0  0  0  0  0  0     0  0 \nQ9   0  0  0  0  0  0  0  0     0 \nQ10  0  0  0  0  0  0  0  0  0    \n\ncorrplot.mixed(Correlation$r, tl.pos = 'lt')\nPerhaps unsurprising, it appears as though negatively worded items negatively correlate with positively worded items and all items correlate with each other. Now that I have a sense of the data and how the items play with each other, I am going to use both SEM and IRT principals to assess the psychometric properties of this scale.\nSEM is preferred here over CFA as SEM relaxes assumptions around cross loadings and error terms, which aid in reproducing more accurate fit statistics. SEM also allows for the estimation of paths between latent variables. First, I am going to run a simple SEM using the lavaan package where by all 10 self-esteem items are loaded onto a latent factor of general self-esteem.\nlibrary(lavaan)\n\nWarning: package 'lavaan' was built under R version 4.4.3\n\n\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n\n\n\nAttaching package: 'lavaan'\n\n\nThe following object is masked from 'package:psych':\n\n    cor2cov\n\nlibrary(semPlot)\n\nWarning: package 'semPlot' was built under R version 4.4.3\n\nData_new &lt;- Data_new %&gt;%\n  filter(gender != \"3\")\n\nSEM1Fac &lt;- 'SE =~ Q1 + Q2 + Q3 + Q4 + Q5 + Q6 + Q7 + Q8 + Q9 + Q10'\nfitSEM1 &lt;- sem(SEM1Fac, Data_new, estimator = \"MLR\")\nsummary(fitSEM1, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 24 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        20\n\n  Number of observations                         34650\n\nModel Test User Model:\n                                               Standard      Scaled\n  Test Statistic                              22889.711   17573.123\n  Degrees of freedom                                 35          35\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.303\n    Yuan-Bentler correction (Mplus variant)                        \n\nModel Test Baseline Model:\n\n  Test statistic                            191819.569  138248.340\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.387\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.881       0.873\n  Tucker-Lewis Index (TLI)                       0.847       0.837\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.881\n  Robust Tucker-Lewis Index (TLI)                            0.847\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)            -370326.408 -370326.408\n  Scaling correction factor                                  1.194\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)    -358881.552 -358881.552\n  Scaling correction factor                                  1.263\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                              740692.816  740692.816\n  Bayesian (BIC)                            740861.877  740861.877\n  Sample-size adjusted Bayesian (SABIC)     740798.317  740798.317\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.137       0.120\n  90 Percent confidence interval - lower         0.136       0.119\n  90 Percent confidence interval - upper         0.139       0.122\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.137\n  90 Percent confidence interval - lower                     0.136\n  90 Percent confidence interval - upper                     0.139\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.057       0.057\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SE =~                                                                 \n    Q1                1.000                               0.596    0.720\n    Q2                0.825    0.005  155.749    0.000    0.491    0.674\n    Q3               -1.211    0.009 -129.245    0.000   -0.722   -0.770\n    Q4                0.776    0.007  119.183    0.000    0.462    0.600\n    Q5               -1.161    0.010 -122.083    0.000   -0.692   -0.718\n    Q6                1.209    0.008  144.216    0.000    0.720    0.798\n    Q7                1.194    0.009  133.869    0.000    0.712    0.778\n    Q8               -0.945    0.011  -89.622    0.000   -0.563   -0.587\n    Q9               -1.181    0.011 -108.667    0.000   -0.704   -0.718\n    Q10              -1.360    0.011 -124.684    0.000   -0.811   -0.768\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1                0.330    0.004   92.713    0.000    0.330    0.481\n   .Q2                0.290    0.003   97.398    0.000    0.290    0.546\n   .Q3                0.359    0.004   84.511    0.000    0.359    0.408\n   .Q4                0.379    0.004  108.115    0.000    0.379    0.639\n   .Q5                0.450    0.005   90.813    0.000    0.450    0.485\n   .Q6                0.296    0.003   90.331    0.000    0.296    0.364\n   .Q7                0.330    0.004   89.487    0.000    0.330    0.394\n   .Q8                0.604    0.005  114.934    0.000    0.604    0.656\n   .Q9                0.467    0.005   89.848    0.000    0.467    0.485\n   .Q10               0.457    0.005   87.910    0.000    0.457    0.410\n    SE                0.355    0.005   74.706    0.000    1.000    1.000\n\nsemPaths(fitSEM1, what = \"std\", edge.label.cex = 0.7, esize = 1,\nintercepts = FALSE,rotation = 4, edge.color = 1, asize = 2.5,\nsizeMan = 5, mar = c(1, 1.5, 1.5, 3), fade = FALSE)\nWhile all items do significantly load onto the latent factor of self-esteem and have decently sized factor loadings (all factor loadings are greater than .58), model fit does not meet acceptable standards. (cite acceptable standards). Since items 4 and 8 have the lowest loadings, I will drop those two items and test if model fit improves.\nSEM1.8Fac &lt;- 'SE =~ Q1 + Q2 + Q3 + Q5 + Q6 + Q7 + Q9 + Q10'\nfitSEM1.8 &lt;- sem(SEM1.8Fac, Data_new, estimator = \"MLR\")\nsummary(fitSEM1.8, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 25 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n\n  Number of observations                         34650\n\nModel Test User Model:\n                                               Standard      Scaled\n  Test Statistic                              19028.142   14181.466\n  Degrees of freedom                                 20          20\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.342\n    Yuan-Bentler correction (Mplus variant)                        \n\nModel Test Baseline Model:\n\n  Test statistic                            160793.472  110899.577\n  Degrees of freedom                                28          28\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.450\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.882       0.872\n  Tucker-Lewis Index (TLI)                       0.834       0.821\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.882\n  Robust Tucker-Lewis Index (TLI)                            0.835\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)            -296035.754 -296035.754\n  Scaling correction factor                                  1.205\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)    -286521.683 -286521.683\n  Scaling correction factor                                  1.281\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                              592103.508  592103.508\n  Bayesian (BIC)                            592238.757  592238.757\n  Sample-size adjusted Bayesian (SABIC)     592187.909  592187.909\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.166       0.143\n  90 Percent confidence interval - lower         0.164       0.141\n  90 Percent confidence interval - upper         0.168       0.145\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.166\n  90 Percent confidence interval - lower                     0.163\n  90 Percent confidence interval - upper                     0.168\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.059       0.059\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SE =~                                                                 \n    Q1                1.000                               0.589    0.712\n    Q2                0.825    0.005  152.677    0.000    0.486    0.666\n    Q3               -1.233    0.010 -126.573    0.000   -0.726   -0.774\n    Q5               -1.184    0.010 -119.726    0.000   -0.697   -0.723\n    Q6                1.222    0.009  140.923    0.000    0.720    0.797\n    Q7                1.211    0.009  131.168    0.000    0.713    0.780\n    Q9               -1.193    0.011 -106.736    0.000   -0.702   -0.716\n    Q10              -1.378    0.011 -122.184    0.000   -0.811   -0.769\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1                0.338    0.004   92.101    0.000    0.338    0.494\n   .Q2                0.296    0.003   96.373    0.000    0.296    0.556\n   .Q3                0.353    0.004   82.828    0.000    0.353    0.401\n   .Q5                0.443    0.005   88.796    0.000    0.443    0.477\n   .Q6                0.297    0.003   86.760    0.000    0.297    0.364\n   .Q7                0.327    0.004   86.363    0.000    0.327    0.391\n   .Q9                0.469    0.005   88.975    0.000    0.469    0.487\n   .Q10               0.456    0.005   85.751    0.000    0.456    0.409\n    SE                0.347    0.005   72.683    0.000    1.000    1.000\n\nsemPaths(fitSEM1.8, what = \"std\", edge.label.cex = 0.7, esize = 1,\nintercepts = FALSE,rotation = 4, edge.color = 1, asize = 2.5,\nsizeMan = 5, mar = c(1, 1.5, 1.5, 3), fade = FALSE)\nModel fit worsened. Thus, the SEM model with all ten items is retained.\nLastly, I want to test that there is truly just one latent factor contained in these 10 items. For the sake of exploratory analyses, I will break items into two separate factors: one factor will contain the five positively worded items and a second factor will contain the five negatively worded items.\nSEM2Fac &lt;- 'PosSE =~ Q1 + Q2 + Q4 + Q6 + Q7\n            NegSE =~ Q3 + Q5 + Q8 + Q9 + Q10'\nfitSEM2 &lt;- sem(SEM2Fac, Data_new, estimator = \"MLR\")\nsummary(fitSEM2, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 30 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                         34650\n\nModel Test User Model:\n                                               Standard      Scaled\n  Test Statistic                              14173.618   11323.056\n  Degrees of freedom                                 34          34\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.252\n    Yuan-Bentler correction (Mplus variant)                        \n\nModel Test Baseline Model:\n\n  Test statistic                            191819.569  138248.340\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.387\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.926       0.918\n  Tucker-Lewis Index (TLI)                       0.902       0.892\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.926\n  Robust Tucker-Lewis Index (TLI)                            0.902\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)            -365968.361 -365968.361\n  Scaling correction factor                                  1.282\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)    -358881.552 -358881.552\n  Scaling correction factor                                  1.263\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                              731978.723  731978.723\n  Bayesian (BIC)                            732156.237  732156.237\n  Sample-size adjusted Bayesian (SABIC)     732089.499  732089.499\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.110       0.098\n  90 Percent confidence interval - lower         0.108       0.097\n  90 Percent confidence interval - upper         0.111       0.099\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.110\n  90 Percent confidence interval - lower                     0.108\n  90 Percent confidence interval - upper                     0.111\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.044       0.044\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  PosSE =~                                                              \n    Q1                1.000                               0.628    0.759\n    Q2                0.832    0.005  161.669    0.000    0.523    0.717\n    Q4                0.771    0.006  121.856    0.000    0.484    0.629\n    Q6                1.188    0.009  127.972    0.000    0.746    0.826\n    Q7                1.158    0.010  117.988    0.000    0.728    0.796\n  NegSE =~                                                              \n    Q3                1.000                               0.734    0.782\n    Q5                0.955    0.006  158.589    0.000    0.701    0.727\n    Q8                0.791    0.008  101.730    0.000    0.581    0.605\n    Q9                1.044    0.008  130.299    0.000    0.766    0.781\n    Q10               1.187    0.008  148.109    0.000    0.871    0.826\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  PosSE ~~                                                              \n    NegSE            -0.392    0.004  -92.290    0.000   -0.849   -0.849\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1                0.290    0.004   80.140    0.000    0.290    0.424\n   .Q2                0.259    0.003   90.544    0.000    0.259    0.487\n   .Q4                0.359    0.003  102.732    0.000    0.359    0.605\n   .Q6                0.259    0.003   75.317    0.000    0.259    0.317\n   .Q7                0.307    0.004   76.461    0.000    0.307    0.367\n   .Q3                0.341    0.004   77.805    0.000    0.341    0.388\n   .Q5                0.438    0.005   85.632    0.000    0.438    0.471\n   .Q8                0.585    0.005  112.689    0.000    0.585    0.634\n   .Q9                0.375    0.005   77.904    0.000    0.375    0.390\n   .Q10               0.355    0.005   72.506    0.000    0.355    0.318\n    PosSE             0.395    0.005   75.246    0.000    1.000    1.000\n    NegSE             0.539    0.006   90.002    0.000    1.000    1.000\n\nsemPaths(fitSEM2, what = \"std\", edge.label.cex = 0.7, esize = 1,\nintercepts = FALSE,rotation = 4, edge.color = 1, asize = 2.5,\nsizeMan = 5, mar = c(1, 1.5, 1.5, 3), fade = FALSE)\nWhile model fit may have slightly improved, it still does not fit industry standards nor was this model originally conceived to possess two factors. Thus, I will retain the 10 item single factor self-esteem scale. Since I have gender data collected as well, I will conduct a multigroup SEM to determine how or if parameter estimates differ for men and women.\nTo determine if men and women differ in how they responded to the self-esteem items, first I need to add in a grouping variable to the model I settled on above (the single factor, 10 item self-esteem scale). All loadings and parameters of the structural model are set to be free with each subsequent model imposing a constraint on aspects of the model. Model fit changes of .01 ∆CFI/TLI and .015 ∆RMSEA suggests evidence for differences between men and women. This first model with zero constraints imposed on the data structure is considered the configural model. The fit for this model will be the same as the model estimated above, however, it will provide estimates for the model for both men and women.\nConfigural &lt;- sem(SEM1Fac, group = \"gender\", data = Data_new, \n                  estimator = \"MLR\")\n#summary(Configural, standardized = TRUE, fit.measures = TRUE)\nTo test for metric invariance, that is, that the relationship between the ten items and the latent self-esteem factor are the same for men and women, I will set factor loadings to be invariant. The model fit from the model will be compared to the configural model and detriments in fit greater than the change values above will provide evidence that men and women differ in how strongly each item loads onto the latent factor.\nMetric &lt;- sem(SEM1Fac, group = \"gender\", group.equal = c(\"loadings\"), \n              data = Data_new, estimator = \"MLR\")\nsummary(Metric, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 29 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        60\n  Number of equality constraints                     9\n\n  Number of observations per group:                   \n    1                                            13938\n    2                                            20712\n\nModel Test User Model:\n                                               Standard      Scaled\n  Test Statistic                              22992.339   18091.475\n  Degrees of freedom                                 79          79\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.271\n    Yuan-Bentler correction (Mplus variant)                        \n  Test statistic for each group:\n    1                                         7120.232    7120.232\n    2                                         10971.243   10971.243\n\nModel Test Baseline Model:\n\n  Test statistic                            191839.246  138203.437\n  Degrees of freedom                                90          90\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.388\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.881       0.870\n  Tucker-Lewis Index (TLI)                       0.864       0.851\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.881\n  Robust Tucker-Lewis Index (TLI)                            0.864\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)            -369727.338 -369727.338\n  Scaling correction factor                                  0.978\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)    -358231.168 -358231.168\n  Scaling correction factor                                  1.224\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                              739556.676  739556.676\n  Bayesian (BIC)                            739987.781  739987.781\n  Sample-size adjusted Bayesian (SABIC)     739825.703  739825.703\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.129       0.115\n  90 Percent confidence interval - lower         0.128       0.113\n  90 Percent confidence interval - upper         0.131       0.116\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.129\n  90 Percent confidence interval - lower                     0.128\n  90 Percent confidence interval - upper                     0.131\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.053       0.053\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SE =~                                                                 \n    Q1                1.000                               0.603    0.712\n    Q2      (.p2.)    0.822    0.005  154.779    0.000    0.496    0.675\n    Q3      (.p3.)   -1.213    0.009 -129.048    0.000   -0.732   -0.775\n    Q4      (.p4.)    0.772    0.007  118.384    0.000    0.466    0.593\n    Q5      (.p5.)   -1.168    0.010 -122.357    0.000   -0.705   -0.717\n    Q6      (.p6.)    1.209    0.008  143.847    0.000    0.729    0.793\n    Q7      (.p7.)    1.200    0.009  134.398    0.000    0.724    0.770\n    Q8      (.p8.)   -0.944    0.011  -89.261    0.000   -0.569   -0.589\n    Q9      (.p9.)   -1.179    0.011 -108.279    0.000   -0.711   -0.717\n    Q10     (.10.)   -1.358    0.011 -124.273    0.000   -0.819   -0.772\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1                3.158    0.007  445.182    0.000    3.158    3.725\n   .Q2                3.279    0.006  535.266    0.000    3.279    4.459\n   .Q3                2.208    0.008  274.296    0.000    2.208    2.337\n   .Q4                3.082    0.007  464.301    0.000    3.082    3.927\n   .Q5                2.329    0.008  279.253    0.000    2.329    2.371\n   .Q6                2.677    0.008  342.965    0.000    2.677    2.912\n   .Q7                2.491    0.008  313.833    0.000    2.491    2.650\n   .Q8                2.609    0.008  316.205    0.000    2.609    2.697\n   .Q9                2.640    0.008  310.866    0.000    2.640    2.661\n   .Q10               2.400    0.009  266.358    0.000    2.400    2.261\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1                0.354    0.006   59.213    0.000    0.354    0.493\n   .Q2                0.295    0.005   61.509    0.000    0.295    0.545\n   .Q3                0.357    0.006   55.477    0.000    0.357    0.400\n   .Q4                0.399    0.006   68.150    0.000    0.399    0.648\n   .Q5                0.469    0.008   60.523    0.000    0.469    0.486\n   .Q6                0.313    0.005   59.736    0.000    0.313    0.371\n   .Q7                0.360    0.006   60.075    0.000    0.360    0.407\n   .Q8                0.612    0.008   73.032    0.000    0.612    0.653\n   .Q9                0.478    0.008   62.962    0.000    0.478    0.486\n   .Q10               0.455    0.007   61.134    0.000    0.455    0.404\n    SE                0.364    0.006   64.062    0.000    1.000    1.000\n\n\nGroup 2 [2]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SE =~                                                                 \n    Q1                1.000                               0.589    0.726\n    Q2      (.p2.)    0.822    0.005  154.779    0.000    0.485    0.672\n    Q3      (.p3.)   -1.213    0.009 -129.048    0.000   -0.714   -0.766\n    Q4      (.p4.)    0.772    0.007  118.384    0.000    0.455    0.603\n    Q5      (.p5.)   -1.168    0.010 -122.357    0.000   -0.688   -0.723\n    Q6      (.p6.)    1.209    0.008  143.847    0.000    0.712    0.800\n    Q7      (.p7.)    1.200    0.009  134.398    0.000    0.707    0.788\n    Q8      (.p8.)   -0.944    0.011  -89.261    0.000   -0.556   -0.583\n    Q9      (.p9.)   -1.179    0.011 -108.279    0.000   -0.694   -0.716\n    Q10     (.10.)   -1.358    0.011 -124.273    0.000   -0.800   -0.763\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1                3.053    0.006  536.891    0.000    3.053    3.759\n   .Q2                3.161    0.005  623.675    0.000    3.161    4.381\n   .Q3                2.266    0.006  351.030    0.000    2.266    2.429\n   .Q4                2.935    0.005  559.457    0.000    2.935    3.891\n   .Q5                2.294    0.007  347.673    0.000    2.294    2.412\n   .Q6                2.580    0.006  417.994    0.000    2.580    2.900\n   .Q7                2.490    0.006  398.757    0.000    2.490    2.776\n   .Q8                2.701    0.007  409.646    0.000    2.701    2.833\n   .Q9                2.752    0.007  411.047    0.000    2.752    2.836\n   .Q10               2.536    0.007  348.833    0.000    2.536    2.420\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1                0.312    0.004   74.308    0.000    0.312    0.474\n   .Q2                0.286    0.004   75.881    0.000    0.286    0.549\n   .Q3                0.360    0.005   67.321    0.000    0.360    0.414\n   .Q4                0.362    0.004   85.383    0.000    0.362    0.637\n   .Q5                0.431    0.006   70.540    0.000    0.431    0.477\n   .Q6                0.285    0.004   71.745    0.000    0.285    0.360\n   .Q7                0.305    0.004   70.260    0.000    0.305    0.379\n   .Q8                0.600    0.006   93.282    0.000    0.600    0.660\n   .Q9                0.460    0.007   70.473    0.000    0.460    0.488\n   .Q10               0.459    0.007   69.966    0.000    0.459    0.418\n    SE                0.347    0.005   68.297    0.000    1.000    1.000\nModel fit for the metric model is:\nThis is\nNext, I will also hold item intercepts invariate to test for scalar invariance. Scalar invariance suggests that item responses are equivalent at the absolute level of the self-esteem trait.\nScalar &lt;- sem(SEM1Fac, group = \"gender\", group.equal = c(\"loadings\", \"intercepts\"), data = Data_new, estimator = \"MLR\")\nsummary(Scalar, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 49 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        61\n  Number of equality constraints                    19\n\n  Number of observations per group:                   \n    1                                            13938\n    2                                            20712\n\nModel Test User Model:\n                                               Standard      Scaled\n  Test Statistic                              23805.643   19132.051\n  Degrees of freedom                                 88          88\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.244\n    Yuan-Bentler correction (Mplus variant)                        \n  Test statistic for each group:\n    1                                         7681.313    7681.313\n    2                                         11450.738   11450.738\n\nModel Test Baseline Model:\n\n  Test statistic                            191839.246  138203.437\n  Degrees of freedom                                90          90\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.388\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.876       0.862\n  Tucker-Lewis Index (TLI)                       0.873       0.859\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.876\n  Robust Tucker-Lewis Index (TLI)                            0.874\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)            -370133.990 -370133.990\n  Scaling correction factor                                  0.813\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)    -358231.168 -358231.168\n  Scaling correction factor                                  1.224\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                              740351.980  740351.980\n  Bayesian (BIC)                            740707.008  740707.008\n  Sample-size adjusted Bayesian (SABIC)     740573.533  740573.533\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.125       0.112\n  90 Percent confidence interval - lower         0.123       0.111\n  90 Percent confidence interval - upper         0.126       0.113\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.125\n  90 Percent confidence interval - lower                     0.123\n  90 Percent confidence interval - upper                     0.126\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.055       0.055\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SE =~                                                                 \n    Q1                1.000                               0.605    0.712\n    Q2      (.p2.)    0.824    0.005  155.843    0.000    0.498    0.675\n    Q3      (.p3.)   -1.209    0.009 -129.474    0.000   -0.731   -0.774\n    Q4      (.p4.)    0.775    0.007  119.151    0.000    0.469    0.594\n    Q5      (.p5.)   -1.158    0.009 -122.177    0.000   -0.700   -0.712\n    Q6      (.p6.)    1.207    0.008  144.587    0.000    0.729    0.793\n    Q7      (.p7.)    1.192    0.009  134.597    0.000    0.721    0.767\n    Q8      (.p8.)   -0.943    0.011  -89.674    0.000   -0.570   -0.589\n    Q9      (.p9.)   -1.178    0.011 -108.710    0.000   -0.712   -0.717\n    Q10     (.10.)   -1.357    0.011 -124.870    0.000   -0.820   -0.772\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1      (.22.)    3.137    0.006  517.499    0.000    3.137    3.697\n   .Q2      (.23.)    3.244    0.005  625.225    0.000    3.244    4.398\n   .Q3      (.24.)    2.191    0.007  307.298    0.000    2.191    2.320\n   .Q4      (.25.)    3.026    0.005  570.059    0.000    3.026    3.837\n   .Q5      (.26.)    2.256    0.007  316.689    0.000    2.256    2.295\n   .Q6      (.27.)    2.671    0.007  379.819    0.000    2.671    2.905\n   .Q7      (.28.)    2.546    0.007  362.580    0.000    2.546    2.709\n   .Q8      (.29.)    2.623    0.007  398.982    0.000    2.623    2.710\n   .Q9      (.30.)    2.656    0.007  364.272    0.000    2.656    2.676\n   .Q10     (.31.)    2.422    0.008  301.276    0.000    2.422    2.280\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1                0.355    0.006   59.279    0.000    0.355    0.492\n   .Q2                0.296    0.005   61.945    0.000    0.296    0.544\n   .Q3                0.358    0.006   55.512    0.000    0.358    0.401\n   .Q4                0.402    0.006   68.624    0.000    0.402    0.647\n   .Q5                0.476    0.008   60.067    0.000    0.476    0.493\n   .Q6                0.313    0.005   59.705    0.000    0.313    0.371\n   .Q7                0.364    0.006   60.237    0.000    0.364    0.412\n   .Q8                0.612    0.008   72.895    0.000    0.612    0.653\n   .Q9                0.478    0.008   62.765    0.000    0.478    0.485\n   .Q10               0.455    0.007   60.998    0.000    0.455    0.403\n    SE                0.365    0.006   64.655    0.000    1.000    1.000\n\n\nGroup 2 [2]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SE =~                                                                 \n    Q1                1.000                               0.590    0.726\n    Q2      (.p2.)    0.824    0.005  155.843    0.000    0.486    0.673\n    Q3      (.p3.)   -1.209    0.009 -129.474    0.000   -0.713   -0.765\n    Q4      (.p4.)    0.775    0.007  119.151    0.000    0.458    0.605\n    Q5      (.p5.)   -1.158    0.009 -122.177    0.000   -0.684   -0.720\n    Q6      (.p6.)    1.207    0.008  144.587    0.000    0.712    0.800\n    Q7      (.p7.)    1.192    0.009  134.597    0.000    0.704    0.786\n    Q8      (.p8.)   -0.943    0.011  -89.674    0.000   -0.557   -0.584\n    Q9      (.p9.)   -1.178    0.011 -108.710    0.000   -0.695   -0.716\n    Q10     (.10.)   -1.357    0.011 -124.870    0.000   -0.801   -0.764\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1      (.22.)    3.137    0.006  517.499    0.000    3.137    3.860\n   .Q2      (.23.)    3.244    0.005  625.225    0.000    3.244    4.487\n   .Q3      (.24.)    2.191    0.007  307.298    0.000    2.191    2.349\n   .Q4      (.25.)    3.026    0.005  570.059    0.000    3.026    3.999\n   .Q5      (.26.)    2.256    0.007  316.689    0.000    2.256    2.375\n   .Q6      (.27.)    2.671    0.007  379.819    0.000    2.671    3.001\n   .Q7      (.28.)    2.546    0.007  362.580    0.000    2.546    2.842\n   .Q8      (.29.)    2.623    0.007  398.982    0.000    2.623    2.750\n   .Q9      (.30.)    2.656    0.007  364.272    0.000    2.656    2.735\n   .Q10     (.31.)    2.422    0.008  301.276    0.000    2.422    2.309\n    SE               -0.072    0.007  -10.487    0.000   -0.123   -0.123\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1                0.312    0.004   74.215    0.000    0.312    0.472\n   .Q2                0.286    0.004   75.544    0.000    0.286    0.547\n   .Q3                0.360    0.005   67.389    0.000    0.360    0.415\n   .Q4                0.363    0.004   84.971    0.000    0.363    0.634\n   .Q5                0.434    0.006   71.101    0.000    0.434    0.482\n   .Q6                0.285    0.004   71.743    0.000    0.285    0.360\n   .Q7                0.307    0.004   70.259    0.000    0.307    0.382\n   .Q8                0.600    0.006   93.236    0.000    0.600    0.659\n   .Q9                0.459    0.007   70.403    0.000    0.459    0.487\n   .Q10               0.458    0.007   69.847    0.000    0.458    0.417\n    SE                0.348    0.005   68.390    0.000    1.000    1.000\nModel fit for the scalar model is:\nThis indicates that\nLast, I will add item uniqueness (also called item residual) as the next invariate parameter as a test for strict invariance. Evidence of strict invariance would indicate that each item’s residual is equal across gender groups.\nStrict &lt;- sem(SEM1Fac, group = \"gender\", group.equal = c(\"loadings\", \"intercepts\", \"residuals\"), data = Data_new, estimator = \"MLR\")\nsummary(Strict, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 39 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        61\n  Number of equality constraints                    29\n\n  Number of observations per group:                   \n    1                                            13938\n    2                                            20712\n\nModel Test User Model:\n                                               Standard      Scaled\n  Test Statistic                              24068.206   19096.890\n  Degrees of freedom                                 98          98\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.260\n    Yuan-Bentler correction (Mplus variant)                        \n  Test statistic for each group:\n    1                                         7670.981    7670.981\n    2                                         11425.908   11425.908\n\nModel Test Baseline Model:\n\n  Test statistic                            191839.246  138203.437\n  Degrees of freedom                                90          90\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.388\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.875       0.862\n  Tucker-Lewis Index (TLI)                       0.885       0.874\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.875\n  Robust Tucker-Lewis Index (TLI)                            0.885\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)            -370265.271 -370265.271\n  Scaling correction factor                                  0.583\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)    -358231.168 -358231.168\n  Scaling correction factor                                  1.224\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                              740594.543  740594.543\n  Bayesian (BIC)                            740865.040  740865.040\n  Sample-size adjusted Bayesian (SABIC)     740763.345  740763.345\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.119       0.106\n  90 Percent confidence interval - lower         0.118       0.105\n  90 Percent confidence interval - upper         0.120       0.107\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.119\n  90 Percent confidence interval - lower                     0.117\n  90 Percent confidence interval - upper                     0.120\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.055       0.055\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SE =~                                                                 \n    Q1                1.000                               0.605    0.725\n    Q2      (.p2.)    0.825    0.005  155.805    0.000    0.499    0.679\n    Q3      (.p3.)   -1.211    0.009 -129.251    0.000   -0.732   -0.774\n    Q4      (.p4.)    0.776    0.007  119.227    0.000    0.469    0.606\n    Q5      (.p5.)   -1.160    0.010 -121.998    0.000   -0.702   -0.722\n    Q6      (.p6.)    1.208    0.008  144.241    0.000    0.731    0.802\n    Q7      (.p7.)    1.193    0.009  133.805    0.000    0.721    0.782\n    Q8      (.p8.)   -0.945    0.011  -89.635    0.000   -0.571   -0.592\n    Q9      (.p9.)   -1.181    0.011 -108.676    0.000   -0.714   -0.723\n    Q10     (.10.)   -1.360    0.011 -124.713    0.000   -0.822   -0.773\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1      (.22.)    3.138    0.006  521.185    0.000    3.138    3.764\n   .Q2      (.23.)    3.244    0.005  633.822    0.000    3.244    4.420\n   .Q3      (.24.)    2.191    0.007  307.173    0.000    2.191    2.316\n   .Q4      (.25.)    3.028    0.005  579.410    0.000    3.028    3.912\n   .Q5      (.26.)    2.258    0.007  318.460    0.000    2.258    2.325\n   .Q6      (.27.)    2.671    0.007  380.326    0.000    2.671    2.931\n   .Q7      (.28.)    2.542    0.007  362.574    0.000    2.542    2.756\n   .Q8      (.29.)    2.623    0.007  399.159    0.000    2.623    2.719\n   .Q9      (.30.)    2.656    0.007  364.083    0.000    2.656    2.688\n   .Q10     (.31.)    2.423    0.008  301.542    0.000    2.423    2.276\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1      (.11.)    0.330    0.004   92.658    0.000    0.330    0.474\n   .Q2      (.12.)    0.290    0.003   97.356    0.000    0.290    0.539\n   .Q3      (.13.)    0.359    0.004   84.548    0.000    0.359    0.401\n   .Q4      (.14.)    0.379    0.004  108.011    0.000    0.379    0.633\n   .Q5      (.15.)    0.451    0.005   90.869    0.000    0.451    0.478\n   .Q6      (.16.)    0.296    0.003   90.309    0.000    0.296    0.357\n   .Q7      (.17.)    0.330    0.004   89.506    0.000    0.330    0.388\n   .Q8      (.18.)    0.604    0.005  114.912    0.000    0.604    0.649\n   .Q9      (.19.)    0.466    0.005   89.808    0.000    0.466    0.478\n   .Q10     (.20.)    0.457    0.005   87.872    0.000    0.457    0.403\n    SE                0.366    0.006   64.706    0.000    1.000    1.000\n\n\nGroup 2 [2]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SE =~                                                                 \n    Q1                1.000                               0.589    0.716\n    Q2      (.p2.)    0.825    0.005  155.805    0.000    0.485    0.669\n    Q3      (.p3.)   -1.211    0.009 -129.251    0.000   -0.713   -0.765\n    Q4      (.p4.)    0.776    0.007  119.227    0.000    0.457    0.596\n    Q5      (.p5.)   -1.160    0.010 -121.998    0.000   -0.683   -0.713\n    Q6      (.p6.)    1.208    0.008  144.241    0.000    0.711    0.794\n    Q7      (.p7.)    1.193    0.009  133.805    0.000    0.702    0.774\n    Q8      (.p8.)   -0.945    0.011  -89.635    0.000   -0.556   -0.582\n    Q9      (.p9.)   -1.181    0.011 -108.676    0.000   -0.695   -0.713\n    Q10     (.10.)   -1.360    0.011 -124.713    0.000   -0.800   -0.764\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1      (.22.)    3.138    0.006  521.185    0.000    3.138    3.816\n   .Q2      (.23.)    3.244    0.005  633.822    0.000    3.244    4.474\n   .Q3      (.24.)    2.191    0.007  307.173    0.000    2.191    2.353\n   .Q4      (.25.)    3.028    0.005  579.410    0.000    3.028    3.950\n   .Q5      (.26.)    2.258    0.007  318.460    0.000    2.258    2.358\n   .Q6      (.27.)    2.671    0.007  380.326    0.000    2.671    2.982\n   .Q7      (.28.)    2.542    0.007  362.574    0.000    2.542    2.801\n   .Q8      (.29.)    2.623    0.007  399.159    0.000    2.623    2.744\n   .Q9      (.30.)    2.656    0.007  364.083    0.000    2.656    2.725\n   .Q10     (.31.)    2.423    0.008  301.542    0.000    2.423    2.313\n    SE               -0.072    0.007  -10.473    0.000   -0.123   -0.123\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Q1      (.11.)    0.330    0.004   92.658    0.000    0.330    0.487\n   .Q2      (.12.)    0.290    0.003   97.356    0.000    0.290    0.552\n   .Q3      (.13.)    0.359    0.004   84.548    0.000    0.359    0.414\n   .Q4      (.14.)    0.379    0.004  108.011    0.000    0.379    0.645\n   .Q5      (.15.)    0.451    0.005   90.869    0.000    0.451    0.491\n   .Q6      (.16.)    0.296    0.003   90.309    0.000    0.296    0.369\n   .Q7      (.17.)    0.330    0.004   89.506    0.000    0.330    0.401\n   .Q8      (.18.)    0.604    0.005  114.912    0.000    0.604    0.661\n   .Q9      (.19.)    0.466    0.005   89.808    0.000    0.466    0.491\n   .Q10     (.20.)    0.457    0.005   87.872    0.000    0.457    0.416\n    SE                0.346    0.005   68.407    0.000    1.000    1.000\nModel fit for the strict model is:\nI have been comparing each model using model fit indicies but there is another way to compare models that compares AIC, BIC, and Chi-Square test of differences. This route also provides probability estimates for each parameter restriction.\nJust as with model fit indicies, the more restrictive model will be compared to a less restrictive model. Using the four models I computed above, the code is relatively simple using the compareFit function in the semTools package.\nlibrary(semTools)\n\nWarning: package 'semTools' was built under R version 4.4.3\n\n\n \n\n\n###############################################################################\n\n\nThis is semTools 0.5-7\n\n\nAll users of R (or SEM) are invited to submit functions or ideas for functions.\n\n\n###############################################################################\n\n\n\nAttaching package: 'semTools'\n\n\nThe following objects are masked from 'package:psych':\n\n    reliability, skew\n\nsummary(compareFit(Configural, Metric))\n\n################### Nested Model Comparison #########################\n\nScaled Chi-Squared Difference Test (method = \"satorra.bentler.2001\")\n\nlavaan-&gt;unknown():  \n   lavaan NOTE: The \"Chisq\" column contains standard test statistics, not the \n   robust test that should be reported per model. A robust difference test is \n   a function of two standard (not robust) statistics.\n           Df    AIC    BIC Chisq Chisq diff Df diff Pr(&gt;Chisq)    \nConfigural 70 739500 740007 22918                                  \nMetric     79 739557 739988 22992     73.219       9  3.549e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n           chisq.scaled df.scaled pvalue.scaled rmsea.robust cfi.robust\nConfigural   17584.988†        70          .000        .137       .881†\nMetric       18091.475         79          .000        .129†      .881 \n           tli.robust  srmr         aic         bic\nConfigural      .847  .053† 739500.051† 740007.235 \nMetric          .864† .053  739556.676  739987.781†\n\n################## Differences in Fit Indices #######################\n                    df.scaled rmsea.robust cfi.robust tli.robust  srmr    aic\nMetric - Configural         9       -0.008          0      0.017 0.001 56.624\n                        bic\nMetric - Configural -19.453"
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\nRacism underlies seemingly race-neutral conservative criticisms of DEI statements among Black and White people in the United States\n\n\n\n\n\n\nDiversity Initiatives\n\n\nSelection\n\n\nPolitical Ideology\n\n\nSocial Dominance Theory\n\n\n\n\n\n\nJan 22, 2024\n\n\nAbigail M. Folberg, Laura Brooks Dueland, Matthew A. Swanson, Sarah Stepanek, Mikki Hebl, Carey S. Ryan\n\n\n\n\n\n\n\nCan I Remain True to Myself at Work? An Experimental Study of Psychologically Safe Versus Unsafe Workplaces on LGBTQ and Straight Perceptions of Authenticity, Vigilance, Belongingness, and Well-Being\n\n\n\n\n\n\nAuthenticity\n\n\nPsychological Safety\n\n\nVigilance\n\n\nWell-being\n\n\n\n\n\n\nDec 15, 2022\n\n\nMatthew A. Swanson\n\n\n\n\n\n\n\nLeveraging DEI Policies and Training to Navigate Conflict in Organizations\n\n\n\n\n\n\nConflict Management\n\n\nDEI Policy\n\n\nOrganizational Training\n\n\nOrganizational Climate\n\n\nTraining Evaluation\n\n\n\n\n\n\nAug 2, 2021\n\n\nDanielle Crawford, Laura Brooks Dueland, Matthew A. Swanson, Sarah Stepanek, Carey S. Ryan\n\n\n\n\n\n\n\nEssential Meaningful Work: A Multistage Proposal for the Development and Validation of the Essential Meaningful Work Inventory (EMWI)\n\n\n\n\n\n\nMeaningful Work\n\n\nEssential Workers\n\n\nScale Development\n\n\nGeneralizable Research\n\n\n\n\n\n\nMay 26, 2021\n\n\nMatthew A. Swanson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Publication Pages/12-15-2022.html",
    "href": "Publication Pages/12-15-2022.html",
    "title": "Can I Remain True to Myself at Work? An Experimental Study of Psychologically Safe Versus Unsafe Workplaces on LGBTQ and Straight Perceptions of Authenticity, Vigilance, Belongingness, and Well-Being",
    "section": "",
    "text": "Publication"
  },
  {
    "objectID": "Publication Pages/12-15-2022.html#abstract",
    "href": "Publication Pages/12-15-2022.html#abstract",
    "title": "Can I Remain True to Myself at Work? An Experimental Study of Psychologically Safe Versus Unsafe Workplaces on LGBTQ and Straight Perceptions of Authenticity, Vigilance, Belongingness, and Well-Being",
    "section": "Abstract",
    "text": "Abstract\nResearchers and managers have begun to consider ways to increase employee authenticity as greater authenticity has been linked to better well-being and mental health, lower stress, and fewer negative life events at work. However, authenticity can conflict with workplace values and norms. Further, the role of identity, such as sexual orientation, is less clear in shaping authentic self-expression at work. I experimentally manipulated psychological safety and evaluated subsequent judgments of authenticity, vigilance, well-being, and belongingness. Participants were straight (n = 294) and LGBTQ (n = 272) workers recruited through Prolific Academic who were randomly assigned to either a psychologically safe or unsafe condition. Using a narrative identity primer, participants were asked to describe in detail an experience at their current job that was either “healthy” or “unhealthy” with a focus on how the participant felt and the significance of the event to the participant. Results indicated that psychologically safe (vs. unsafe) environments facilitated greater authenticity and lower vigilance which, in turn, related to greater well-being and belongingness. Further, these relationships were largely the same for LGBTQ and straight participants. However, qualitative accounts revealed that LGBTQ and straight participants paid attention to different aspects of the work environment when determining how psychologically safe (or unsafe) the environment was. Both LGBTQ and straight participants drew upon experiences with leadership, workplace support and abuse, safety, and COVID-19 in their depictions of psychologically safe or unsafe workplaces. Yet, for LGBTQ participants, it was largely the treatment of their sexual identities that shaped experiences with psychological safety. Thus, while the path to psychological safety differs for LGBTQ and straight employees, psychological safety appears to be similarly important in facilitating employee authenticity."
  },
  {
    "objectID": "Publication Pages/1-22-2024.html",
    "href": "Publication Pages/1-22-2024.html",
    "title": "Racism underlies seemingly race-neutral conservative criticisms of DEI statements among Black and White people in the United States",
    "section": "",
    "text": "Publication  Bloomberg  American Psychological Association"
  },
  {
    "objectID": "Publication Pages/1-22-2024.html#abstract",
    "href": "Publication Pages/1-22-2024.html#abstract",
    "title": "Racism underlies seemingly race-neutral conservative criticisms of DEI statements among Black and White people in the United States",
    "section": "Abstract",
    "text": "Abstract\nWe examined how potential job candidates react to a hiring organization that requests diversity, equity and inclusion (DEI) statements, which conservatives in the United States and elsewhere have criticized as being unrelated to job function and inappropriately political or ideological. Across three studies (two of which were pre-registered), we compared reactions to requests for DEI (vs. teamwork or conservative values) statements as a function of race (Black vs. White), political conservatism and symbolic racism (Total N = 1108). When a DEI (vs. teamwork or politically conservative values) statement was requested, participants who were more (vs. less) conservative perceived the organization as less just, expressed less interest in the job, and expected poorer person-organization fit, even when a job-related rationale was provided. Further, participants who were more (vs. less) conservative evaluated a request for a statement consistent with conservative values more favourably. Thus, criticisms that DEI statements are overly political are not applied to other statements that might elicit similar concerns. Moreover, an internal meta-analysis suggested that the relationships of conservatism to justice and interest (but not person-organization fit) in response to requests for DEI (vs. teamwork) statements were not independent of racism. Findings were consistent with social dominance theory; racism may underlie seemingly race-neutral backlash to DEI statements."
  },
  {
    "objectID": "Portfolio pages/MLMAEffect.html",
    "href": "Portfolio pages/MLMAEffect.html",
    "title": "Using Multilevel Meta-Analysis to Better Understand Complex Business Research",
    "section": "",
    "text": "As practitioners of data science and IO psychology, we are sometimes tasked with understanding the effects of a particular training program or yearly event. In a perfect world, we would begin prepping the organization for evaluation work and would draft up timelines for evaluation and would work with the organization to identify potential samples. We would also identify any relevant data that has already been collected and potentially work that information into the evaluation schedules. Next, and dependent upon the type and length of training to be evaluated, the evaluation schedule would be enacted and data would be collected and analyzed (likely over the course of several training cycles to evaluate longitudinal impacts) and presented to the organization. However, what happens when the organization does not have the time and money to wait for the outcome of the evaluation cycle? For example, imagine a hospital struggling with teamwork among their frontline staff.\nIn this example, communication among frontline staff has broken down and become siloed, which has led to several critical errors while admitting patients. The hospital wants to implement team training that targets team cohesion and problem solving/conflict resolution. Currently, there is a very popular training course on the market that claims to target these outcomes that the hospital is considering (the hospital also does not have the time and money to develop their own in-house training). However, the training is expensive so the hospital has hired a consultant to review the literature on this training and present their findings on the effects reported in research conducted on this training course.\nThere are a few ways the consultant could go about reviewing and presenting the literature that has been conducted on this teamwork training course. First, the consultant could identify research most relevant to the organization (ex. finding research that evaluates that training course using similar samples) and create summary sheets for each article. These summaries then provide the organization some initial information on the types of research that has been conducted on the training course, the range of effect sizes found, and some common outcomes that training participation has been related to. However, one draw back to this approach is that the consultant cannot speak to the average effect size across these studies and the consultant runs the risk of missing the forest for the trees, especially when there exists a large amount of literature evaluating the training. Additionally, it seems unlikely that the hospital has the time or desire to read summaries of dozens of relevant studies. As such, the consultant may select focal articles to present (which can obfuscate the understanding of the training’s efficacy), or the consultant may look to places where relevant research has been summarized before, typically in the form of a meta-analysis.\nWithout going into to much detail, meta-analysis is a statistical approach that combines effect sizes from multiple separate research studies (Deeks, Higgins, and Altman 2019). Meta-analysis typically takes the form of a fixed-effect model (which assumes that studies share an effect size) or a random-effects model (which assumes a distribution of effect sizes) (see (Borenstein et al. 2010) for more details on the common meta-analysis models). However, and pertinent to this example, these models assume independence of effect sizes. In other words, that the effect sizes come from discrete samples or participants. This becomes a challenge for training research that often samples the same participants multiple times (i.e., baseline test, pre-test, post-test, follow-up test). Because of this challenge, researchers will often select a single effect size, average across effect sizes, or they do not conduct meta-analysis with the studies (Morris 2023).\nTypically for training evaluation research, research studies will report the means, standard deviations, and sample sizes for each group at each data collection time point. As it pertains to the goals of this project, I have identified 10 research studies that have evaluated the teamwork training course. Each of these 10 studies evaluated three groups of participants three times: a baseline pre test assessment, a post test assessment that occurred right after the training event was completed, and then a follow-up test that occurred anywhere from 1 to 12 months after the training event. I will discuss the groups of participants when I test for the effect of moderators here. The means and standard deviations calculated at each time point represents an average training “effectiveness” metric that has been decided upon by the researchers who conducted the research contained in these 10 studies (i.e., it is shared metric of training effectiveness). Note: I created this data set for the purpose of this demonstration. Much greater detail on how to synthesize research and how to locate, code, and describe potential studies can be found in The Handbook of Research Synthesis and Mata-Analysis by Cooper, Hedges, and Valentine (2019).\nFor the sake of this project, there will be no attrition between time points, but please recognize that this is relatively rare within this type of research. Consultants will have to make decisions over how to treat sample attrition as the analyses require a single sample size for each group.\nIn this project, I will demonstrate how to calculate the meta-analytic effects for the differences between the pre and (separately) the post and follow-up time points and illustrate the information that can be gleaned from this approach.\nFirst, I will load in the data.\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ez)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(janitor)\n\nData &lt;- read_excel(\"C:/Users/matts/OneDrive/Desktop/002 Matt Desktop/WebsiteData/Data/MLMA example data.xlsx\")\nhead(Data)\n\n# A tibble: 6 × 11\n  Study_ID Effect_ID  Mod1 `Months since follow`     N Mean_pre Mean_post\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;                 &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1        1         1     1                     2   306     3.5       5.86\n2        1         2     2                     6   248     2.71      5.79\n3        1         3     3                    12   329     2.48      3.3 \n4        2         4     1                     4    82     3.78      4.42\n5        2         5     2                     2   418     3.07      6.4 \n6        2         6     3                     1   309     4.12      6.63\n# ℹ 4 more variables: Mean_follow &lt;dbl&gt;, SD1 &lt;dbl&gt;, SD2 &lt;dbl&gt;, SD3 &lt;dbl&gt;\nRecent development in meta-analysis has provided an avenue for handling multiple effect sizes from the same group: multilevel meta-analysis (MLMA). The key difference between MLMA and other meta-analyses is that MLMA partitions effect size variation across three levels: within and between study variation (also discussed as heterogeneity), and the individual effect size sampling error. The benefit of implementing MLMA here is that an unbiased estimate of the population effect can be calculated using data from all participants across each time point, which also addresses issues of effect size independence.\nNotation of the formulas underpinning each level of a MLMA is as follows:\nFormula for Level 1: \\[\n\\hat\\theta_{ij} = \\theta_{ij} + \\epsilon_{ij}\n\\]\nFormula for Level 2: \\[\n\\theta_{ij} = \\kappa_{j} + \\zeta_{(2)ij}\n\\]\nFormula for Level 3: \\[\n\\kappa_{j} = \\mu + \\zeta_{(3)j}\n\\]\nwhereby \\(\\hat\\theta_{ij}\\) represents an estimation of the true effect \\(\\theta_{ij}\\). This estimation occurs for each effect size, i, nested in group j. \\(\\kappa_{j}\\) represents the average effect size within each group and μ represents the average population effect (Harrer et al. 2021). Lastly, \\(\\zeta_{(2)ij}\\) refers to the within-level heterogeneity (level 2) and \\(\\zeta_{(3)j}\\) represents the between-level heterogeneity (level 3). As demonstrated by (Harrer et al. 2021), these three formulas can be combined to form the three-level meta-regression model: \\[\n\\hat\\theta_{ij} = \\mu + \\zeta_{(2)ij} + \\zeta_{(3)j} + \\epsilon_{ij}\n\\]\nHere, \\(\\mu\\) represents the overall average population effect (See Cheung (2014) for more details on the development of MLMA formulas).\nWith this information, standardized mean differences can be calculated for each time point comparison (e.g., pre vs. post time point differences, pre vs. follow-up time point differences). These standardized mean differences are still biased estimates so they will need to be converted to Hedges’ g, which is an unbiased estimate (BORENSTEIN and HEDGES 2019).\nGiven that the same group of participants provide data at multiple time points in each study, we will assume a correlation of r = .50, as recommended by Morris and DeShon (2002). Note that if the correlation between time points is provided in the studies themselves, OR if access to the raw data is provided, the consultant should calculate the correlations themselves and avoid the assumption of r = .50.\nThere is also an important correction factor that has to be calculated as well when calculating calculating Hedges g. This correction factor, J, is applied to the formula and is notated as:\n\\[\nJ(n-1)= 1-\\frac{3}{4_{df}-1}\n\\]\nwhere \\(J(n-1)\\) is the number of pairs. J is calculated for each effect and multiplied by the standardized mean difference (SMD), which produces g. We also use these correction factors to calculate an unbiased estimate of variance (BORENSTEIN and HEDGES 2019).\nI will demonstrate with the code below how to calculate these values using the data I have just loaded. I will need to add in a row of data for the correlations between time points (if I could not gather that information from the studies themselves). Reminder: since I am using standardized mean differences in my calculation of the meta effect, decisions have to be made for which comparisons to look at. Of course, all time points can be compared to each other but this becomes tedious and often not informative. Thus, I will note here that pre scores will be compared to both post and follow-up time points (I will touch on quadratic effects later on). Of course, the consultant could evaluate the effect size for post to follow-up time points but as both of these time points occur after the training events, this information is largely irrelevant to the hospital’s goals of understanding the potential effect of this training.\n# set correlation = .5\nri &lt;- .5\nData$ri &lt;- ri\n\n#Comparing Pre and Post Time Points\n\n#compute effect size  for pre vs. post\nData &lt;- escalc(measure = \"SMCR\", \n         m1i= Mean_pre, \n         m2i = Mean_post,\n         sd1i = SD1, \n         sd2i = SD2, \n         ni = N,\n        ri = ri,\n        flip = TRUE,\n        data = Data)\n\nData &lt;- Data |&gt; mutate(df = N - 1)\n\n\n#compute standard error for Cohen's d using sampling variance\nData &lt;- Data |&gt; mutate(std_error_d = sqrt(vi / N))\n\n#Need to convert Cohen's d to Hedges' g\n\n##correction factor J\nData &lt;- Data |&gt; mutate(j =  1 - (3 / (4 * df - 1)))\n\n##Hedges' g: effect size\nData &lt;- Data |&gt; mutate(hedges_g =  yi * j)\n##Hedges' g: std error\nData &lt;- Data |&gt; mutate(std_error_g =  std_error_d * j)\n#Hedges' g: variance \nData &lt;- Data |&gt;  mutate(var_g = vi * j) \nhead(Data)\n\n\n  Study_ID Effect_ID Mod1 Months.since.follow   N Mean_pre Mean_post \n1        1         1    1                   2 306     3.50      5.86 \n2        1         2    2                   6 248     2.71      5.79 \n3        1         3    3                  12 329     2.48      3.30 \n4        2         4    1                   4  82     3.78      4.42 \n5        2         5    2                   2 418     3.07      6.40 \n6        2         6    3                   1 309     4.12      6.63 \n  Mean_follow  SD1  SD2  SD3  ri     yi     vi  df std_error_d         j \n1        4.84 0.43 0.72 1.19 0.5 5.4749 0.0522 305 0.013066610 0.9975390 \n2        3.10 1.06 0.87 1.61 0.5 2.8968 0.0210 247 0.009191252 0.9969605 \n3        2.14 0.71 1.40 0.88 0.5 1.1523 0.0051 328 0.003920716 0.9977117 \n4        3.90 1.49 1.20 0.85 0.5 0.4255 0.0133  81 0.012735244 0.9907121 \n5        5.21 1.55 1.16 1.82 0.5 2.1445 0.0079 417 0.004345570 0.9982004 \n6        3.42 1.56 1.61 1.97 0.5 1.6051 0.0074 308 0.004895294 0.9975630 \n   hedges_g std_error_g       var_g \n1 5.4613894 0.013034452 0.052116728 \n2 2.8880221 0.009163315 0.020887141 \n3 1.1496496 0.003911744 0.005045819 \n4 0.4215863 0.012616960 0.013175765 \n5 2.1406610 0.004337749 0.007879297 \n6 1.6011411 0.004883364 0.007386799\nAs you can see from the dataset, we now have calculated Hedge’s g and its accompanying standard error and variance for each pre and post comparison contained in these 10 research studies. For additional information on the choices I make for the code and what each argument entails, I suggest reviewing the CRAN repository for the metafor package. Since I compared pre to post scores, I added the argument “flip = true” so that the effect sizes would be in the expected direction (assuming that the training has some positive effect).\nNow I will run the initial comparison of pre and post scores using the values I just prepared above. I will call this model “Pre_V_Post” in the code.\n#Pre Vs. Post Comparison\noptions(scipen = 999)\n\n#Calculating MLMA effect\nPre_v_Post &lt;- rma.mv(yi = hedges_g, \n                        V = var_g, \n                        test = \"knha\",\n                        random = ~ 1 | Study_ID/Effect_ID,\n                        data = Data)\nsummary(Pre_v_Post)\n\n\nMultivariate Meta-Analysis Model (k = 30; method: REML)\n\n  logLik  Deviance       AIC       BIC      AICc   \n-59.3806  118.7613  124.7613  128.8631  125.7213   \n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed              factor \nsigma^2.1  0.5260  0.7253     10     no            Study_ID \nsigma^2.2  2.8925  1.7007     30     no  Study_ID/Effect_ID \n\nTest for Heterogeneity:\nQ(df = 29) = 3191.6410, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    tval  df    pval   ci.lb   ci.ub    \n  1.0220  0.3971  2.5739  29  0.0154  0.2099  1.8340  * \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint.rma.mv(Pre_v_Post)\n\n\n          estimate  ci.lb  ci.ub \nsigma^2.1   0.5260 0.0000 3.5401 \nsigma.1     0.7253 0.0000 1.8815 \n\n          estimate  ci.lb  ci.ub \nsigma^2.2   2.8925 1.5943 5.7841 \nsigma.2     1.7007 1.2627 2.4050 \n\npredict.rma(Pre_v_Post)\n\n\n   pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n 1.0220 0.3971 0.2099 1.8340 -2.8457 4.8897 \n\nround(Pre_v_Post$tau2, 4)\n\n[1] 0\n\nPre_v_Post$sigma2\n\n[1] 0.5260172 2.8925091\n\nfunnel(Pre_v_Post, xlab = \"Standardized Mean Difference\")\n\n\n\n\n\n\n\nforest(Pre_v_Post, header = c(\"Effect Number\", \"Weight (%)   SMD   [95% CI]\"), slab = paste(Effect_ID), mlab = \"Pooled Estimate\",  order = \"obs\", xlab = \"Standardized Mean Difference (95% CI)\", digits = 2L, cex = 1, shade = \"zebra\", showweights = TRUE)\nResults from this model indicate an overall positive and significant mean effect size, g = 1.02, 95% Confidence Interval (CI)[0.210, 1.834]. Given that the effect is positive, we can conclude that there is meta-analytic evidence that training outcomes improved immediately upon completion of the training. I also presented the SMDs and effect sizes used in this calculation via a funnel plot and as a forest plot. The forest plot visually displays the SMD for each effect size used in this model as well as their 95% CI ranges and the weight each effect contributes to the pooled estimate. Model results also indicate significant heterogeneity, Q(29) = 3191.64, p &lt;.001."
  },
  {
    "objectID": "Portfolio pages/MLMAEffect.html#calculating-effect-sizes",
    "href": "Portfolio pages/MLMAEffect.html#calculating-effect-sizes",
    "title": "Using Multilevel Meta-Analysis to Better Understand Complex Business Research",
    "section": "Calculating Effect Sizes",
    "text": "Calculating Effect Sizes"
  },
  {
    "objectID": "Portfolio pages/SelfEsteem SEM & IRT.html#item-correlations",
    "href": "Portfolio pages/SelfEsteem SEM & IRT.html#item-correlations",
    "title": "Psychometric Properties of Self Esteem via SEM and IRT",
    "section": "Item Correlations",
    "text": "Item Correlations"
  },
  {
    "objectID": "Portfolio pages/SelfEsteem SEM & IRT.html#multigroup-sem",
    "href": "Portfolio pages/SelfEsteem SEM & IRT.html#multigroup-sem",
    "title": "Psychometric Properties of Self Esteem via SEM and IRT",
    "section": "Multigroup SEM",
    "text": "Multigroup SEM"
  },
  {
    "objectID": "Portfolio pages/MLMAQuad.html",
    "href": "Portfolio pages/MLMAQuad.html",
    "title": "Testing for Quadratic Effects in Multilevel Meta-Analysis",
    "section": "",
    "text": "As described in the overall goal of this project, the hospital is interested in understanding what the effect they could reasonably expect to have when implementing a new teamwork training course into their workforce. Using data from previously published literature, 30 studies were identified across ten research papers. Each of these studies evaluated participant teamwork skills prior to the implementation of the training, right after the training, and several months later. I have previously demonstrated how to find the overall effect for a comparison of pre to post scores and the overall effect comparing pre to follow-up scores. However, what about a model that considers the three time points in each study? It is reasonable to expect that teamwork scores will increase right after training (as compared to pre scores) and then plateau or even weaken over time (but still remain significantly greater than pre scores). To test for this, we will have to change the shape of the data a bit and add in a quadratic term. First, I will load in the data.\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ez)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(janitor)\n\nQuadData &lt;- read_excel(\"C:/Users/matts/OneDrive/Desktop/MLMA Quad Data.xlsx\")\nhead(QuadData)\n\n# A tibble: 6 × 11\n  Study_ID Effect_ID  Mod1 Months_since_post     N Mean_pre Mean_post\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1        1         1     1                 0   306     3.5       5.86\n2        1         2     1                 2   306     3.5      NA   \n3        1         3     2                 0   248     2.71      5.79\n4        1         4     2                 6   248     2.71     NA   \n5        1         5     3                 0   329     2.48      3.3 \n6        1         6     3                12   329     2.48     NA   \n# ℹ 4 more variables: Mean_follow &lt;dbl&gt;, SD1 &lt;dbl&gt;, SD2 &lt;dbl&gt;, SD3 &lt;dbl&gt;"
  },
  {
    "objectID": "Portfolio pages/MLMAMod.html",
    "href": "Portfolio pages/MLMAMod.html",
    "title": "Performing Moderation Analysis Within Multilevel Meta-Analysis",
    "section": "",
    "text": "Project Goal\n\n\nContinuing the evaluation of teamwork training effects as discussed in my project on multilevel meta-analysis (MLMA), which can be found here, I consider how these MLMA effects may depend on the presence of moderators. Specifically, I am interested in understanding if the positive training effects found when comparing pre and post-test scores depends, in part, on the employee group assessed. In the data set for this project, there is a variable called “Mod1” which is a categorical variable denoting the employee group that is providing the data for each effect. The key for this variable is as follows: 1 = frontline staff, 2 = care management staff, 3 = top-level management staff.\n\nData Preparation\n\nFirst, I will rename and add labels for the moderating variable from the dataset I created when calculating Hedges g using the pre vs. post standardized mean difference. I will save these changes as a new dataset.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ez)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(metafor)\n\nData &lt;- read_csv(\"C:/Users/matts/OneDrive/Desktop/002 Matt Desktop/WebsiteData/Data/PreVPostAggData.csv\")\n\nDataMod &lt;- Data %&gt;%\n  rename(Employee_Group = Mod1) %&gt;%\n  mutate(Employee_Group = factor(Employee_Group, levels = c(1,2,3),\n                                 labels = c(\"Frontline\", \"CareManage\", \"TopManage\")))\nstr(DataMod)\n\ntibble [30 × 23] (S3: tbl_df/tbl/data.frame)\n $ ...1               : num [1:30] 1 2 3 4 5 6 7 8 9 10 ...\n $ Study_ID           : num [1:30] 1 1 1 2 2 2 3 3 3 4 ...\n $ Effect_ID          : num [1:30] 1 2 3 4 5 6 7 8 9 10 ...\n $ Employee_Group     : Factor w/ 3 levels \"Frontline\",\"CareManage\",..: 1 2 3 1 2 3 1 2 3 1 ...\n $ Months.since.follow: num [1:30] 2 6 12 4 2 1 1 4 12 12 ...\n $ N                  : num [1:30] 306 248 329 82 418 309 244 429 462 121 ...\n $ Mean_pre           : num [1:30] 3.5 2.71 2.48 3.78 3.07 4.12 5.11 3.22 6.13 2.67 ...\n $ Mean_post          : num [1:30] 5.86 5.79 3.3 4.42 6.4 6.63 5.28 5.49 5.99 3.25 ...\n $ Mean_follow        : num [1:30] 4.84 3.1 2.14 3.9 5.21 3.42 4.98 4.78 6.45 4.31 ...\n $ SD1                : num [1:30] 0.43 1.06 0.71 1.49 1.55 1.56 0.81 1.26 1.79 1.3 ...\n $ SD2                : num [1:30] 0.72 0.87 1.4 1.2 1.16 1.61 1.06 1.41 0.63 1.41 ...\n $ SD3                : num [1:30] 1.19 1.61 0.88 0.85 1.82 1.97 0.25 0.98 1.79 1.19 ...\n $ ri                 : num [1:30] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...\n $ yi                 : num [1:30] 5.475 2.897 1.152 0.426 2.145 ...\n $ vi                 : num [1:30] 0.05225 0.02095 0.00506 0.0133 0.00789 ...\n $ df                 : num [1:30] 305 247 328 81 417 308 243 428 461 120 ...\n $ std_error_d        : num [1:30] 0.01307 0.00919 0.00392 0.01274 0.00435 ...\n $ j                  : num [1:30] 0.998 0.997 0.998 0.991 0.998 ...\n $ hedges_g           : num [1:30] 5.461 2.888 1.15 0.422 2.141 ...\n $ std_error_g        : num [1:30] 0.01303 0.00916 0.00391 0.01262 0.00434 ...\n $ var_g              : num [1:30] 0.05212 0.02089 0.00505 0.01318 0.00788 ...\n $ se_cor             : num [1:30] 0.1143 0.127 0.1103 0.2209 0.0978 ...\n $ agg_effect         : num [1:30] 3.17 3.17 3.17 1.39 1.39 ...\n\n\nYou will notice in this dataset that each row of data already has yi, vi, and Hedges g values calculated. I did this step here and would have to be conducted prior to adding any moderating variables to the meta models. Also note that these values correspond to pre vs. post standardized mean differences; if I wanted to add a moderator to a MLMA model that compares the effect sizes for pre and follow-up time points, I would have to calculate the yi, vi, and Hedges g for this comparison prior to any moderation analyses.\n\nMLMA Moderation Analysis\n\nI will now add in the moderating variable, Employee_Group, to the MLMA model that compares scores between the pre and post time points. I will also run a model that does not include the intercept as that aids in interpreting the effect sizes for each level of the moderator.\n\nPrevPost_EmployTypeMod &lt;- rma.mv(yi = hedges_g, \n                        V = var_g,\n                        mods = ~ factor(Employee_Group),\n                        random = ~ 1 | Study_ID/Effect_ID, \n                        data = DataMod)\nPrevPost_EmployTypeMod\n\n\nMultivariate Meta-Analysis Model (k = 30; method: REML)\n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed              factor \nsigma^2.1  0.6611  0.8131     10     no            Study_ID \nsigma^2.2  2.5181  1.5868     30     no  Study_ID/Effect_ID \n\nTest for Residual Heterogeneity:\nQE(df = 27) = 3173.2511, p-val &lt; .0001\n\nTest of Moderators (coefficients 2:3):\nQM(df = 2) = 5.0769, p-val = 0.0790\n\nModel Results:\n\n                                  estimate      se     zval    pval    ci.lb \nintrcpt                             1.7092  0.5662   3.0186  0.0025   0.5994 \nfactor(Employee_Group)CareManage   -0.4895  0.7126  -0.6870  0.4921  -1.8861 \nfactor(Employee_Group)TopManage    -1.5734  0.7145  -2.2020  0.0277  -2.9738 \n                                    ci.ub     \nintrcpt                            2.8189  ** \nfactor(Employee_Group)CareManage   0.9070     \nfactor(Employee_Group)TopManage   -0.1730   * \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n##moderator w/o intercept\nPrevPost_EmployTypeMod_Nointer &lt;- rma.mv(yi = hedges_g, \n            V = var_g,\n            mods =  ~0 + factor(Employee_Group),\n            random = ~ 1 | Study_ID/Effect_ID, \n            data = DataMod)\n\nPrevPost_EmployTypeMod_Nointer\n\n\nMultivariate Meta-Analysis Model (k = 30; method: REML)\n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed              factor \nsigma^2.1  0.6611  0.8131     10     no            Study_ID \nsigma^2.2  2.5181  1.5868     30     no  Study_ID/Effect_ID \n\nTest for Residual Heterogeneity:\nQE(df = 27) = 3173.2511, p-val &lt; .0001\n\nTest of Moderators (coefficients 1:3):\nQM(df = 3) = 12.0177, p-val = 0.0073\n\nModel Results:\n\n                                  estimate      se    zval    pval    ci.lb \nfactor(Employee_Group)Frontline     1.7092  0.5662  3.0186  0.0025   0.5994 \nfactor(Employee_Group)CareManage    1.2196  0.5651  2.1581  0.0309   0.1120 \nfactor(Employee_Group)TopManage     0.1358  0.5676  0.2392  0.8109  -0.9767 \n                                   ci.ub     \nfactor(Employee_Group)Frontline   2.8189  ** \nfactor(Employee_Group)CareManage  2.3273   * \nfactor(Employee_Group)TopManage   1.2483     \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI tested for the moderating effect of employee group (i.e., frontline staff, care management staff, & top-level management staff) on the standardized mean difference between pre and post-test scores. Results of the test of moderators reveals that employee group did not significantly moderate the pre vs. post difference, Qm(2) = 5.077, p = .079.\n##Pairwise Comparisons\nAlthough the omnibus test indicates no difference in levels of employee group as a moderating factor explaining variance in the pre vs. post score difference, I will provide some code here to run pairwise comparisons of the each of the three levels of this moderator for the sake of illustration. This is also useful if a specific comparison between levels of the moderator needs to be reported on.\n\npairmat(PrevPost_EmployTypeMod_Nointer, btt = 1:3)\n\n                                                                 [,1] [,2] [,3]\nfactor(Employee_Group)Frontline-factor(Employee_Group)CareManage   -1    1    0\nfactor(Employee_Group)Frontline-factor(Employee_Group)TopManage    -1    0    1\nfactor(Employee_Group)CareManage-factor(Employee_Group)TopManage    0   -1    1\n\nanova(PrevPost_EmployTypeMod_Nointer, X=pairmat(btt=1:3))\n\n\nHypotheses:                                                                           \n1: -factor(Employee_Group)Frontline + factor(Employee_Group)CareManage = 0 \n2:  -factor(Employee_Group)Frontline + factor(Employee_Group)TopManage = 0 \n3: -factor(Employee_Group)CareManage + factor(Employee_Group)TopManage = 0 \n\nResults:\n   estimate     se    zval   pval   \n1:  -0.4895 0.7126 -0.6870 0.4921   \n2:  -1.5734 0.7145 -2.2020 0.0277 * \n3:  -1.0838 0.7137 -1.5187 0.1288"
  },
  {
    "objectID": "Portfolio pages/MLMAEffect.html#heterogeneity",
    "href": "Portfolio pages/MLMAEffect.html#heterogeneity",
    "title": "Using Multilevel Meta-Analysis to Better Understand Complex Business Research",
    "section": "Heterogeneity",
    "text": "Heterogeneity\nOne key benefit to random-effects meta-analyses is that the variability in the average effect size can be quantified by assessing the heterogeneity present in the effect. MLMA builds on this and allows for assessing both the between- and within-level heterogeneity. Heterogeneity has historically been represented by Q, which compares of the observed variance to what is expected due to sampling error (Morris 2023). That said, current best practices have moved away from only reporting Q and have instead adopted the I2 statistic, which denotes the proportion of variance between and within studies attributable to the true heterogeneity of effect size. Additionally, I will report the variance for each level, τ2.\n\nCalculate I2\n\noptions(scipen = 999)\nW &lt;- diag(1/Pre_v_Post$vi)\nX &lt;- model.matrix(Pre_v_Post)\nP &lt;- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W\n100 * sum(Pre_v_Post$sigma2) / (sum(Pre_v_Post$sigma2) + (Pre_v_Post$k-Pre_v_Post$p)/sum(diag(P)))\n\n[1] 99.7982\n\n#variance attribution (between & within)\n100 * Pre_v_Post$sigma2 / (sum(Pre_v_Post$sigma2) + (Pre_v_Post$k-Pre_v_Post$p)/sum(diag(P)))\n\n[1] 15.3562 84.4420\n\n\nUsing the information we just calculated, as well as information provided in the output of the model itself, we can understand quite a bit regarding the variance in the pre vs. post effect. First, and as mentioned above, results from the MLMA indicate significant heterogeneity, Q(29) = 3191.64, p &lt;.001. Using the sigma 2.1 and 2.2 values and matrix multiplication output, I have also identified that both the between-level variance I2 = 15.36%, τ2 = 0.526, and at the within-study level, I2 = 84.44%, τ2 = 2.893. We can also grab confidence intervals for τ2 values using the confint.rma.mv function in the metafor package, which I will run in the next chunk of code.\nI also know that approximately 99.80% of the total variance is due to heterogeneity and approximately 0.20% of the total variance is attributable to sampling variance. These results indicate that almost all of the variance in the effect is attributable to heterogeneity, and thus likely dependent on the presence of moderators. Another way to quantify this degree of uncertainty in the pre vs. post effect size is by calculating the prediction interval surrounding this effect.\n\n\nPrediction Interval\nThe prediction interval provides an estimate of the range of possible values expected in a population.\n\nconfint.rma.mv(Pre_v_Post)\n\n\n          estimate  ci.lb  ci.ub \nsigma^2.1   0.5260 0.0000 3.5401 \nsigma.1     0.7253 0.0000 1.8815 \n\n          estimate  ci.lb  ci.ub \nsigma^2.2   2.8925 1.5943 5.7841 \nsigma.2     1.7007 1.2627 2.4050 \n\npredict.rma(Pre_v_Post)\n\n\n   pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n 1.0220 0.3971 0.2099 1.8340 -2.8457 4.8897 \n\n\nThe prediction interval for this SMD effect ranges from -2.846 to 4.890, suggesting that any new study conducted with this sample population could expect to find negative effects (i.e., participants perform worse on training outcomes after engaging in the training), no effects, or positive effects (i.e., participants perform better on training outcomes after engaging in the training). Of course, these SMD values are also exceptionally large, which are questionable at best and likely are driven by error and the high degree of heterogeneity present in this effect (BORENSTEIN and HEDGES 2019)."
  },
  {
    "objectID": "Portfolio pages/MLMAEffect.html#prediction-interval",
    "href": "Portfolio pages/MLMAEffect.html#prediction-interval",
    "title": "Using Multilevel Meta-Analysis to Better Understand Complex Business Research",
    "section": "Prediction Interval",
    "text": "Prediction Interval\nThe prediction interval provides an estimate of the range of possible values expected in a population.\n\nconfint.rma.mv(Pre_v_Post)\n\n\n          estimate  ci.lb  ci.ub \nsigma^2.1   0.5260 0.0000 3.5401 \nsigma.1     0.7253 0.0000 1.8815 \n\n          estimate  ci.lb  ci.ub \nsigma^2.2   2.8925 1.5943 5.7841 \nsigma.2     1.7007 1.2627 2.4050 \n\npredict.rma(Pre_v_Post)\n\n\n   pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n 1.0220 0.3971 0.2099 1.8340 -2.8457 4.8897 \n\n\nThe prediction interval for this SMD effect ranges from -2.846 to 4.890, suggesting that any new study conducted with this sample population could expect to find negative effects (i.e., participants perform worse on training outcomes after engaging in the training), no effects, or positive effects (i.e., participants perform better on training outcomes after engaging in the training). Of course, these SMD values are also exceptionally large, which are questionable at best and likely are driven by error and the high degree of heterogeneity present in this effect (BORENSTEIN and HEDGES 2019)."
  },
  {
    "objectID": "Portfolio pages/MLMAEffect.html#testing-for-outliers-and-publication-bias",
    "href": "Portfolio pages/MLMAEffect.html#testing-for-outliers-and-publication-bias",
    "title": "Using Multilevel Meta-Analysis to Better Understand Complex Business Research",
    "section": "Testing for Outliers and Publication Bias",
    "text": "Testing for Outliers and Publication Bias\nOutliers should also be assessed as well as testing for publication bias.\n\n##outliers -- dfbeta \ndfbetas.rma.mv(Pre_v_Post) |&gt; mutate(influence = if_else(intrcpt &gt; .99 | intrcpt &lt; -.99, 'TRUE', '0'))\n\n        intrcpt influence\n1   0.396297256         0\n2   0.143790700         0\n3  -0.076287096         0\n4  -0.070287411         0\n5   0.098341001         0\n6   0.046261655         0\n7  -0.065757147         0\n8   0.090510979         0\n9  -0.094057717         0\n10  0.064823527         0\n11  0.053342867         0\n12 -0.683981975         0\n13  0.222709413         0\n14 -0.173795907         0\n15  0.029814446         0\n16 -0.005786368         0\n17  0.080354135         0\n18 -0.053061603         0\n19  0.182491009         0\n20  0.010184159         0\n21 -0.124972673         0\n22  0.099789483         0\n23 -0.075163499         0\n24  0.041619982         0\n25 -0.060537876         0\n26 -0.019188372         0\n27 -0.031570842         0\n28 -0.032071639         0\n29  0.048275103         0\n30 -0.001980348         0\n\n\nFirst, I ran the dfbetas.rma.rv function, which tests if the removal of any single effect (there are 30 effects contained in this model) changes the beta value of the intercept by more than \\(|1|\\). As you can see in the output, the removal of any single effect does not influence the intercept beta by the absolute value of 1, thus all 30 effects can be retained in the model (i.e., no strong evidence that any effects are outliers from the sample).\nPublication bias can occur when the studies that are included in a meta-analysis tend to favor one type of effect (i.e., significant findings), thus potentially obfuscating the “true” effect as significant findings tend to get published over non-significant or uninteresting findings. I will test for publication bias using both a multivariate version of Egger’s test and the three-parameter selection model (3PSM). Egger’s test is typically a linear regression that tests if there is symmetry between effect size and sample size. A significant Egger’s test suggests that effect sizes depend on sample size, which can provide some evidence of selection bias (Pustejovsky and Rodgers 2019). The 3PSM test estimates three parameters (\\(\\mu\\), τ2, and 𝛿2) using maximum likelihood estimation. Each 3PSM model has a single cut point at alpha level .025 and a significant likelihood ratio test indicates that non-significant findings are less likely to be published than significant findings. Carter et al. (2019) provides more information for this test.\n\n#compute adjusted SE for Egger's regression test per Pustejovsky and Rodgers (2019)\nData &lt;- Data |&gt; mutate(se_cor = sqrt(4/N))\n\n#Small study effect (Egger's test)\nrma.mv(yi = hedges_g ~ 1 + se_cor, \n                   V = var_g,\n                   test = \"knha\",\n                   digits = 3,\n                   random = ~ 1 | Study_ID/Effect_ID,\n                   data = Data)\n\n\nMultivariate Meta-Analysis Model (k = 30; method: REML)\n\nVariance Components:\n\n           estim   sqrt  nlvls  fixed              factor \nsigma^2.1  0.337  0.581     10     no            Study_ID \nsigma^2.2  3.091  1.758     30     no  Study_ID/Effect_ID \n\nTest for Residual Heterogeneity:\nQE(df = 28) = 3180.178, p-val &lt; .001\n\nTest of Moderators (coefficient 2):\nF(df1 = 1, df2 = 28) = 0.606, p-val = 0.443\n\nModel Results:\n\n         estimate     se    tval  df   pval    ci.lb  ci.ub    \nintrcpt     1.590  0.823   1.931  28  0.064   -0.096  3.277  . \nse_cor     -3.489  4.482  -0.778  28  0.443  -12.671  5.693    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n##Selection effects (3PSM)\n\n###aggregate effects\nAggregate_Eff_PrevPost &lt;- Data %&gt;% group_by(Study_ID) %&gt;% mutate(agg_effect = mean(hedges_g)) %&gt;% ungroup \n\n###compute univariate model\nPre_v_Post_Agg &lt;- rma(yi = agg_effect, \n                   vi = var_g,\n                   digits = 3,\n                   data = Aggregate_Eff_PrevPost)\nPre_v_Post_Agg  \n\n\nRandom-Effects Model (k = 30; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 1.417 (SE = 0.379)\ntau (square root of estimated tau^2 value):      1.190\nI^2 (total heterogeneity / total variability):   99.51%\nH^2 (total variability / sampling variability):  205.97\n\nTest for Heterogeneity:\nQ(df = 29) = 3238.603, p-val &lt; .001\n\nModel Results:\n\nestimate     se   zval   pval  ci.lb  ci.ub      \n   1.019  0.219  4.645  &lt;.001  0.589  1.449  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#3PSM test\nselmodel(Pre_v_Post_Agg, type=\"stepfun\", steps=c(.025))\n\n\nRandom-Effects Model (k = 30; tau^2 estimator: ML)\n\ntau^2 (estimated amount of total heterogeneity): 2.703 (SE = 1.017)\ntau (square root of estimated tau^2 value):      1.644\n\nTest for Heterogeneity:\nLRT(df = 1) = 3067.099, p-val &lt; .001\n\nModel Results:\n\nestimate     se    zval   pval   ci.lb  ci.ub    \n  -0.675  0.887  -0.762  0.446  -2.414  1.063    \n\nTest for Selection Model Parameters:\nLRT(df = 1) = 13.652, p-val &lt; .001\n\nSelection Model Results:\n\n                     k  estimate     se     zval   pval  ci.lb  ci.ub      \n0     &lt; p &lt;= 0.025  27     1.000    ---      ---    ---    ---    ---      \n0.025 &lt; p &lt;= 1       3     0.044  0.043  -22.350  &lt;.001  0.000  0.128  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResults of the Egger’s test do not indicate non-symmetry, b = 1.590, SE = 0.823, p = .064, however results of the 3PSM selection bias may, in fact, be influencing results of the pre vs. post MLMA model, X2(1) = 13.652, p &lt; .001. Thus, there is some evidence that publication bias is an issue here (remember that this dataset has been generated) and the validity of these MLMA models is at risk. One additional piece of information from the output I will point out is the adjusted effect size that is calculated alongside these analyses. The effect size decreased from g = 1.022 to g = 1.019 in the adjusted model. A difference of .003 in effect size is considered trivial in the literature (BORENSTEIN and HEDGES 2019), but it is wise to be cautious when interpreting this model. Additional research is needed to build confidence in this effect, especially given the high degree of heterogeneity and wide prediction interval affiliated with this effect."
  },
  {
    "objectID": "Portfolio pages/MLMAEffect.html#calculating-effect-sizes-for-pre-vs.-post-time-points",
    "href": "Portfolio pages/MLMAEffect.html#calculating-effect-sizes-for-pre-vs.-post-time-points",
    "title": "Using Multilevel Meta-Analysis to Better Understand Complex Business Research",
    "section": "Calculating Effect Sizes for Pre vs. Post Time Points",
    "text": "Calculating Effect Sizes for Pre vs. Post Time Points"
  },
  {
    "objectID": "Portfolio pages/MLMAEffect.html#pre-vs.-follow-up-time-points",
    "href": "Portfolio pages/MLMAEffect.html#pre-vs.-follow-up-time-points",
    "title": "Using Multilevel Meta-Analysis to Better Understand Complex Business Research",
    "section": "Pre vs. Follow-up Time Points",
    "text": "Pre vs. Follow-up Time Points\nLet’s do this again and compare pre to follow-up scores. I will just provide a brief overview of the results as the same information I covered above applies for this model.\n\n#Comparing Pre and Follow Time Points\n\n#compute effect size  for pre vs. follow\nData2 &lt;- escalc(measure = \"SMCR\", \n         m1i= Mean_pre, \n         m2i = Mean_follow,\n         sd1i = SD1, \n         sd2i = SD3, \n         ni = N,\n        ri = ri,\n        flip = TRUE,\n        data = Data)\n\nData2 &lt;- Data2 |&gt; mutate(df = N - 1)\n\n\n#compute standard error for Cohen's d using sampling variance\nData2 &lt;- Data2 |&gt; mutate(std_error_d = sqrt(vi / N))\n\n#convert Cohen's d to Hedges' g: effect size & standard error & variance\n\n##correction factor J\nData2 &lt;- Data2 |&gt; mutate(j =  1 - (3 / (4 * df - 1)))\n\n##Hedges' g: effect size\nData2 &lt;- Data2 |&gt; mutate(hedges_g =  yi * j)\n##Hedges' g: std error\nData2 &lt;- Data2 |&gt; mutate(std_error_g =  std_error_d * j)\n#Hedges' g: variance \nData2 &lt;- Data2 |&gt;  mutate(var_g = vi * j) \nhead(Data2)\n\n\n  Study_ID Effect_ID Mod1 Months.since.follow   N Mean_pre Mean_post \n1        1         1    1                   2 306     3.50      5.86 \n2        1         2    2                   6 248     2.71      5.79 \n3        1         3    3                  12 329     2.48      3.30 \n4        2         4    1                   4  82     3.78      4.42 \n5        2         5    2                   2 418     3.07      6.40 \n6        2         6    3                   1 309     4.12      6.63 \n  Mean_follow  SD1  SD2  SD3  ri      yi     vi  df std_error_d         j \n1        4.84 0.43 0.72 1.19 0.5  3.1086 0.0191 305 0.007891818 0.9975390 \n2        3.10 1.06 0.87 1.61 0.5  0.3668 0.0043 247 0.004165682 0.9969605 \n3        2.14 0.71 1.40 0.88 0.5 -0.4778 0.0034 328 0.003208286 0.9977117 \n4        3.90 1.49 1.20 0.85 0.5  0.0798 0.0122  81 0.012214516 0.9907121 \n5        5.21 1.55 1.16 1.82 0.5  1.3782 0.0047 417 0.003340438 0.9982004 \n6        3.42 1.56 1.61 1.97 0.5 -0.4476 0.0036 308 0.003394487 0.9975630 \n     hedges_g std_error_g       var_g    se_cor \n1  3.10095836 0.007872396 0.019011020 0.1143324 \n2  0.36569112 0.004153021 0.004290441 0.1270001 \n3 -0.47668397 0.003200945 0.003378681 0.1102636 \n4  0.07904744 0.012101068 0.012120312 0.2208631 \n5  1.37568006 0.003334427 0.004655871 0.0978232 \n6 -0.44653338 0.003386214 0.003551788 0.1137760 \n\n\nAll necessary information has now been calculated to do this comparison.\n\n#Pre vs. follow-Up Comparison\noptions(scipen = 999)\n\n#Calculating MLMA effect\nPre_v_Follow &lt;- rma.mv(yi = hedges_g, \n                        V = var_g, \n                        test = \"knha\",\n                        random = ~ 1 | Study_ID/Effect_ID,\n                        data = Data2)\nsummary(Pre_v_Follow)\n\n\nMultivariate Meta-Analysis Model (k = 30; method: REML)\n\n  logLik  Deviance       AIC       BIC      AICc   \n-59.5806  119.1613  125.1613  129.2632  126.1213   \n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed              factor \nsigma^2.1  0.0000  0.0001     10     no            Study_ID \nsigma^2.2  3.4263  1.8510     30     no  Study_ID/Effect_ID \n\nTest for Heterogeneity:\nQ(df = 29) = 4071.9106, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    tval  df    pval    ci.lb   ci.ub    \n  0.5502  0.3446  1.5966  29  0.1212  -0.1546  1.2550    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint.rma.mv(Pre_v_Follow)\n\n\n          estimate  ci.lb  ci.ub \nsigma^2.1   0.0000 0.0000 1.9874 \nsigma.1     0.0001 0.0000 1.4098 \n\n          estimate  ci.lb  ci.ub \nsigma^2.2   3.4263 1.9957 6.1190 \nsigma.2     1.8510 1.4127 2.4737 \n\npredict.rma(Pre_v_Follow)\n\n\n   pred     se   ci.lb  ci.ub   pi.lb  pi.ub \n 0.5502 0.3446 -0.1546 1.2550 -3.3006 4.4010 \n\nround(Pre_v_Follow$tau2, 4)\n\n[1] 0\n\nPre_v_Follow$sigma2\n\n[1] 0.00000001561201 3.42631262292152\n\nfunnel(Pre_v_Follow, xlab = \"Standardized Mean Difference\")\n\n\n\n\n\n\n\nforest(Pre_v_Follow, header = c(\"Effect Number\", \"Weight (%)   SMD   [95% CI]\"), slab = paste(Effect_ID), mlab = \"Pooled Estimate\",  order = \"obs\", xlab = \"Standardized Mean Difference (95% CI)\", digits = 2L, cex = 1, shade = \"zebra\", showweights = TRUE)\n\n\n\n\n\n\n\n\nResults from this model indicate a non-significant effect, g = 0.550, 95% CI [-0.155, 1.255]. I will touch upon adding moderators to MLMA models and testing for quadratic effects (ex. testing for an initial improvement in training outcomes that then decrease over time) in separate projects that can be found on my website."
  },
  {
    "objectID": "Portfolio pages/MLMAEffect.html#the-data",
    "href": "Portfolio pages/MLMAEffect.html#the-data",
    "title": "Using Multilevel Meta-Analysis to Better Understand Complex Business Research",
    "section": "The Data",
    "text": "The Data"
  },
  {
    "objectID": "Portfolio pages/MLMAEffect.html#multilevel-meta-analysis",
    "href": "Portfolio pages/MLMAEffect.html#multilevel-meta-analysis",
    "title": "Using Multilevel Meta-Analysis to Better Understand Complex Business Research",
    "section": "Multilevel Meta Analysis",
    "text": "Multilevel Meta Analysis"
  },
  {
    "objectID": "Publication Pages/8-02-2021.html",
    "href": "Publication Pages/8-02-2021.html",
    "title": "Leveraging DEI Policies and Training to Navigate Conflict in Organizations",
    "section": "",
    "text": "Introduction. Although greater workforce diversity brings potential benefits, such as the availability of varied perspectives, it can also lead to greater conflict, undermining employee relationships, and thus effective problem-solving. Indeed, organizations have invested a great deal of time and money in various diversity, equity, and inclusion (DEI) initiatives to maximize the benefits and minimize the costs of a diverse workforce. Surprisingly, however, these initiatives frequently fail to include effective conflict management strategies.\nHere we review the current status of organizational conflict literature, conflict management strategies, and their role (or lack thereof) in diversity initiatives. We conclude by providing evidence-based recommendations for the integration of effective conflict management strategies in diversity initiatives and training.\n\n\n\nPoster presented at the 2021 Society for the Psychological Study of Social Issues conference"
  },
  {
    "objectID": "Publication Pages/8-02-2021.html#abstract",
    "href": "Publication Pages/8-02-2021.html#abstract",
    "title": "Leveraging DEI Policies and Training to Navigate Conflict in Organizations",
    "section": "",
    "text": "Introduction. Although greater workforce diversity brings potential benefits, such as the availability of varied perspectives, it can also lead to greater conflict, undermining employee relationships, and thus effective problem-solving. Indeed, organizations have invested a great deal of time and money in various diversity, equity, and inclusion (DEI) initiatives to maximize the benefits and minimize the costs of a diverse workforce. Surprisingly, however, these initiatives frequently fail to include effective conflict management strategies.\nHere we review the current status of organizational conflict literature, conflict management strategies, and their role (or lack thereof) in diversity initiatives. We conclude by providing evidence-based recommendations for the integration of effective conflict management strategies in diversity initiatives and training.\n\n\n\nPoster presented at the 2021 Society for the Psychological Study of Social Issues conference"
  },
  {
    "objectID": "Publication Pages/5-26-2021.html",
    "href": "Publication Pages/5-26-2021.html",
    "title": "Essential Meaningful Work: A Multistage Proposal for the Development and Validation of the Essential Meaningful Work Inventory (EMWI)",
    "section": "",
    "text": "Introduction. Meaningful work is becoming a ubiquitous component of managerial and motivational discourse, reflecting a shift in societal perceptions regarding how to best motivate employees beyond pay and reward systems (Michaelson, 2020). Meaningfulness is subjective and eudaimonic(Bailey et al., 2019). That is, the pursuit of meaningfulness at work is often viewed as an inherent need that employees are motivated to fulfill (Lips-Wiersma & Morris, 2009). Most meaningful work research utilizes data from WEIRD(i.e., white, educated, industrialized, rich, democratic) participants, hold mid-to-upper-level job positions within their organization, or are white-collar employees (Bailey et al., 2019). The generalizability of empirical findings to frontline and essential workers is questionable at best. Indeed, research has demonstrated that white-collar employees likely differ in their sources of meaningful work from blue-and pink-collar employees (Lips-Wiersma et al., 2016). Several researchers have even gone so far as to suggest that meaningful work is best fostered for employees in white-collar positions (Lysovaet al., 2019). The lack of consideration is reflected within current meaningful work inventories which largely ignore employee samples that are not “professional” or white-collar. The importance of understanding how frontline and essential workers perceive meaningfulness in their work has been underscored by the COVID-19 pandemic. Essential and frontline employees have had to navigate increasingly anxiety-provoking situations, often with little to no support from leadership and society at large as resources dry up and teams are spread thin (Adams & Walls, 2020). Indeed, the pandemic has exasperated the issues that many frontline and essential workers have been facing for decades (Guerrero et al., 2020). Further, many essential and frontline workers are immigrants or come from lower SES families and face barriers and challenges that often go unseen by the general public (Kerwin& Warren, 2020). Understanding meaningful work from the perspectives of frontline and essential workers may help identify ways (in addition to improving financial rewards) to attract and retain people in these positions.\nGoals. The goals of the proposed research are to:\n\nIdentify factors important to the development of meaningful work for frontline and essential workers.\nCreate and validate a scale to evaluate meaningful work directly for frontline and essential workers.\n\n\n\n\nPoster Presented at the 2021 Association for Psychological Science Conference"
  },
  {
    "objectID": "Publication Pages/5-26-2021.html#abstract",
    "href": "Publication Pages/5-26-2021.html#abstract",
    "title": "Essential Meaningful Work: A Multistage Proposal for the Development and Validation of the Essential Meaningful Work Inventory (EMWI)",
    "section": "",
    "text": "Introduction. Meaningful work is becoming a ubiquitous component of managerial and motivational discourse, reflecting a shift in societal perceptions regarding how to best motivate employees beyond pay and reward systems (Michaelson, 2020). Meaningfulness is subjective and eudaimonic(Bailey et al., 2019). That is, the pursuit of meaningfulness at work is often viewed as an inherent need that employees are motivated to fulfill (Lips-Wiersma & Morris, 2009). Most meaningful work research utilizes data from WEIRD(i.e., white, educated, industrialized, rich, democratic) participants, hold mid-to-upper-level job positions within their organization, or are white-collar employees (Bailey et al., 2019). The generalizability of empirical findings to frontline and essential workers is questionable at best. Indeed, research has demonstrated that white-collar employees likely differ in their sources of meaningful work from blue-and pink-collar employees (Lips-Wiersma et al., 2016). Several researchers have even gone so far as to suggest that meaningful work is best fostered for employees in white-collar positions (Lysovaet al., 2019). The lack of consideration is reflected within current meaningful work inventories which largely ignore employee samples that are not “professional” or white-collar. The importance of understanding how frontline and essential workers perceive meaningfulness in their work has been underscored by the COVID-19 pandemic. Essential and frontline employees have had to navigate increasingly anxiety-provoking situations, often with little to no support from leadership and society at large as resources dry up and teams are spread thin (Adams & Walls, 2020). Indeed, the pandemic has exasperated the issues that many frontline and essential workers have been facing for decades (Guerrero et al., 2020). Further, many essential and frontline workers are immigrants or come from lower SES families and face barriers and challenges that often go unseen by the general public (Kerwin& Warren, 2020). Understanding meaningful work from the perspectives of frontline and essential workers may help identify ways (in addition to improving financial rewards) to attract and retain people in these positions.\nGoals. The goals of the proposed research are to:\n\nIdentify factors important to the development of meaningful work for frontline and essential workers.\nCreate and validate a scale to evaluate meaningful work directly for frontline and essential workers.\n\n\n\n\nPoster Presented at the 2021 Association for Psychological Science Conference"
  },
  {
    "objectID": "Resume-MSI.html",
    "href": "Resume-MSI.html",
    "title": "Resume / CV",
    "section": "",
    "text": "Education\n\n\n\n\n\n\n\n\n\n2019 - 2022\nUNIVERSITY OF NEBRASKA OMAHA\nPh.D. in Industrial Organizational Psychology\nGPA = 4.00\n2016 - 2018\nUNIVERSITY OF Akron\nM.A. in Industrial Organizational Psychology\nGPA = 3.78\n2013 - 2016\nKENT STATE UNIVERSITY\nB.S. in Psychology\nGPA = 3.75\n\n\n\n\n\n\n\n\n\nExperience\n\n\n\n\n\n\n\n\nWORK EXPERIENCE:\n2022 - Present\nWriter - IOATWORK\n\nTranslated academic research to an applied audience via monthly research article reviews\nAssessed and reported research designs and advanced statistics to a practitioner audience\n\n\n\n\n2019 - 2021\nExternal Consultant - The Center for Applied Psychological Services\n\nActed as project lead and managed data collection procedures (both for large- and small-scale data sets)\nAnalyzed big data using Excel, SPSS, and R to create and present technical reports\nUtilized dashboard/visualization toolsets (Tableau) for data presentation and analysis\nServed as the expert for people analytics and data reporting\nDeveloped Knowledge, Skills, & Ability-linked items for evidence-based employee selection\nConducted a variety of data analyses (Regression, Validity, Frequencies, Data Visualization)\nWorked with law enforcement to develop officer promotional exams\n\n\n\n\n2017 - 2019\nIO Analyst, Client Solutions & Program Management - Corporate College\n\nConducted job analyses and redesigned hiring procedures and evaluative criteria\nDeveloped over 70 evidence-based training courses targeting workplace competencies such as Diversity, Equity, and Inclusion, change management, and accountability\nEnsured data quality and proper collection standards Converted data into business insights through predictive modeling\nCreated predictive models to assess satisfaction, turnover, and organizational climate & culture\nImproved the frontline and leadership talent of dozens of clients via organizational training\nWorked with companies like American Greetings, Cleveland Cavalier and Oatey to address absenteeism and training issues, resulting in healthier work environments and improved retention\nTrained a small team on how to evaluate and score evaluations\n\n\n\n\n2017 - 2017\nConsultant - Center for Organizational Research\n\nConducted data analyses and compiled results into professional reports\nCustomized tests, measures, survey, and selection systems\nCreated and validated surveys to assess perceptions of success for client’s internal projects\n\n\n\n\n\n\n\n\n\n\n\nTEACHING AND PROFESSIONAL ACTIVITIES:\n2019 - 2022\nTeaching Assistant - University of Nebraska Omaha\n\nDeveloped course content, taught graduate-level R coding, and graded coursework\nLectured weekly for graduate-level analysis of variance (ANOVA)\n\n\n\n\n2020 - 2023\nConference Reviewer\n\nReviewed submissions, provided feedback, and determined application status for research submitted to the following conferences:\n\nMidwest Academy of Management\nSociety for the Psychological Study of Social Issues\nReviewer for APS Student Research & Grant Award\n\n\n\n\n\n2023 - 2023\nJournal Editor\n\nServed as Guest Editor for a special edition of the Journal of Social Issues\n\n\n\n\n\n\n\n\n\n\n\nRESEARCH EXPERIENCE:\n2019 - Present\nLab Researcher - Diversity and Inclusion Research Lab\n\nProvided input to research methodology and presentation efforts\nDeveloped empirical work to test questions regarding diversity and inclusion at work\nAnalyzed data sets using R, SPSS, SAS, and Python, and wrote empirical findings for both academic and practitioner audiences\n\n\n\n\n2019 - 2021\nExternal Research Consultant - Tri-Faith\n\nServed as a research consultant to Tri-Faith’s research initiatives\nHelped align project ideas with research questions\n\n\n\n\n2015 - 2016\nResearch Assistant - Emotion, Stress, and Relationships Lab\n\nOversaw clinical trials targeted at developing a language-based psychotherapy\n\n\n\n\n\nConference Presentations & Posters\n\nFolberg, A., Votruba, A., Marshburn, C., Swanson, M., Crawford, D., Kaiser, C. (2023). Reimagining Resolution: Addressing Racism in Academic Institutions with Conflict Resolution. Interactive discussion to be presented at the annual conference for the Society for the Psychological Study of Social Issues, Denver, CO.\n\nXimenes, M., Folberg, A., Swanson, M. A., Dueland, L., Stepanek, S., Ryan C. (2023). The Role of Conservatism in Evaluations of Diversity Statements. Poster to be presented at the Midwestern Psychological Association, Chicago, IL.\n\nSwanson, M. A.(2022). Can I Remain True to Myself at Work? An Experimental Study of Psychologically Safe versus Unsafe Workplaces on LGBTQ+ and Heterosexual Perceptions of Authenticity, Belongingness, Vigilance, and Resiliency. Poster presented at the Student Research and Creative Activity Fair, Omaha, NE. \n\nStepanek, S., Dueland, L., Folberg, A. M., Swanson, M. A., & Ryan, C. S. (2021).Whites (vs. Blacks) and conservatives exhibit less interest in applying for jobs that request diversity statements in application materials. Poster presented at the annual conference of the Association for Psychological Science, Virtual. \n\n‍Swanson, M. A.(2021). Essential Meaningful Work: A Multistage Proposal for the Development and Validation of the Essential Meaningful Work Inventory (EMWI). Poster presented at the annual conference of the Association for Psychological Science, Virtual. \n\nCrawford, D., Swanson, M. A., Dueland, L. B., Stepanek, S., & Ryan, C. (2021). A Missing Component of Diversity Training and Diversity Initiatives: Conflict Management Training. Poster presented at the annual conference for the Society for the Psychological Study of Social Issues, Virtual.\n\n‍Dodds, B. L., Ryan, C. S., Swanson, M. A. (2021). The Relationships of Perceived Parental Social Support to Vigilance and Resilience among LGBTQ and Straight Cisgender Adults. Poster presented at the University of Nebraska Omaha’s University Honors Program, Omaha, NE. \n\nDueland, L. B., Folberg, A. M., Swanson, M. A., & Ryan, C. S. (2020). Perceptions of Requests for Diversity Statements in Job Advertisements. Poster presented at the annual meeting of the Society for the Psychological Study of Social Issues, Denver, CO. (Conference cancelled) \n\nCrawford, D., Swanson, M. A., Dueland, L. B., Stepanek, S., & Ryan, C. (2020).Conflict management techniques within diversity initiatives: A critical missing link. Interactive discussion facilitated at the Society for the Psychological Study of Social Issues Conference, Denver, CO. (Conference cancelled) \nDueland, L. B., Folberg, A., Swanson, M. A., & Ryan, C. (2020). Reactions to selection processes involving diversity statements. Poster presented at the Society for Industrial Organizational Psychology Conference, Austin, TX.(Conference cancelled) \n\n‍Swanson,M. A. (2015) The Effects of Positive Teacher-Child Relationships on School Achievement and Motivation in Adolescence. Poster presented at the Undergraduate Research Conference, Kent, OH.\n\nSkills & Coursework\n\n\n\n\n\n\n\n\nSkills\nCoursework\n\n\nStatistical Analysis Software: R, SPSS,PROCESS, SAS, Excel, Mplus, LIWC, SQL\n\nExperience using SQL, R, Python, SAS, Mplus, and SPSS for predictive data modeling and analyses\nExpertise in qualitative data collection and analysis procedures using natural language processing and textual analysis with programs like R and LIWC\nWed design using HMTL, YAML and CSS\n\nData Collection Software: Qualtrics, Survey Monkey, Cloud Research\n\nData sets ranged from a few dozen to over 50,000 sampled individuals\n\n\nAfrican American Psychology\nDiversity in Organizations\nGroups and Teams\nIndustrial Motivation and Morale\nLeadership\nLearning\nMultilevel Modeling in R\nResearch Methods\nSocial Psychology\nStructural Equation Modeling\nAdvanced Psychological Tests and Measurements\nIndustrial Organizational Psychology\nMultivariate and Computational Methods in Psychology\nOrganizational Psychology\nPerformance Feedback and Evaluations\nPersonnel Selection and Advanced App Test ISS\nPsychological Research: Quantitative and Computational Methods 1\nPsychological Research: Quantitative and Computational Methods 2\nRole of Attitudes and Values in Industrial Organizational Psychology\nTraining"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Matthew Swanson, Ph.D.",
    "section": "Education",
    "text": "Education\nPh.D. in Industrial Organizational Psychology | 2022 | University of Nebraska, Omaha\nM.A. in Industrial Organizational Psychology | 2018 | University of Akron\nB.S. in Psychology | 2016 | Kent State University"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Matthew Swanson, Ph.D.",
    "section": "Research Interests",
    "text": "Research Interests\n\nData Analysis\nStatistical Methodology\nDiversity, Equity, and Inclusion (DEI)\nMeaningful Work\nTraining Evaluation\nAuthentic Self-Expression"
  },
  {
    "objectID": "Portfolio pages/MLMAQuad.html#hedges-g",
    "href": "Portfolio pages/MLMAQuad.html#hedges-g",
    "title": "Testing for Quadratic Effects in Multilevel Meta-Analysis",
    "section": "Hedges’ g",
    "text": "Hedges’ g\nFor the sake of this example, I am going to recalculate Hedges’ g for each of these comparisons in order to demonstrate again how to calculate meta-analytic effects using a dataset that has a slightly different set up than I have shown previously. I will also need to create the quadratic term by multiplying the number of months since the post test by itself.\n\n# set correlation = .5\nri &lt;- .5\nQuadData$ri &lt;- ri\n\n#compute effect size  for pre vs. post\nQuadDataPP &lt;- escalc(measure = \"SMCR\", \n         m1i= Mean_pre, \n         m2i = Mean_post,\n         sd1i = SD1, \n         sd2i = SD2, \n         ni = N,\n        ri = ri,\n        flip = TRUE,\n        data = QuadData)\n\n#compute effect size  for pre vs. follow\nQuadDataPF &lt;- escalc(measure = \"SMCR\", \n         m1i= Mean_pre, \n         m2i = Mean_follow,\n         sd1i = SD1, \n         sd2i = SD3, \n         ni = N,\n        ri = ri,\n        flip = TRUE,\n        data = QuadData)\n\nQuadFull &lt;- merge(QuadDataPP, QuadDataPF, by = c('Study_ID', 'Effect_ID'), all.x = TRUE)\nQuadFull &lt;- unite(QuadFull, yi, c(yi.x, yi.y), na.rm = TRUE)\nQuadFull &lt;- unite(QuadFull, vi, c(vi.x, vi.y), na.rm = TRUE)\nQuadFull &lt;- unite(QuadFull, N, c(N.x), na.rm = TRUE)\nQuadFull &lt;- unite(QuadFull, Months_since_post, c(Months_since_post.x), na.rm = TRUE)\n\nQuadFull &lt;- QuadFull %&gt;%\n  mutate_at(c('yi', 'vi', 'N', 'Months_since_post'), as.numeric)\n\n#Create quadratic term\nQuadFull &lt;- QuadFull %&gt;%\n  mutate(Quad_Term = Months_since_post * Months_since_post)\n\nQuadFull &lt;- QuadFull |&gt; mutate(df = N - 1)\n\n\n#compute standard error for Cohen's d using sampling variance\nQuadFull &lt;- QuadFull |&gt; mutate(std_error_d = sqrt(vi / N))\n\n#convert Cohen's d to Hedges' g: effect size & standard error & variance\n\n##correction factor J\nQuadFull &lt;- QuadFull |&gt; mutate(j =  1 - (3 / (4 * df - 1)))\n\n##Hedges' g: effect size\nQuadFull &lt;- QuadFull |&gt; mutate(hedges_g =  yi * j)\n##Hedges' g: std error\nQuadFull &lt;- QuadFull |&gt; mutate(std_error_g =  std_error_d * j)\n#Hedges' g: variance \nQuadFull &lt;- QuadFull |&gt;  mutate(var_g = vi * j) \nhead(QuadFull)\n\n  Study_ID Effect_ID Mod1.x Months_since_post   N Mean_pre.x Mean_post.x\n1        1         1      1                 0 306       3.50        5.86\n2        1         2      1                 2 306       3.50          NA\n3        1         3      2                 0 248       2.71        5.79\n4        1         4      2                 6 248       2.71          NA\n5        1         5      3                 0 329       2.48        3.30\n6        1         6      3                12 329       2.48          NA\n  Mean_follow.x SD1.x SD2.x SD3.x ri.x         yi          vi Mod1.y\n1            NA  0.43  0.72    NA  0.5  5.4748632 0.052245305      1\n2          4.84  0.43    NA  1.19  0.5  3.1086088 0.019057922      1\n3            NA  1.06  0.87    NA  0.5  2.8968271 0.020950821      2\n4          3.10  1.06    NA  1.61  0.5  0.3668060 0.004303521      2\n5            NA  0.71  1.40    NA  0.5  1.1522864 0.005057392      3\n6          2.14  0.71    NA  0.88  0.5 -0.4777773 0.003386430      3\n  Months_since_post.y N.y Mean_pre.y Mean_post.y Mean_follow.y SD1.y SD2.y\n1                   0 306       3.50        5.86            NA  0.43  0.72\n2                   2 306       3.50          NA          4.84  0.43    NA\n3                   0 248       2.71        5.79            NA  1.06  0.87\n4                   6 248       2.71          NA          3.10  1.06    NA\n5                   0 329       2.48        3.30            NA  0.71  1.40\n6                  12 329       2.48          NA          2.14  0.71    NA\n  SD3.y ri.y Quad_Term  df std_error_d         j   hedges_g std_error_g\n1    NA  0.5         0 305 0.013066610 0.9975390  5.4613894 0.013034452\n2  1.19  0.5         4 305 0.007891818 0.9975390  3.1009584 0.007872396\n3    NA  0.5         0 247 0.009191252 0.9969605  2.8880221 0.009163315\n4  1.61  0.5        36 247 0.004165682 0.9969605  0.3656911 0.004153021\n5    NA  0.5         0 328 0.003920716 0.9977117  1.1496496 0.003911744\n6  0.88  0.5       144 328 0.003208286 0.9977117 -0.4766840 0.003200945\n        var_g\n1 0.052116728\n2 0.019011020\n3 0.020887141\n4 0.004290441\n5 0.005045819\n6 0.003378681"
  },
  {
    "objectID": "Portfolio pages/MLMAQuad.html#plotting-linear-and-quadratic-trends",
    "href": "Portfolio pages/MLMAQuad.html#plotting-linear-and-quadratic-trends",
    "title": "Testing for Quadratic Effects in Multilevel Meta-Analysis",
    "section": "Plotting Linear and Quadratic Trends",
    "text": "Plotting Linear and Quadratic Trends\nI calculated Hedges’ g for each comparison of interest and ensured that each value is read into a single dataset by using the unite function. Of course I could clean this dataset further by removing more of the duplicate columns of data that are created by combining two data frames, but for now I will move on and build a MLMA model that includes the quadratic term. The I will also plot this model.\n\n# | warning: false\n#Creating MLMA Quad Model\n\nMLMAQuad &lt;- rma.mv(yi = hedges_g, \n                        V = var_g,\n                        mods = ~ Months_since_post + Quad_Term,\n                        random = ~ 1 | Study_ID/Effect_ID, \n                        data = QuadFull)\nMLMAQuad\n\n\nMultivariate Meta-Analysis Model (k = 60; method: REML)\n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed              factor \nsigma^2.1  0.6558  0.8098     10     no            Study_ID \nsigma^2.2  2.7679  1.6637     60     no  Study_ID/Effect_ID \n\nTest for Residual Heterogeneity:\nQE(df = 57) = 7446.3788, p-val &lt; .0001\n\nTest of Moderators (coefficients 2:3):\nQM(df = 2) = 3.5028, p-val = 0.1735\n\nModel Results:\n\n                   estimate      se     zval    pval    ci.lb   ci.ub     \nintrcpt              1.0182  0.3867   2.6331  0.0085   0.2603  1.7761  ** \nMonths_since_post   -0.3867  0.2131  -1.8147  0.0696  -0.8044  0.0310   . \nQuad_Term            0.0333  0.0178   1.8716  0.0613  -0.0016  0.0681   . \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#figure\nggplot(QuadFull, aes(x=Months_since_post, y=hedges_g)) + \n  geom_point(aes(size = N), color=\"black\", shape=21, stroke =1.5, fill=\"white\")+\n  geom_smooth(show.legend= F, color=\"#40B0A6\", method = \"lm\", formula = y~x, fill = \"#40B0A6\")+\n  geom_smooth(show.legend= F, color=\"#E1BE6A\", method = \"lm\", formula = y~poly(x,2), fill = \"#E1BE6A\")+\n  geom_hline(yintercept = 0, color = \"black\", linetype=2)+\n  coord_cartesian((xlim = c(0, 12))) +\n  theme_classic() +\n  labs(y=\"Standardized Mean Difference\", x=\"Months Since Post-Test\") +\n  theme(legend.position = \"none\",\n        axis.title.x = element_text(margin = unit(c(5, 0, 0, 0), \"mm\")),\n        axis.title.y = element_text(margin = unit(c(0, 5, 0, 0), \"mm\")),\n        axis.text.x=element_text(colour=\"black\"),\n        axis.text.y=element_text(colour=\"black\")) \n\n\n\n\n\n\n\n\nResults of this analysis indicates that there was no significant linear (p = .070) or quadratic effect (p = .061) of training over time. In other words, the significant improvement in training outcomes from pre to post assessment time points (as standardized mean difference) did not continue to improve over time, nor did post and follow-up time points differ from each other (when compared to pre training scores) as the quadratic trend line was not significant."
  }
]