{
  "hash": "37c951748eb36a6f7289dbea0beb4719",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Using Multilevel Meta-Analysis to Better Understand Complex Business Research\"\neditor: visual\nauthor: Matthew Swanson\ncategories: ['Multilevel Meta-Analysis', 'R', 'Organizational Training', 'Engagement', 'Data Science']\nwarning: FALSE\nmessage: FALSE\nbibliography: references.bib\nformat: \n  html: \n    page-layout: full\n---\n\n\n\n::: text-center\n# **Project Goal**\n:::\n\nAs practitioners of data science and IO psychology, we are sometimes tasked with understanding the effects of a particular training program or yearly event. In a perfect world, we would begin prepping the organization for evaluation work and would draft up timelines for evaluation and would work with the organization to identify potential samples. We would also identify any relevant data that has already been collected and potentially work that information into the evaluation schedules. Next, and dependent upon the type and length of training to be evaluated, the evaluation schedule would be enacted and data would be collected and analyzed (likely over the course of several training cycles to evaluate longitudinal impacts) and presented to the organization. However, what happens when the organization does not have the time and money to wait for the outcome of the evaluation cycle? For example, imagine a hospital struggling with teamwork among their frontline staff.\n\nIn this example, communication among frontline staff has broken down and become siloed, which has led to several critical errors while admitting patients. The hospital wants to implement team training that targets team cohesion and problem solving/conflict resolution. Currently, there is a very popular training course on the market that claims to target these outcomes that the hospital is considering (the hospital also does not have the time and money to develop their own in-house training). However, the training is expensive so the hospital has hired a consultant to review the literature on this training and present their findings on the effects reported in research conducted on this training course.\n\nThere are a few ways the consultant could go about reviewing and presenting the literature that has been conducted on this teamwork training course. First, the consultant could identify research most relevant to the organization (ex. finding research that evaluates that training course using similar samples) and create summary sheets for each article. These summaries then provide the organization some initial information on the types of research that has been conducted on the training course, the range of effect sizes found, and some common outcomes that training participation has been related to. However, one draw back to this approach is that the consultant cannot speak to the average effect size across these studies and the consultant runs the risk of missing the forest for the trees, especially when there exists a large amount of literature evaluating the training. Additionally, it seems unlikely that the hospital has the time or desire to read summaries of dozens of relevant studies. As such, the consultant may select focal articles to present (which can obfuscate the understanding of the training's efficacy), or the consultant may look to places where relevant research has been summarized before, typically in the form of a meta-analysis.\n\nWithout going into to much detail, meta-analysis is a statistical approach that combines effect sizes from multiple separate research studies [@deeks2019]. Meta-analysis typically takes the form of a fixed-effect model (which assumes that studies share an effect size) or a random-effects model (which assumes a distribution of effect sizes) (see [@borenstein2010] for more details on the common meta-analysis models). However, and pertinent to this example, these models assume independence of effect sizes. In other words, that the effect sizes come from discrete samples or participants. This becomes a challenge for training research that often samples the same participants multiple times (i.e., baseline test, pre-test, post-test, follow-up test). Because of this challenge, researchers will often select a single effect size, average across effect sizes, or they do not conduct meta-analysis with the studies [@morris2023].\n\n::: text-center\n# **The Data**\n:::\n\nTypically for training evaluation research, research studies will report the means, standard deviations, and sample sizes for each group at each data collection time point. As it pertains to the goals of this project, I have identified 10 research studies that have evaluated the teamwork training course. Each of these 10 studies evaluated three groups of participants three times: a baseline pre test assessment, a post test assessment that occurred right after the training event was completed, and then a follow-up test that occurred anywhere from 1 to 12 months after the training event. I will discuss the groups of participants when I test for the effect of moderators [here](/Portfolio%20pages/MLMAMod.qmd). The means and standard deviations calculated at each time point represents an average training \"effectiveness\" metric that has been decided upon by the researchers who conducted the research contained in these 10 studies (i.e., it is shared metric of training effectiveness). *Note: I created this data set for the purpose of this demonstration.* Much greater detail on how to synthesize research and how to locate, code, and describe potential studies can be found in [The Handbook of Research Synthesis and Mata-Analysis](https://www.russellsage.org/publications/handbook-research-synthesis-and-meta-analysis) by Cooper, Hedges, and Valentine (2019).\n\nFor the sake of this project, there will be no attrition between time points, but please recognize that this is relatively rare within this type of research. Consultants will have to make decisions over how to treat sample attrition as the analyses require a single sample size for each group.\n\nIn this project, I will demonstrate how to calculate the meta-analytic effects for the differences between the pre and (separately) the post and follow-up time points and illustrate the information that can be gleaned from this approach.\n\nFirst, I will load in the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ez)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(janitor)\n\nData <- read_excel(\"C:/Users/matts/OneDrive/Desktop/002 Matt Desktop/WebsiteData/Data/MLMA example data.xlsx\")\nhead(Data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 11\n  Study_ID Effect_ID  Mod1 `Months since follow`     N Mean_pre Mean_post\n     <dbl>     <dbl> <dbl>                 <dbl> <dbl>    <dbl>     <dbl>\n1        1         1     1                     2   306     3.5       5.86\n2        1         2     2                     6   248     2.71      5.79\n3        1         3     3                    12   329     2.48      3.3 \n4        2         4     1                     4    82     3.78      4.42\n5        2         5     2                     2   418     3.07      6.4 \n6        2         6     3                     1   309     4.12      6.63\n# ℹ 4 more variables: Mean_follow <dbl>, SD1 <dbl>, SD2 <dbl>, SD3 <dbl>\n```\n\n\n:::\n:::\n\n\n\n::: text-center\n# **Multilevel Meta Analysis**\n:::\n\nRecent development in meta-analysis has provided an avenue for handling multiple effect sizes from the same group: multilevel meta-analysis (MLMA). The key difference between MLMA and other meta-analyses is that MLMA partitions effect size variation across three levels: within and between study variation (also discussed as heterogeneity), and the individual effect size sampling error. The benefit of implementing MLMA here is that an unbiased estimate of the population effect can be calculated using data from all participants across each time point, which also addresses issues of effect size independence.\n\nNotation of the formulas underpinning each level of a MLMA is as follows:\n\n*Formula for Level 1:* $$\n\\hat\\theta_{ij} = \\theta_{ij} + \\epsilon_{ij}\n$$\n\n*Formula for Level 2:* $$\n\\theta_{ij} = \\kappa_{j} + \\zeta_{(2)ij}\n$$\n\n*Formula for Level 3:* $$\n\\kappa_{j} = \\mu + \\zeta_{(3)j}\n$$\n\nwhereby $\\hat\\theta_{ij}$ represents an estimation of the true effect $\\theta_{ij}$. This estimation occurs for each effect size, *i*, nested in group *j*. $\\kappa_{j}$ represents the average effect size within each group and μ represents the average population effect [@harrer2021]. Lastly, $\\zeta_{(2)ij}$ refers to the within-level heterogeneity (level 2) and $\\zeta_{(3)j}$ represents the between-level heterogeneity (level 3). As demonstrated by [@harrer2021], these three formulas can be combined to form the three-level meta-regression model: $$\n\\hat\\theta_{ij} = \\mu + \\zeta_{(2)ij} + \\zeta_{(3)j} + \\epsilon_{ij}\n$$\n\nHere, $\\mu$ represents the overall average population effect (See @cheung2014 for more details on the development of MLMA formulas).\n\n::: text-center\n## **Calculating Effect Sizes for Pre vs. Post Time Points**\n:::\n\nWith this information, standardized mean differences can be calculated for each time point comparison (e.g., pre vs. post time point differences, pre vs. follow-up time point differences). These standardized mean differences are still biased estimates so they will need to be converted to Hedges' *g*, which is an unbiased estimate [@borenstein2019].\n\nGiven that the same group of participants provide data at multiple time points in each study, we will assume a correlation of *r* = .50, as recommended by @morris2002. Note that if the correlation between time points is provided in the studies themselves, OR if access to the raw data is provided, the consultant should calculate the correlations themselves and avoid the assumption of *r* = .50.\n\nThere is also an important correction factor that has to be calculated as well when calculating calculating Hedges *g*. This correction factor, *J*, is applied to the formula and is notated as:\n\n$$\nJ(n-1)= 1-\\frac{3}{4_{df}-1}\n$$\n\nwhere $J(n-1)$ is the number of pairs. *J* is calculated for each effect and multiplied by the standardized mean difference (SMD), which produces *g*. We also use these correction factors to calculate an unbiased estimate of variance [@borenstein2019].\n\nI will demonstrate with the code below how to calculate these values using the data I have just loaded. I will need to add in a row of data for the correlations between time points (if I could not gather that information from the studies themselves). Reminder: since I am using standardized mean differences in my calculation of the meta effect, decisions have to be made for which comparisons to look at. Of course, all time points can be compared to each other but this becomes tedious and often not informative. Thus, I will note here that pre scores will be compared to both post and follow-up time points (I will touch on quadratic effects later on). Of course, the consultant could evaluate the effect size for post to follow-up time points but as both of these time points occur after the training events, this information is largely irrelevant to the hospital's goals of understanding the potential effect of this training.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set correlation = .5\nri <- .5\nData$ri <- ri\n\n#Comparing Pre and Post Time Points\n\n#compute effect size  for pre vs. post\nData <- escalc(measure = \"SMCR\", \n         m1i= Mean_pre, \n         m2i = Mean_post,\n         sd1i = SD1, \n         sd2i = SD2, \n         ni = N,\n        ri = ri,\n        flip = TRUE,\n        data = Data)\n\nData <- Data |> mutate(df = N - 1)\n\n\n#compute standard error for Cohen's d using sampling variance\nData <- Data |> mutate(std_error_d = sqrt(vi / N))\n\n#Need to convert Cohen's d to Hedges' g\n\n##correction factor J\nData <- Data |> mutate(j =  1 - (3 / (4 * df - 1)))\n\n##Hedges' g: effect size\nData <- Data |> mutate(hedges_g =  yi * j)\n##Hedges' g: std error\nData <- Data |> mutate(std_error_g =  std_error_d * j)\n#Hedges' g: variance \nData <- Data |>  mutate(var_g = vi * j) \nhead(Data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  Study_ID Effect_ID Mod1 Months.since.follow   N Mean_pre Mean_post \n1        1         1    1                   2 306     3.50      5.86 \n2        1         2    2                   6 248     2.71      5.79 \n3        1         3    3                  12 329     2.48      3.30 \n4        2         4    1                   4  82     3.78      4.42 \n5        2         5    2                   2 418     3.07      6.40 \n6        2         6    3                   1 309     4.12      6.63 \n  Mean_follow  SD1  SD2  SD3  ri     yi     vi  df std_error_d         j \n1        4.84 0.43 0.72 1.19 0.5 5.4749 0.0522 305 0.013066610 0.9975390 \n2        3.10 1.06 0.87 1.61 0.5 2.8968 0.0210 247 0.009191252 0.9969605 \n3        2.14 0.71 1.40 0.88 0.5 1.1523 0.0051 328 0.003920716 0.9977117 \n4        3.90 1.49 1.20 0.85 0.5 0.4255 0.0133  81 0.012735244 0.9907121 \n5        5.21 1.55 1.16 1.82 0.5 2.1445 0.0079 417 0.004345570 0.9982004 \n6        3.42 1.56 1.61 1.97 0.5 1.6051 0.0074 308 0.004895294 0.9975630 \n   hedges_g std_error_g       var_g \n1 5.4613894 0.013034452 0.052116728 \n2 2.8880221 0.009163315 0.020887141 \n3 1.1496496 0.003911744 0.005045819 \n4 0.4215863 0.012616960 0.013175765 \n5 2.1406610 0.004337749 0.007879297 \n6 1.6011411 0.004883364 0.007386799 \n```\n\n\n:::\n:::\n\n\n\nAs you can see from the dataset, we now have calculated Hedge's *g* and its accompanying standard error and variance for each pre and post comparison contained in these 10 research studies. For additional information on the choices I make for the code and what each argument entails, I suggest reviewing the CRAN repository for the metafor package. Since I compared pre to post scores, I added the argument \"flip = true\" so that the effect sizes would be in the expected direction (assuming that the training has some positive effect).\n\nNow I will run the initial comparison of pre and post scores using the values I just prepared above. I will call this model \"Pre_V_Post\" in the code.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Pre Vs. Post Comparison\noptions(scipen = 999)\n\n#Calculating MLMA effect\nPre_v_Post <- rma.mv(yi = hedges_g, \n                        V = var_g, \n                        test = \"knha\",\n                        random = ~ 1 | Study_ID/Effect_ID,\n                        data = Data)\nsummary(Pre_v_Post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMultivariate Meta-Analysis Model (k = 30; method: REML)\n\n  logLik  Deviance       AIC       BIC      AICc   \n-59.3806  118.7613  124.7613  128.8631  125.7213   \n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed              factor \nsigma^2.1  0.5260  0.7253     10     no            Study_ID \nsigma^2.2  2.8925  1.7007     30     no  Study_ID/Effect_ID \n\nTest for Heterogeneity:\nQ(df = 29) = 3191.6410, p-val < .0001\n\nModel Results:\n\nestimate      se    tval  df    pval   ci.lb   ci.ub    \n  1.0220  0.3971  2.5739  29  0.0154  0.2099  1.8340  * \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nconfint.rma.mv(Pre_v_Post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n          estimate  ci.lb  ci.ub \nsigma^2.1   0.5260 0.0000 3.5401 \nsigma.1     0.7253 0.0000 1.8815 \n\n          estimate  ci.lb  ci.ub \nsigma^2.2   2.8925 1.5943 5.7841 \nsigma.2     1.7007 1.2627 2.4050 \n```\n\n\n:::\n\n```{.r .cell-code}\npredict.rma(Pre_v_Post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n 1.0220 0.3971 0.2099 1.8340 -2.8457 4.8897 \n```\n\n\n:::\n\n```{.r .cell-code}\nround(Pre_v_Post$tau2, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\nPre_v_Post$sigma2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5260172 2.8925091\n```\n\n\n:::\n\n```{.r .cell-code}\nfunnel(Pre_v_Post, xlab = \"Standardized Mean Difference\")\n```\n\n::: {.cell-output-display}\n![](MLMAEffect_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nforest(Pre_v_Post, header = c(\"Effect Number\", \"Weight (%)   SMD   [95% CI]\"), slab = paste(Effect_ID), mlab = \"Pooled Estimate\",  order = \"obs\", xlab = \"Standardized Mean Difference (95% CI)\", digits = 2L, cex = 1, shade = \"zebra\", showweights = TRUE)\n```\n\n::: {.cell-output-display}\n![](MLMAEffect_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n\n\nResults from this model indicate an overall positive and significant mean effect size, *g* = 1.02, 95% Confidence Interval (CI)\\[0.210, 1.834\\]. Given that the effect is positive, we can conclude that there is meta-analytic evidence that training outcomes improved immediately upon completion of the training. I also presented the SMDs and effect sizes used in this calculation via a funnel plot and as a forest plot. The forest plot visually displays the SMD for each effect size used in this model as well as their 95% CI ranges and the weight each effect contributes to the pooled estimate. Model results also indicate significant heterogeneity, *Q*(29) = 3191.64, *p* \\<.001.\n\n## Heterogeneity\n\nOne key benefit to random-effects meta-analyses is that the variability in the average effect size can be quantified by assessing the heterogeneity present in the effect. MLMA builds on this and allows for assessing both the between- and within-level heterogeneity. Heterogeneity has historically been represented by *Q*, which compares of the observed variance to what is expected due to sampling error [@morris2023]. That said, current best practices have moved away from only reporting *Q* and have instead adopted the I^2^ statistic, which denotes the proportion of variance between and within studies attributable to the true heterogeneity of effect size. Additionally, I will report the variance for each level, τ^2^.\n\n### Calculate I^2^\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(scipen = 999)\nW <- diag(1/Pre_v_Post$vi)\nX <- model.matrix(Pre_v_Post)\nP <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W\n100 * sum(Pre_v_Post$sigma2) / (sum(Pre_v_Post$sigma2) + (Pre_v_Post$k-Pre_v_Post$p)/sum(diag(P)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 99.7982\n```\n\n\n:::\n\n```{.r .cell-code}\n#variance attribution (between & within)\n100 * Pre_v_Post$sigma2 / (sum(Pre_v_Post$sigma2) + (Pre_v_Post$k-Pre_v_Post$p)/sum(diag(P)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15.3562 84.4420\n```\n\n\n:::\n:::\n\n\n\nUsing the information we just calculated, as well as information provided in the output of the model itself, we can understand quite a bit regarding the variance in the pre vs. post effect. First, and as mentioned above, results from the MLMA indicate significant heterogeneity, *Q*(29) = 3191.64, *p* \\<.001. Using the sigma 2.1 and 2.2 values and matrix multiplication output, I have also identified that both the between-level variance I^2^ = 15.36%, τ^2^ = 0.526, and at the within-study level, I^2^ = 84.44%, τ^2^ = 2.893. We can also grab confidence intervals for τ^2^ values using the confint.rma.mv function in the metafor package, which I will run in the next chunk of code.\n\nI also know that approximately 99.80% of the total variance is due to heterogeneity and approximately 0.20% of the total variance is attributable to sampling variance. These results indicate that almost all of the variance in the effect is attributable to heterogeneity, and thus likely dependent on the presence of moderators. Another way to quantify this degree of uncertainty in the pre vs. post effect size is by calculating the prediction interval surrounding this effect.\n\n### Prediction Interval\n\nThe prediction interval provides an estimate of the range of possible values expected in a population.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint.rma.mv(Pre_v_Post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n          estimate  ci.lb  ci.ub \nsigma^2.1   0.5260 0.0000 3.5401 \nsigma.1     0.7253 0.0000 1.8815 \n\n          estimate  ci.lb  ci.ub \nsigma^2.2   2.8925 1.5943 5.7841 \nsigma.2     1.7007 1.2627 2.4050 \n```\n\n\n:::\n\n```{.r .cell-code}\npredict.rma(Pre_v_Post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n 1.0220 0.3971 0.2099 1.8340 -2.8457 4.8897 \n```\n\n\n:::\n:::\n\n\n\nThe prediction interval for this SMD effect ranges from -2.846 to 4.890, suggesting that any new study conducted with this sample population could expect to find negative effects (i.e., participants perform worse on training outcomes after engaging in the training), no effects, or positive effects (i.e., participants perform better on training outcomes after engaging in the training). Of course, these SMD values are also exceptionally large, which are questionable at best and likely are driven by error and the high degree of heterogeneity present in this effect [@borenstein2019].\n\n## Testing for Outliers and Publication Bias\n\nOutliers should also be assessed as well as testing for publication bias.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##outliers -- dfbeta \ndfbetas.rma.mv(Pre_v_Post) |> mutate(influence = if_else(intrcpt > .99 | intrcpt < -.99, 'TRUE', '0'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        intrcpt influence\n1   0.396297256         0\n2   0.143790700         0\n3  -0.076287096         0\n4  -0.070287411         0\n5   0.098341001         0\n6   0.046261655         0\n7  -0.065757147         0\n8   0.090510979         0\n9  -0.094057717         0\n10  0.064823527         0\n11  0.053342867         0\n12 -0.683981975         0\n13  0.222709413         0\n14 -0.173795907         0\n15  0.029814446         0\n16 -0.005786368         0\n17  0.080354135         0\n18 -0.053061603         0\n19  0.182491009         0\n20  0.010184159         0\n21 -0.124972673         0\n22  0.099789483         0\n23 -0.075163499         0\n24  0.041619982         0\n25 -0.060537876         0\n26 -0.019188372         0\n27 -0.031570842         0\n28 -0.032071639         0\n29  0.048275103         0\n30 -0.001980348         0\n```\n\n\n:::\n:::\n\n\n\nFirst, I ran the dfbetas.rma.rv function, which tests if the removal of any *single* effect (there are 30 effects contained in this model) changes the beta value of the intercept by more than $|1|$. As you can see in the output, the removal of any single effect does not influence the intercept beta by the absolute value of 1, thus all 30 effects can be retained in the model (i.e., no strong evidence that any effects are outliers from the sample).\n\nPublication bias can occur when the studies that are included in a meta-analysis tend to favor one type of effect (i.e., significant findings), thus potentially obfuscating the \"true\" effect as significant findings tend to get published over non-significant or uninteresting findings. I will test for publication bias using both a multivariate version of Egger's test and the three-parameter selection model (3PSM). Egger's test is typically a linear regression that tests if there is symmetry between effect size and sample size. A significant Egger's test suggests that effect sizes depend on sample size, which can provide some evidence of selection bias [@pustejovsky2019]. The 3PSM test estimates three parameters ($\\mu$, τ^2^, and 𝛿~2~) using maximum likelihood estimation. Each 3PSM model has a single cut point at alpha level .025 and a significant likelihood ratio test indicates that non-significant findings are less likely to be published than significant findings. @carter2019 provides more information for this test.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#compute adjusted SE for Egger's regression test per Pustejovsky and Rodgers (2019)\nData <- Data |> mutate(se_cor = sqrt(4/N))\n\n#Small study effect (Egger's test)\nrma.mv(yi = hedges_g ~ 1 + se_cor, \n                   V = var_g,\n                   test = \"knha\",\n                   digits = 3,\n                   random = ~ 1 | Study_ID/Effect_ID,\n                   data = Data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMultivariate Meta-Analysis Model (k = 30; method: REML)\n\nVariance Components:\n\n           estim   sqrt  nlvls  fixed              factor \nsigma^2.1  0.337  0.581     10     no            Study_ID \nsigma^2.2  3.091  1.758     30     no  Study_ID/Effect_ID \n\nTest for Residual Heterogeneity:\nQE(df = 28) = 3180.178, p-val < .001\n\nTest of Moderators (coefficient 2):\nF(df1 = 1, df2 = 28) = 0.606, p-val = 0.443\n\nModel Results:\n\n         estimate     se    tval  df   pval    ci.lb  ci.ub    \nintrcpt     1.590  0.823   1.931  28  0.064   -0.096  3.277  . \nse_cor     -3.489  4.482  -0.778  28  0.443  -12.671  5.693    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n##Selection effects (3PSM)\n\n###aggregate effects\nAggregate_Eff_PrevPost <- Data %>% group_by(Study_ID) %>% mutate(agg_effect = mean(hedges_g)) %>% ungroup \n\n###compute univariate model\nPre_v_Post_Agg <- rma(yi = agg_effect, \n                   vi = var_g,\n                   digits = 3,\n                   data = Aggregate_Eff_PrevPost)\nPre_v_Post_Agg  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRandom-Effects Model (k = 30; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 1.417 (SE = 0.379)\ntau (square root of estimated tau^2 value):      1.190\nI^2 (total heterogeneity / total variability):   99.51%\nH^2 (total variability / sampling variability):  205.97\n\nTest for Heterogeneity:\nQ(df = 29) = 3238.603, p-val < .001\n\nModel Results:\n\nestimate     se   zval   pval  ci.lb  ci.ub      \n   1.019  0.219  4.645  <.001  0.589  1.449  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n#3PSM test\nselmodel(Pre_v_Post_Agg, type=\"stepfun\", steps=c(.025))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRandom-Effects Model (k = 30; tau^2 estimator: ML)\n\ntau^2 (estimated amount of total heterogeneity): 2.703 (SE = 1.017)\ntau (square root of estimated tau^2 value):      1.644\n\nTest for Heterogeneity:\nLRT(df = 1) = 3067.099, p-val < .001\n\nModel Results:\n\nestimate     se    zval   pval   ci.lb  ci.ub    \n  -0.675  0.887  -0.762  0.446  -2.414  1.063    \n\nTest for Selection Model Parameters:\nLRT(df = 1) = 13.652, p-val < .001\n\nSelection Model Results:\n\n                     k  estimate     se     zval   pval  ci.lb  ci.ub      \n0     < p <= 0.025  27     1.000    ---      ---    ---    ---    ---      \n0.025 < p <= 1       3     0.044  0.043  -22.350  <.001  0.000  0.128  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\nResults of the Egger's test do not indicate non-symmetry, *b* = 1.590, *SE* = 0.823, *p* = .064, however results of the 3PSM selection bias may, in fact, be influencing results of the pre vs. post MLMA model, *X*^2^(1) = 13.652, *p* \\< .001. Thus, there is some evidence that publication bias is an issue here (remember that this dataset has been generated) and the validity of these MLMA models is at risk. One additional piece of information from the output I will point out is the adjusted effect size that is calculated alongside these analyses. The effect size decreased from *g* = 1.022 to *g* = 1.019 in the adjusted model. A difference of .003 in effect size is considered trivial in the literature [@borenstein2019], but it is wise to be cautious when interpreting this model. Additional research is needed to build confidence in this effect, especially given the high degree of heterogeneity and wide prediction interval affiliated with this effect.\n\n## Pre vs. Follow-up Time Points\n\nLet's do this again and compare pre to follow-up scores. I will just provide a brief overview of the results as the same information I covered above applies for this model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Comparing Pre and Follow Time Points\n\n#compute effect size  for pre vs. follow\nData2 <- escalc(measure = \"SMCR\", \n         m1i= Mean_pre, \n         m2i = Mean_follow,\n         sd1i = SD1, \n         sd2i = SD3, \n         ni = N,\n        ri = ri,\n        flip = TRUE,\n        data = Data)\n\nData2 <- Data2 |> mutate(df = N - 1)\n\n\n#compute standard error for Cohen's d using sampling variance\nData2 <- Data2 |> mutate(std_error_d = sqrt(vi / N))\n\n#convert Cohen's d to Hedges' g: effect size & standard error & variance\n\n##correction factor J\nData2 <- Data2 |> mutate(j =  1 - (3 / (4 * df - 1)))\n\n##Hedges' g: effect size\nData2 <- Data2 |> mutate(hedges_g =  yi * j)\n##Hedges' g: std error\nData2 <- Data2 |> mutate(std_error_g =  std_error_d * j)\n#Hedges' g: variance \nData2 <- Data2 |>  mutate(var_g = vi * j) \nhead(Data2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  Study_ID Effect_ID Mod1 Months.since.follow   N Mean_pre Mean_post \n1        1         1    1                   2 306     3.50      5.86 \n2        1         2    2                   6 248     2.71      5.79 \n3        1         3    3                  12 329     2.48      3.30 \n4        2         4    1                   4  82     3.78      4.42 \n5        2         5    2                   2 418     3.07      6.40 \n6        2         6    3                   1 309     4.12      6.63 \n  Mean_follow  SD1  SD2  SD3  ri      yi     vi  df std_error_d         j \n1        4.84 0.43 0.72 1.19 0.5  3.1086 0.0191 305 0.007891818 0.9975390 \n2        3.10 1.06 0.87 1.61 0.5  0.3668 0.0043 247 0.004165682 0.9969605 \n3        2.14 0.71 1.40 0.88 0.5 -0.4778 0.0034 328 0.003208286 0.9977117 \n4        3.90 1.49 1.20 0.85 0.5  0.0798 0.0122  81 0.012214516 0.9907121 \n5        5.21 1.55 1.16 1.82 0.5  1.3782 0.0047 417 0.003340438 0.9982004 \n6        3.42 1.56 1.61 1.97 0.5 -0.4476 0.0036 308 0.003394487 0.9975630 \n     hedges_g std_error_g       var_g    se_cor \n1  3.10095836 0.007872396 0.019011020 0.1143324 \n2  0.36569112 0.004153021 0.004290441 0.1270001 \n3 -0.47668397 0.003200945 0.003378681 0.1102636 \n4  0.07904744 0.012101068 0.012120312 0.2208631 \n5  1.37568006 0.003334427 0.004655871 0.0978232 \n6 -0.44653338 0.003386214 0.003551788 0.1137760 \n```\n\n\n:::\n:::\n\n\n\nAll necessary information has now been calculated to do this comparison.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Pre vs. follow-Up Comparison\noptions(scipen = 999)\n\n#Calculating MLMA effect\nPre_v_Follow <- rma.mv(yi = hedges_g, \n                        V = var_g, \n                        test = \"knha\",\n                        random = ~ 1 | Study_ID/Effect_ID,\n                        data = Data2)\nsummary(Pre_v_Follow)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMultivariate Meta-Analysis Model (k = 30; method: REML)\n\n  logLik  Deviance       AIC       BIC      AICc   \n-59.5806  119.1613  125.1613  129.2632  126.1213   \n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed              factor \nsigma^2.1  0.0000  0.0001     10     no            Study_ID \nsigma^2.2  3.4263  1.8510     30     no  Study_ID/Effect_ID \n\nTest for Heterogeneity:\nQ(df = 29) = 4071.9106, p-val < .0001\n\nModel Results:\n\nestimate      se    tval  df    pval    ci.lb   ci.ub    \n  0.5502  0.3446  1.5966  29  0.1212  -0.1546  1.2550    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nconfint.rma.mv(Pre_v_Follow)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n          estimate  ci.lb  ci.ub \nsigma^2.1   0.0000 0.0000 1.9874 \nsigma.1     0.0001 0.0000 1.4098 \n\n          estimate  ci.lb  ci.ub \nsigma^2.2   3.4263 1.9957 6.1190 \nsigma.2     1.8510 1.4127 2.4737 \n```\n\n\n:::\n\n```{.r .cell-code}\npredict.rma(Pre_v_Follow)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   pred     se   ci.lb  ci.ub   pi.lb  pi.ub \n 0.5502 0.3446 -0.1546 1.2550 -3.3006 4.4010 \n```\n\n\n:::\n\n```{.r .cell-code}\nround(Pre_v_Follow$tau2, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\nPre_v_Follow$sigma2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.00000001561201 3.42631262292152\n```\n\n\n:::\n\n```{.r .cell-code}\nfunnel(Pre_v_Follow, xlab = \"Standardized Mean Difference\")\n```\n\n::: {.cell-output-display}\n![](MLMAEffect_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nforest(Pre_v_Follow, header = c(\"Effect Number\", \"Weight (%)   SMD   [95% CI]\"), slab = paste(Effect_ID), mlab = \"Pooled Estimate\",  order = \"obs\", xlab = \"Standardized Mean Difference (95% CI)\", digits = 2L, cex = 1, shade = \"zebra\", showweights = TRUE)\n```\n\n::: {.cell-output-display}\n![](MLMAEffect_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n:::\n\n\n\nResults from this model indicate a non-significant effect, *g* = 0.550, 95% CI \\[-0.155, 1.255\\]. I will touch upon adding moderators to MLMA models and testing for quadratic effects (ex. testing for an initial improvement in training outcomes that then decrease over time) in separate projects that can be found on my website.\n",
    "supporting": [
      "MLMAEffect_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}